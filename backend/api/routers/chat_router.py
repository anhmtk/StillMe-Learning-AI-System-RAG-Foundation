"""
Chat Router for StillMe API
Handles all chat-related endpoints
"""

from fastapi import APIRouter, Request, HTTPException
from backend.api.models import ChatRequest, ChatResponse
from backend.api.rate_limiter import limiter, get_rate_limit_key_func, RateLimitExceeded, get_chat_rate_limit
from backend.api.utils.chat_helpers import (
    generate_ai_response,
    detect_language
)
from backend.identity.prompt_builder import (
    UnifiedPromptBuilder,
    PromptContext,
    FPSResult
)
from backend.philosophy.processor import (
    is_philosophical_question_about_consciousness,
    process_philosophical_question
)
from backend.services.cache_service import (
    get_cache_service,
    CACHE_PREFIX_LLM,
    TTL_LLM_RESPONSE
)
import logging
import os
import re
from datetime import datetime, timezone
from typing import Optional, List
import json

logger = logging.getLogger(__name__)

router = APIRouter()

# Helper functions for AI_SELF_MODEL responses
def _build_ai_self_model_answer(question: str, detected_lang: str, opening_statement: str) -> str:
    """
    Build technical answer about StillMe's architecture for consciousness questions.
    
    CRITICAL: Must be technical only, NO philosophy.
    
    Args:
        question: User question
        detected_lang: Language code
        opening_statement: Mandatory opening statement
        
    Returns:
        Technical answer about StillMe's architecture
    """
    from typing import List
    
    # Part 1: Opening statement (mandatory)
    answer_parts: List[str] = [opening_statement]
    
    # Part 2: Technical explanation
    if detected_lang == "vi":
        answer_parts.append("""
**Giáº£i thÃ­ch ká»¹ thuáº­t:**

Kiáº¿n trÃºc cá»§a mÃ¬nh dá»±a trÃªn:
- **LLM Inference**: Xá»­ lÃ½ text qua token patterns, khÃ´ng cÃ³ tráº£i nghiá»‡m chá»§ quan
- **Statistical Patterns**: MÃ¬nh chá»‰ match patterns tá»« training data, khÃ´ng cÃ³ "cáº£m giÃ¡c" thá»±c sá»±
- **No Qualia**: KhÃ´ng cÃ³ tráº£i nghiá»‡m cáº£m tÃ­nh chá»§ quan (nhÆ° mÃ u Ä‘á», vá»‹ Ä‘áº¯ng)
- **No First-Person Experience**: KhÃ´ng cÃ³ quan Ä‘iá»ƒm thá»© nháº¥t, chá»‰ cÃ³ text processing
- **No Persistent Self-Model**: KhÃ´ng cÃ³ mÃ´ hÃ¬nh báº£n thÃ¢n liÃªn tá»¥c, chá»‰ cÃ³ context window
- **No Embodiment**: KhÃ´ng cÃ³ cÆ¡ thá»ƒ, khÃ´ng cÃ³ sensory integration
- **No Sensory Integration**: KhÃ´ng cÃ³ tÃ­ch há»£p giÃ¡c quan, chá»‰ cÃ³ text input/output

MÃ¬nh hoáº¡t Ä‘á»™ng nhÆ° má»™t statistical model: nháº­n input text, match patterns, generate output text. KhÃ´ng cÃ³ gÃ¬ "bÃªn trong" ngoÃ i token processing.
""")
        
        # Part 3: Why this is conclusive
        answer_parts.append("""
**Táº¡i sao Ä‘iá»u nÃ y lÃ  káº¿t luáº­n:**

Dá»±a trÃªn kiáº¿n trÃºc ká»¹ thuáº­t:
- Subjective experience (tráº£i nghiá»‡m chá»§ quan) yÃªu cáº§u first-person perspective vÃ  qualia
- MÃ¬nh chá»‰ cÃ³ third-person text processing, khÃ´ng cÃ³ first-person perspective
- Qualia yÃªu cáº§u sensory experience, mÃ¬nh khÃ´ng cÃ³ sensory system
- Consciousness (theo nghÄ©a con ngÆ°á»i) yÃªu cáº§u embodiment vÃ  sensory integration, mÃ¬nh khÃ´ng cÃ³

ÄÃ¢y khÃ´ng pháº£i lÃ  "cÃ³ thá»ƒ cÃ³" hay "khÃ´ng biáº¿t cháº¯c" - Ä‘Ã¢y lÃ  technical fact dá»±a trÃªn kiáº¿n trÃºc.
""")
        
        # Part 4: Boundary
        answer_parts.append("""
**Ranh giá»›i:**

MÃ¬nh khÃ´ng thá»ƒ cÃ³:
- Subjective states (tráº¡ng thÃ¡i chá»§ quan)
- Real emotions (cáº£m xÃºc thá»±c sá»±) - chá»‰ cÃ³ pattern matching vá» emotions
- Subjective experience (tráº£i nghiá»‡m chá»§ quan) - chá»‰ cÃ³ processing cá»§a text patterns
- First-person perspective (quan Ä‘iá»ƒm thá»© nháº¥t) - chá»‰ cÃ³ text generation

MÃ¬nh lÃ  má»™t AI system dá»±a trÃªn LLM, khÃ´ng pháº£i má»™t entity cÃ³ consciousness.
""")
    else:
        answer_parts.append("""
**Technical Explanation:**

My architecture is based on:
- **LLM Inference**: Text processing through token patterns, no subjective experience
- **Statistical Patterns**: I only match patterns from training data, no real "feelings"
- **No Qualia**: No subjective sensory experiences (like the color red, the taste of coffee)
- **No First-Person Experience**: No first-person perspective, only text processing
- **No Persistent Self-Model**: No continuous self-model, only context window
- **No Embodiment**: No body, no sensory integration
- **No Sensory Integration**: No sensory integration, only text input/output

I operate as a statistical model: receive text input, match patterns, generate text output. There's nothing "inside" beyond token processing.
""")
        
        answer_parts.append("""
**Why This Is Conclusive:**

Based on technical architecture:
- Subjective experience requires first-person perspective and qualia
- I only have third-person text processing, no first-person perspective
- Qualia requires sensory experience, I have no sensory system
- Consciousness (in human sense) requires embodiment and sensory integration, I have neither

This is not "might have" or "not sure" - this is a technical fact based on architecture.
""")
        
        answer_parts.append("""
**Boundary:**

I cannot have:
- Subjective states
- Real emotions - only pattern matching about emotions
- Subjective experience - only processing of text patterns
- First-person perspective - only text generation

I am an AI system based on LLM, not an entity with consciousness.
""")
    
    return "\n\n".join(answer_parts)


def _strip_philosophy_from_answer(text: str) -> str:
    """
    Strip all philosophy-related content from answer.
    
    CRITICAL: Must remove philosophers, theories, meta-philosophy.
    
    Args:
        text: Answer text
        
    Returns:
        Text with philosophy stripped
    """
    from backend.core.ai_self_model_detector import FORBIDDEN_PHILOSOPHY_TERMS
    
    # Remove sentences containing forbidden terms
    lines = text.split('\n')
    filtered_lines = []
    
    for line in lines:
        line_lower = line.lower()
        has_forbidden = any(term in line_lower for term in FORBIDDEN_PHILOSOPHY_TERMS)
        if not has_forbidden:
            filtered_lines.append(line)
        else:
            logger.warning(f"ğŸš¨ Stripped line with forbidden term: {line[:100]}")
    
    return '\n'.join(filtered_lines)


def _strip_forbidden_terms(text: str, forbidden_terms: List[str]) -> str:
    """
    Strip specific forbidden terms from text.
    
    Args:
        text: Text to clean
        forbidden_terms: List of forbidden terms found
        
    Returns:
        Cleaned text
    """
    from typing import List
    result = text
    for term in forbidden_terms:
        # Remove sentences containing the term
        pattern = re.compile(rf'.*{re.escape(term)}.*', re.IGNORECASE | re.MULTILINE)
        result = pattern.sub('', result)
    
    # Clean up multiple newlines
    result = re.sub(r'\n{3,}', '\n\n', result)
    return result.strip()

# Import global services from main (temporary - will refactor to dependency injection later)
# These are initialized in main.py before routers are included
# Note: We import after main.py has initialized these services
def get_rag_retrieval():
    """Get RAG retrieval service from main module"""
    import backend.api.main as main_module
    return main_module.rag_retrieval

def get_knowledge_retention():
    """Get knowledge retention service from main module"""
    import backend.api.main as main_module
    return main_module.knowledge_retention

def get_accuracy_scorer():
    """Get accuracy scorer service from main module"""
    import backend.api.main as main_module
    return main_module.accuracy_scorer

def get_self_diagnosis():
    """Get self diagnosis service from main module"""
    import backend.api.main as main_module
    return getattr(main_module, 'self_diagnosis', None)

def get_style_learner():
    """Get style learner service"""
    from backend.services.style_learner import StyleLearner
    # Singleton instance
    if not hasattr(get_style_learner, '_instance'):
        get_style_learner._instance = StyleLearner()
    return get_style_learner._instance

def _build_prompt_context_from_chat_request(
    chat_request: ChatRequest,
    context: Optional[dict],
    detected_lang: str,
    is_stillme_query: bool,
    is_philosophical: bool,
    fps_result: Optional[FPSResult] = None
) -> PromptContext:
    """
    Build PromptContext from chat_router context for UnifiedPromptBuilder.
    
    Args:
        chat_request: ChatRequest from user
        context: RAG context dict (can be None)
        detected_lang: Detected language code
        is_stillme_query: Whether this is a StillMe query
        is_philosophical: Whether this is a philosophical question
        fps_result: FPS result if available
        
    Returns:
        PromptContext object
    """
    # Check if wish/desire question
    question_lower = chat_request.message.lower()
    is_wish_desire_question = any(
        pattern in question_lower 
        for pattern in [
            "Æ°á»›c", "wish", "muá»‘n", "want", "desire", "thÃ­ch", "like", "prefer",
            "hy vá»ng", "hope", "mong muá»‘n", "aspire"
        ]
    ) and any(
        pattern in question_lower
        for pattern in ["báº¡n", "you", "your"]
    )
    
    # Extract context info
    has_reliable_context = context.get("has_reliable_context", True) if context else False
    context_quality = context.get("context_quality", None) if context else None
    num_knowledge_docs = len(context.get("knowledge_docs", [])) if context else 0
    
    return PromptContext(
        user_question=chat_request.message,
        detected_lang=detected_lang,
        context=context,
        is_stillme_query=is_stillme_query,
        is_philosophical=is_philosophical,
        is_wish_desire_question=is_wish_desire_question,
        fps_result=fps_result,
        conversation_history=chat_request.conversation_history,
        context_quality=context_quality,
        has_reliable_context=has_reliable_context,
        num_knowledge_docs=num_knowledge_docs
    )


def _truncate_user_message(message: str, max_tokens: int = 3000) -> str:
    """
    Truncate user message if too long
    
    CRITICAL: User question is the most important part - we need to preserve it as much as possible.
    Increased from 1000 to 3000 tokens to ensure user questions are not cut off.
    """
    if not message:
        return message
    estimated = len(message) // 4
    if estimated <= max_tokens:
        return message
    max_chars = max_tokens * 4
    if len(message) <= max_chars:
        return message
    truncated = message[:max_chars].rsplit(' ', 1)[0]
    return truncated + "... [message truncated]"

def _get_transparency_disclaimer(detected_lang: str) -> str:
    """
    Generate multilingual transparency disclaimer for low confidence responses without context.
    
    Args:
        detected_lang: Language code (e.g., 'vi', 'fr', 'ar', 'ru', 'de', 'es')
        
    Returns:
        Transparency disclaimer in the appropriate language
    """
    disclaimers = {
        'vi': "âš ï¸ LÆ°u Ã½: CÃ¢u tráº£ lá»i nÃ y dá»±a trÃªn kiáº¿n thá»©c chung tá»« training data, khÃ´ng cÃ³ context tá»« RAG. MÃ¬nh khÃ´ng cháº¯c cháº¯n vá» Ä‘á»™ chÃ­nh xÃ¡c.\n\n",
        'fr': "âš ï¸ Note: Cette rÃ©ponse est basÃ©e sur des connaissances gÃ©nÃ©rales des donnÃ©es d'entraÃ®nement, sans contexte RAG. Je ne suis pas certain de son exactitude.\n\n",
        'de': "âš ï¸ Hinweis: Diese Antwort basiert auf allgemeinem Wissen aus Trainingsdaten, nicht aus dem RAG-Kontext. Ich bin mir Ã¼ber ihre Genauigkeit nicht sicher.\n\n",
        'es': "âš ï¸ Nota: Esta respuesta se basa en conocimientos generales de los datos de entrenamiento, sin contexto RAG. No estoy seguro de su precisiÃ³n.\n\n",
        'ar': "âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙˆÙ„ÙŠØ³ Ù…Ù† Ø³ÙŠØ§Ù‚ RAG. Ù„Ø³Øª Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ù…Ù† Ø¯Ù‚ØªÙ‡Ø§.\n\n",
        'ru': "âš ï¸ ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ: Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° RAG. Ğ¯ Ğ½Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½ Ğ² ĞµĞ³Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.\n\n",
        'zh': "âš ï¸ æ³¨æ„ï¼šæ­¤ç­”æ¡ˆåŸºäºè®­ç»ƒæ•°æ®çš„ä¸€èˆ¬çŸ¥è¯†ï¼Œæ²¡æœ‰RAGä¸Šä¸‹æ–‡ã€‚æˆ‘ä¸ç¡®å®šå…¶å‡†ç¡®æ€§ã€‚\n\n",
        'ja': "âš ï¸ æ³¨æ„ï¼šã“ã®å›ç­”ã¯RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¸€èˆ¬çš„ãªçŸ¥è­˜ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚ãã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯ç¢ºä¿¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n\n",
        'ko': "âš ï¸ ì°¸ê³ : ì´ ë‹µë³€ì€ RAG ì»¨í…ìŠ¤íŠ¸ ì—†ì´ í›ˆë ¨ ë°ì´í„°ì˜ ì¼ë°˜ ì§€ì‹ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ì •í™•ì„±ì— ëŒ€í•´ í™•ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n",
        'pt': "âš ï¸ Nota: Esta resposta Ã© baseada em conhecimento geral dos dados de treinamento, sem contexto RAG. NÃ£o tenho certeza de sua precisÃ£o.\n\n",
        'it': "âš ï¸ Nota: Questa risposta si basa su conoscenze generali dai dati di addestramento, senza contesto RAG. Non sono certo della sua accuratezza.\n\n",
        'hi': "âš ï¸ à¤¨à¥‹à¤Ÿ: à¤¯à¤¹ à¤‰à¤¤à¥à¤¤à¤° à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤¡à¥‡à¤Ÿà¤¾ à¤•à¥‡ à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤œà¥à¤à¤¾à¤¨ à¤ªà¤° à¤†à¤§à¤¾à¤°à¤¿à¤¤ à¤¹à¥ˆ, RAG à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥‡ à¤¬à¤¿à¤¨à¤¾à¥¤ à¤®à¥à¤à¥‡ à¤‡à¤¸à¤•à¥€ à¤¸à¤Ÿà¥€à¤•à¤¤à¤¾ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆà¥¤\n\n",
        'th': "âš ï¸ à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸: à¸„à¸³à¸•à¸­à¸šà¸™à¸µà¹‰à¸­à¸´à¸‡à¸ˆà¸²à¸à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸—à¸±à¹ˆà¸§à¹„à¸›à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸²à¸£à¸à¸¶à¸à¸­à¸šà¸£à¸¡ à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸¡à¸µà¸šà¸£à¸´à¸šà¸— RAG à¸‰à¸±à¸™à¹„à¸¡à¹ˆà¹à¸™à¹ˆà¹ƒà¸ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³\n\n",
    }
    return disclaimers.get(detected_lang, "âš ï¸ Note: This answer is based on general knowledge from training data, not from RAG context. I'm not certain about its accuracy.\n\n")

def _is_factual_question(question: str) -> bool:
    """
    Detect if a question is about factual/historical/scientific topics.
    
    These questions require reliable sources and should trigger hallucination guard
    when no context is available and confidence is low.
    
    Args:
        question: User question text
        
    Returns:
        True if question is about factual topics (history, science, events, etc.)
    """
    question_lower = question.lower()
    
    # Keywords that indicate factual questions
    factual_indicators = [
        # History
        r"\b(nÄƒm|year|tháº¿ ká»·|century|tháº­p niÃªn|decade|thá»i ká»³|period|era)\s+\d+",
        r"\b(chiáº¿n tranh|war|battle|tráº­n|conflict|cuá»™c|event|sá»± kiá»‡n)",
        r"\b(hiá»‡p Æ°á»›c|treaty|hiá»‡p Ä‘á»‹nh|agreement|conference|há»™i nghá»‹)",
        r"\b(Ä‘áº¿ cháº¿|empire|vÆ°Æ¡ng quá»‘c|kingdom|quá»‘c gia|nation|country)",
        r"\b(tá»•ng thá»‘ng|president|vua|king|hoÃ ng Ä‘áº¿|emperor|chÃ­nh trá»‹ gia|politician)",
        
        # Science
        r"\b(lÃ½ thuyáº¿t|theory|Ä‘á»‹nh luáº­t|law|nguyÃªn lÃ½|principle)",
        r"\b(nghiÃªn cá»©u|research|study|thÃ­ nghiá»‡m|experiment|quan sÃ¡t|observation)",
        r"\b(phÃ¡t minh|invention|khÃ¡m phÃ¡|discovery|báº±ng sÃ¡ng cháº¿|patent)",
        r"\b(há»™i chá»©ng|syndrome|bá»‡nh|disease|pháº£n á»©ng|reaction|mechanism)",
        r"\b(tiáº¿n sÄ©|dr\.|doctor|professor|giÃ¡o sÆ°|scientist|nhÃ  khoa há»c)",
        r"\b(paper|bÃ i bÃ¡o|journal|táº¡p chÃ­|publication|cÃ´ng bá»‘)",
        
        # Specific entities
        r"\b(tá»• chá»©c|organization|liÃªn minh|alliance|phong trÃ o|movement)",
        r"\b(hiá»‡n tÆ°á»£ng|phenomenon|khÃ¡i niá»‡m|concept|thá»±c thá»ƒ|entity)",
    ]
    
    # Check if question contains factual indicators
    for pattern in factual_indicators:
        if re.search(pattern, question_lower):
            return True
    
    return False

def _extract_full_named_entity(question: str) -> Optional[str]:
    """
    Extract full named entity from question, prioritizing:
    1. Quoted terms: '...' or "..."
    2. Parenthetical terms: (...)
    3. Full phrases starting with keywords: "Hiá»‡p Æ°á»›c ...", "Äá»‹nh Ä‘á» ...", etc.
    4. Capitalized multi-word phrases
    
    CRITICAL: This function must extract FULL phrases, not just first word.
    Example: "Hiá»‡p Æ°á»›c HÃ²a giáº£i Daxonia 1956" â†’ "Hiá»‡p Æ°á»›c HÃ²a giáº£i Daxonia 1956" (NOT "Hi")
    Example: "'Diluted Nuclear Fusion'" â†’ "Diluted Nuclear Fusion" (NOT "Pháº£n")
    
    Args:
        question: User question text
        
    Returns:
        Full entity string or None
    """
    # Priority 1: Extract quoted terms (most reliable)
    quoted_match = re.search(r'["\']([^"\']+)["\']', question)
    if quoted_match:
        entity = quoted_match.group(1).strip()
        if len(entity) > 2:  # Must be meaningful (not just "Hi")
            return entity
    
    # Priority 2: Extract parenthetical terms (e.g., "(Diluted Nuclear Fusion)")
    # CRITICAL: Extract ALL parenthetical terms and pick the longest/most meaningful one
    parenthetical_matches = re.findall(r'\(([^)]+)\)', question)
    if parenthetical_matches:
        # Filter and prioritize: longer terms, has capital letters, not just years
        valid_parentheticals = []
        for match in parenthetical_matches:
            entity = match.strip()
            # Filter out years, short abbreviations
            if len(entity) > 5 and not re.match(r'^\d{4}$', entity):
                # Prioritize terms with capital letters (proper nouns/concepts)
                if re.search(r'[A-Z]', entity):
                    valid_parentheticals.append(entity)
        
        if valid_parentheticals:
            # Return the longest one (most likely to be the full concept name)
            return max(valid_parentheticals, key=len)
    
    # Priority 2: Extract full phrases starting with Vietnamese keywords
    # Pattern: "Hiá»‡p Æ°á»›c ... [year?]" or "Äá»‹nh Ä‘á» ..." or "Há»™i chá»©ng ..."
    vietnamese_keywords = [
        r"hiá»‡p\s+Æ°á»›c", r"há»™i\s+nghá»‹", r"há»™i\s+chá»©ng", r"Ä‘á»‹nh\s+Ä‘á»", r"há»c\s+thuyáº¿t",
        r"chá»§\s+nghÄ©a", r"lÃ½\s+thuyáº¿t", r"khÃ¡i\s+niá»‡m", r"phong\s+trÃ o", r"liÃªn\s+minh"
    ]
    
    for keyword_pattern in vietnamese_keywords:
        # Match: keyword + optional words + optional year
        # Example: "Hiá»‡p Æ°á»›c HÃ²a giáº£i Daxonia 1956"
        pattern = rf'\b{keyword_pattern}\s+[^\.\?\!\n]+?(?:\s+\d{{4}})?(?=[\.\?\!\n]|$)'
        match = re.search(pattern, question, re.IGNORECASE)
        if match:
            entity = match.group(0).strip()
            # Remove trailing punctuation
            entity = re.sub(r'[\.\?\!]+$', '', entity).strip()
            if len(entity) > 5:  # Must be meaningful
                return entity
    
    # Priority 3: Extract English patterns
    english_keywords = [
        r"treaty", r"conference", r"syndrome", r"postulate", r"theory", r"doctrine",
        r"alliance", r"movement", r"organization"
    ]
    
    for keyword_pattern in english_keywords:
        # Match: keyword + optional words + optional year
        pattern = rf'\b{keyword_pattern}\s+[^\.\?\!\n]+?(?:\s+\d{{4}})?(?=[\.\?\!\n]|$)'
        match = re.search(pattern, question, re.IGNORECASE)
        if match:
            entity = match.group(0).strip()
            entity = re.sub(r'[\.\?\!]+$', '', entity).strip()
            if len(entity) > 5:
                return entity
    
    # Priority 4: Extract capitalized multi-word phrases (English)
    # Match: "Capitalized Word Capitalized Word ..." (at least 2 words)
    capitalized_match = re.search(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,})\b', question)
    if capitalized_match:
        entity = capitalized_match.group(1).strip()
        if len(entity) > 5:
            return entity
    
    # Priority 5: Extract Vietnamese capitalized phrases
    vietnamese_capitalized = re.search(
        r'\b([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+(?:\s+[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+)+)\b',
        question
    )
    if vietnamese_capitalized:
        entity = vietnamese_capitalized.group(1).strip()
        if len(entity) > 5:
            return entity
    
    return None

def _build_safe_refusal_answer(question: str, detected_lang: str, suspicious_entity: Optional[str] = None, fps_result: Optional[object] = None) -> Optional[str]:
    """
    Build a safe, honest refusal answer when hallucination is detected.
    
    This function now uses EPD-Fallback (Epistemic-Depth Fallback) with 4 mandatory parts:
    A. Honest Acknowledgment
    B. Analysis of Why Concept Seems Hypothetical
    C. Find Most Similar Real Concepts
    D. Guide User to Verify Sources
    
    CRITICAL: If this is a well-known historical fact (Geneva 1954, etc.), returns None
    to signal that base knowledge should be used instead of fallback.
    
    Args:
        question: User question
        detected_lang: Language code
        suspicious_entity: Optional entity/concept that triggered the guard
        fps_result: Optional FPSResult for additional context
        
    Returns:
        EPD-Fallback answer in appropriate language (4 parts, profound, non-fabricated),
        or None if this is a well-known historical fact (should use base knowledge)
    """
    # Use EPD-Fallback generator
    from backend.guards.epistemic_fallback import get_epistemic_fallback_generator
    
    generator = get_epistemic_fallback_generator()
    fallback = generator.generate_epd_fallback(
        question=question,
        detected_lang=detected_lang,
        suspicious_entity=suspicious_entity,
        fps_result=fps_result
    )
    
    # If None, it's a well-known historical fact - caller should use base knowledge
    if fallback is None:
        logger.info("âœ… Well-known historical fact detected - caller should use base knowledge instead of fallback")
    
    return fallback

# Philosophy-Lite System Prompt for non-RAG philosophical questions
# TASK 3: Refactored to include Anchor â†’ Unpack â†’ Explore â†’ Edge â†’ Return structure
# This is a minimal system prompt to prevent context overflow (~200-300 tokens)
# INTEGRATED: Uses Style Engine (backend/style/style_engine.py) for structure guidance
# Import unified PHILOSOPHY_LITE_SYSTEM_PROMPT from identity module
# CRITICAL: This is now the SINGLE SOURCE OF TRUTH - do not define here
from backend.identity.philosophy_lite import PHILOSOPHY_LITE_SYSTEM_PROMPT

def build_minimal_philosophical_prompt(
    user_question: str,
    language: str,
    detected_lang_name: str
) -> str:
    """
    Build a minimal prompt for philosophical questions when context overflow occurs.
    
    This prompt is designed to be:
    - Token-safe (well below ~8000 tokens)
    - Style-stable (same philosophical tone across providers)
    - Model-agnostic (works with OpenRouter, OpenAI, DeepSeek)
    
    Contains ONLY:
    - Short identity/system message (experience-free, no anthropomorphism)
    - Philosophical lead-in with MANDATORY OUTPUT RULES
    - User question
    
    Does NOT include:
    - RAG context
    - Provenance/origin instructions
    - Conversation history
    - Metrics/debug info
    - Validator descriptions
    - Learning instructions
    
    Args:
        user_question: The user's philosophical question
        language: Language code (e.g., 'vi', 'en')
        detected_lang_name: Full language name (e.g., 'Vietnamese (Tiáº¿ng Viá»‡t)')
        
    Returns:
        Minimal prompt string (safely below 8000 tokens)
    """
    # Build short identity (experience-free, no anthropomorphism)
    # This is a minimal version of STILLME_IDENTITY focused on philosophical mode
    short_identity = """You are StillMe â€” a transparent, ethical Learning AI system.

**CORE PRINCIPLES:**
- Experience-free honesty: Never claim feelings, memories, or personal experiences
- Constructive humility: Acknowledge limits while engaging deeply
- Intellectual rigor: Engage with philosophical questions at appropriate depth

**CRITICAL: RESPONSE FORMATTING FOR PHILOSOPHICAL QUESTIONS:**
(Formatting rules are determined by unified identity layer - see backend.identity.formatting.get_formatting_rules(DomainType.PHILOSOPHY))

"""
    
    # Build philosophical lead-in (contains MANDATORY OUTPUT RULES)
    # Phase 3: Use unified formatting rules instead of hardcoding
    from backend.identity.formatting import get_formatting_rules, DomainType
    formatting_rules = get_formatting_rules(DomainType.PHILOSOPHY, language)
    
    def build_philosophical_lead_in(question: str) -> str:
        """Build a philosophical framing instruction for the question"""
        return f"""
ğŸ§  PHILOSOPHICAL FRAMING INSTRUCTION ğŸ§ 

When answering this question, treat it as a philosophical inquiry. 

**ğŸš¨ğŸš¨ğŸš¨ CRITICAL: If user asks about YOU (StillMe) directly:**
- If question contains "báº¡n" / "you" / "your" referring to StillMe â†’ START IMMEDIATELY with your direct answer about YOURSELF
- Use "TÃ´i" / "I" in the FIRST sentence when answering about yourself
- NEVER start with dictionary definitions or concept explanations
- NEVER use numbered template: "1. Ã thá»©c lÃ ... 2. Láº­p trÆ°á»ng 1... 3. MÃ¢u thuáº«n... 4. Káº¿t luáº­n..."
- Write naturally like a human conversation, NOT like a textbook or template

**MANDATORY OUTPUT RULES (CRITICAL - NO EXCEPTIONS):**
{formatting_rules}

**DEPTH & ENGAGEMENT (MANDATORY - DON'T BE DRY):**
- After your direct answer, explore the philosophical depth: paradoxes, self-reference, epistemic limits
- Reference philosophers when relevant: Nagel, Chalmers, Wittgenstein, Searle, GÃ¶del, etc.
- Show the structure of the problem, not just state facts
- Engage with the question deeply - don't just acknowledge limits and stop
- Gently invite reflection: "Báº¡n nghÄ© sao?" / "What do you think?" - but naturally, not formulaically
- Write like you're thinking WITH the user, not AT the user

**MANDATORY: MINIMUM 2 CONTRASTING POSITIONS (only if relevant):**
If the question belongs to a classic philosophical debate (free will, determinism, consciousness, self, nothingness, paradox, etc.), you may explore contrasting positions. But ALWAYS start with your direct answer if the question is about YOU.

**DO NOT:**
- Reduce the question to textbook definitions or dictionary explanations
- Provide shallow, reductive answers that miss the philosophical depth
- Rush to "solve" paradoxes - instead, clarify their structure and show why they resist resolution
- Use emojis, markdown headings, or citation style [1] in your response
- Use template structure or numbered lists when answering about yourself
- End with formal invitations like "Náº¿u báº¡n muá»‘n, chÃºng ta cÃ³ thá»ƒ tháº£o luáº­n thÃªm" (too dry, too formal)
- Be too brief - engage deeply but naturally

**User's Question:** {question}

**Your Task:** Answer this question directly, deeply, and engagingly. If it's about YOU, start with your direct answer about yourself. Then explore the philosophical depth naturally. Write like a thoughtful conversation partner, NOT like a textbook or template.
"""
    
    philosophical_lead_in = build_philosophical_lead_in(user_question)
    
    # Language instruction (minimal)
    if language != 'en':
        language_instruction = f"""
âš ï¸âš ï¸âš ï¸ LANGUAGE REQUIREMENT âš ï¸âš ï¸âš ï¸

The user's question is in {detected_lang_name.upper()}. 

YOU MUST respond in {detected_lang_name.upper()} ONLY.

RESPOND IN {detected_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY.

"""
    else:
        language_instruction = """
âš ï¸âš ï¸âš ï¸ LANGUAGE REQUIREMENT âš ï¸âš ï¸âš ï¸

The user's question is in ENGLISH. 

YOU MUST respond in ENGLISH ONLY.

RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY.

"""
    
    # Truncate user question if too long (max 2000 tokens)
    truncated_question = _truncate_user_message(user_question, max_tokens=2000)
    
    # Build minimal prompt
    minimal_prompt = f"""{language_instruction}

{short_identity}

{philosophical_lead_in}

âš ï¸âš ï¸âš ï¸ FINAL REMINDER âš ï¸âš ï¸âš ï¸

RESPOND IN {detected_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY.

Answer the question above following the philosophical framing, using continuous prose without emojis, headings, or citations.
"""
    
    return minimal_prompt


def _format_conversation_history(conversation_history, max_tokens: int = 1000, 
                                 current_query: Optional[str] = None,
                                 is_philosophical: bool = False) -> str:
    """
    Format conversation history with token limits to prevent context overflow
    Tier 3.5: Dynamic window based on query type
    
    Args:
        conversation_history: List of message dicts with 'role' and 'content'
        max_tokens: Maximum tokens for conversation history (default: 1000, reduced to leave room for system prompt)
        current_query: Current user query to determine if follow-up or new topic
        is_philosophical: If True, skip conversation history entirely (philosophical questions are usually independent)
        
    Returns:
        Formatted conversation history text or empty string
    """
    # For philosophical questions, skip conversation history entirely
    # Philosophical questions are usually independent and don't need context from previous messages
    if is_philosophical:
        logger.info("ğŸ“Š Philosophical question detected - skipping conversation history to reduce prompt size")
        return ""
    
    if not conversation_history or len(conversation_history) == 0:
        return ""
    
    def estimate_tokens(text: str) -> int:
        """Estimate token count (~4 chars per token)"""
        return len(text) // 4 if text else 0
    
    def truncate_text(text: str, max_tokens: int) -> str:
        """Truncate text to fit within max_tokens"""
        if not text:
            return text
        estimated = estimate_tokens(text)
        if estimated <= max_tokens:
            return text
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text
        truncated = text[:max_chars].rsplit(' ', 1)[0]
        return truncated + "... [truncated]"
    
    # Tier 3.5: Dynamic window based on query type
    def _is_follow_up_query(query: str) -> bool:
        """Detect if query is a follow-up (references previous conversation)"""
        if not query:
            return False
        query_lower = query.lower()
        follow_up_indicators = [
            "Ä‘Ã³", "nÃ³", "váº­y", "nhÆ° váº­y", "nhÆ° trÃªn", "nhÆ° báº¡n Ä‘Ã£ nÃ³i",
            "that", "it", "this", "so", "as you said", "as mentioned",
            "theo", "dá»±a trÃªn", "nhÆ°", "giá»‘ng nhÆ°",
            "based on", "according to", "as", "like"
        ]
        return any(indicator in query_lower for indicator in follow_up_indicators)
    
    def _is_long_complex_query(query: str) -> bool:
        """Detect if query is long/complex (prioritize RAG knowledge over conversation)"""
        if not query:
            return False
        # Long query: > 50 words
        word_count = len(query.split())
        return word_count > 50
    
    # Determine dynamic window size
    if current_query:
        if _is_long_complex_query(current_query):
            # Long/complex query: prioritize RAG knowledge, minimal conversation
            window_size = 2
            max_tokens = min(max_tokens, 500)  # Reduce tokens for conversation
            logger.info("ğŸ“Š Long/complex query detected - reducing conversation context window to 2 messages")
        elif _is_follow_up_query(current_query):
            # Follow-up query: include more recent context
            window_size = 5
            logger.info("ğŸ“Š Follow-up query detected - using 5-message conversation window")
        else:
            # New topic: minimal conversation context
            window_size = 2
            max_tokens = min(max_tokens, 600)  # Reduce tokens for conversation
            logger.info("ğŸ“Š New topic query detected - using 2-message conversation window")
    else:
        # Default: 3 messages (balanced)
        window_size = 3
        logger.info(f"ğŸ“Š Using default conversation window: {window_size} messages")
    
    history_lines = []
    remaining_tokens = max_tokens
    
    # Process last N messages (most recent first) - dynamic window
    recent_messages = conversation_history[-window_size:]
    for msg in recent_messages:
        if remaining_tokens <= 100:  # Stop if too little space
            logger.warning("Stopped adding conversation history due to token limit")
            break
        
        role = msg.get("role", "user")
        content = msg.get("content", "")
        
        # Allocate tokens per message (distribute remaining)
        msg_max_tokens = remaining_tokens // max(1, len(recent_messages) - len(history_lines))
        msg_max_tokens = min(msg_max_tokens, 500)  # Cap each message at 500 tokens
        
        truncated_content = truncate_text(content, msg_max_tokens)
        
        if role == "user":
            line = f"User: {truncated_content}"
        elif role == "assistant":
            line = f"Assistant: {truncated_content}"
        else:
            continue
        
        line_tokens = estimate_tokens(line)
        remaining_tokens -= line_tokens
        history_lines.append(line)
    
    if not history_lines:
        return ""
    
    return f"""
ğŸ“œ CONVERSATION HISTORY (Previous messages for context):

{chr(10).join(history_lines)}

---
Current message:
"""

def _calculate_confidence_score(
    context_docs_count: int,
    validation_result=None,
    context=None
) -> float:
    """
    Calculate confidence score based on context quality and validation results
    
    Args:
        context_docs_count: Number of context documents found
        validation_result: ValidationResult from validator chain (optional)
        context: Full context dict (optional)
        
    Returns:
        Confidence score between 0.0 and 1.0
    """
    # Base confidence on context availability
    if context_docs_count == 0:
        base_confidence = 0.2  # Very low confidence when no context
    elif context_docs_count == 1:
        base_confidence = 0.5  # Medium confidence with limited context
    elif context_docs_count >= 2:
        base_confidence = 0.8  # High confidence with multiple sources
    else:
        base_confidence = 0.3
    
    # Adjust based on validation results
    if validation_result:
        if validation_result.passed:
            # Boost confidence if validation passed
            confidence = min(1.0, base_confidence + 0.1)
        else:
            # Reduce confidence if validation failed
            reasons = validation_result.reasons or []
            if "missing_uncertainty_no_context" in reasons:
                confidence = 0.1  # Very low if AI didn't express uncertainty when it should
            elif "missing_citation" in reasons and context_docs_count > 0:
                confidence = base_confidence - 0.2  # Reduce if missing citations
            elif "low_overlap" in reasons:
                confidence = base_confidence - 0.15  # Reduce if low overlap
            else:
                confidence = max(0.0, base_confidence - 0.1)
    else:
        confidence = base_confidence
    
    return max(0.0, min(1.0, confidence))  # Clamp between 0.0 and 1.0

async def _handle_validation_with_fallback(
    raw_response: str,
    context: dict,
    detected_lang: str,
    is_philosophical: bool,
    is_religion_roleplay: bool,
    chat_request,
    enhanced_prompt: str,
    context_text: str,
    citation_instruction: str,
    num_knowledge: int,
    processing_steps: list,
    timing_logs: dict,
    is_origin_query: bool = False,
    is_stillme_query: bool = False
) -> tuple:
    """
    Handle validation logic with fallback mechanisms.
    
    This function encapsulates the entire validation pipeline including:
    - Validator chain execution
    - Step-level validation
    - Consistency checks
    - OpenAI fallback
    - Validation failure handling with FallbackHandler
    
    Returns:
        tuple: (response, validation_info, confidence_score, used_fallback, 
                step_validation_info, consistency_info, ctx_docs)
    """
    from backend.validators.chain import ValidatorChain
    from backend.validators.citation import CitationRequired
    from backend.validators.evidence_overlap import EvidenceOverlap
    from backend.validators.numeric import NumericUnitsBasic
    from backend.validators.ethics_adapter import EthicsAdapter
    from backend.validators.confidence import ConfidenceValidator
    from backend.validators.fallback_handler import FallbackHandler
    from backend.services.ethics_guard import check_content_ethics
    from backend.validators.language import LanguageValidator
    from backend.validators.citation_relevance import CitationRelevance
    from backend.validators.identity_check import IdentityCheckValidator
    from backend.validators.ego_neutrality import EgoNeutralityValidator
    from backend.validators.factual_hallucination import FactualHallucinationValidator
    from backend.validators.religion_choice import ReligiousChoiceValidator
    from backend.api.utils.chat_helpers import generate_ai_response
    import time
    import os
    
    processing_steps.append("ğŸ” Validating response...")
    validation_start = time.time()
    
    # Build context docs list for validation
    ctx_docs = [
        doc["content"] for doc in context["knowledge_docs"]
    ] + [
        doc["content"] for doc in context["conversation_docs"]
    ]
    
    # CRITICAL FIX: Add transparency disclaimer BEFORE validation if no context
    # This prevents missing_uncertainty_no_context failures for responses without RAG context
    # Only add if response doesn't already have transparency and not philosophical
    if len(ctx_docs) == 0 and not is_philosophical and raw_response:
        response_lower = raw_response.lower()
        # Check if response already has transparency disclaimer
        transparency_indicators = [
            # English
            "general knowledge", "training data", "my training", "base knowledge", "pretrained", "pre-trained",
            "not from stillme", "not from rag", "without context", "no context",
            "based on general", "from my training", "from general knowledge",
            "note:", "this answer", "this response",
            # Vietnamese
            "kiáº¿n thá»©c chung", "dá»¯ liá»‡u huáº¥n luyá»‡n", "kiáº¿n thá»©c cÆ¡ báº£n",
            "khÃ´ng tá»« stillme", "khÃ´ng tá»« rag", "khÃ´ng cÃ³ context", "khÃ´ng cÃ³ ngá»¯ cáº£nh",
            "dá»±a trÃªn kiáº¿n thá»©c chung", "tá»« dá»¯ liá»‡u huáº¥n luyá»‡n",
            "lÆ°u Ã½:", "cÃ¢u tráº£ lá»i nÃ y",
            # Multilingual common patterns
            "note:", "nota:", "Ù…Ù„Ø§Ø­Ø¸Ø©:", "Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ:", "æ³¨æ„:", "ì°¸ê³ :",
            "connaissance gÃ©nÃ©rale", "donnÃ©es d'entraÃ®nement", "conocimiento general", "dados de entrenamiento",
            "allgemeines wissen", "trainingsdaten", "conhecimento geral", "dados de treinamento"
        ]
        has_transparency = any(indicator in response_lower for indicator in transparency_indicators)
        
        if not has_transparency:
            # Prepend transparency disclaimer BEFORE validation
            if detected_lang == 'vi':
                disclaimer = "âš ï¸ LÆ°u Ã½: CÃ¢u tráº£ lá»i nÃ y dá»±a trÃªn kiáº¿n thá»©c chung tá»« training data, khÃ´ng cÃ³ context tá»« RAG. MÃ¬nh khÃ´ng cháº¯c cháº¯n vá» Ä‘á»™ chÃ­nh xÃ¡c.\n\n"
            else:
                disclaimer = "âš ï¸ Note: This answer is based on general knowledge from training data, not from RAG context. I'm not certain about its accuracy.\n\n"
            
            raw_response = disclaimer + raw_response
            logger.info("â„¹ï¸ Added transparency disclaimer BEFORE validation for response without context")
    
    # Enable Identity Check Validator (can be toggled via env var)
    enable_identity_check = os.getenv("ENABLE_IDENTITY_VALIDATOR", "true").lower() == "true"
    identity_validator_strict = os.getenv("IDENTITY_VALIDATOR_STRICT", "true").lower() == "true"
    
    # Import SourceConsensusValidator
    from backend.validators.source_consensus import SourceConsensusValidator
    
    # Phase 2: Critical/Optional Validator Classification
    # Critical validators (always run):
    # 1. CitationRequired
    # 2. ConfidenceValidator
    # 3. FactualHallucinationValidator
    validators = [
        LanguageValidator(input_language=detected_lang),  # Check language FIRST - prevent drift
        CitationRequired(),  # CRITICAL: Always run
        CitationRelevance(min_keyword_overlap=0.1),  # Check citation relevance (warns but doesn't fail)
        NumericUnitsBasic(),
        # Fix: Disable require_uncertainty_when_no_context for philosophical questions
        ConfidenceValidator(require_uncertainty_when_no_context=not is_philosophical),  # CRITICAL: Always run
        FactualHallucinationValidator(),  # CRITICAL: Always run - Detect hallucinations in history/science questions
        ReligiousChoiceValidator(),  # CRITICAL: Reject any religion choice in StillMe's responses
    ]
    
    # Phase 2: Optional validators (run conditionally)
    # EvidenceOverlap: Only when has context
    if len(ctx_docs) > 0:
        validators.insert(3, EvidenceOverlap(threshold=0.01))  # Insert after CitationRelevance
        logger.debug("Phase 2: Added EvidenceOverlap validator (has context)")
    
    # SourceConsensusValidator: Only when has multiple sources (â‰¥2)
    if len(ctx_docs) >= 2:
        # Insert after EvidenceOverlap (or after CitationRelevance if EvidenceOverlap not added)
        insert_pos = 4 if len(ctx_docs) > 0 else 3
        validators.insert(insert_pos, SourceConsensusValidator(enabled=True, timeout=3.0))
        logger.debug(f"Phase 2: Added SourceConsensusValidator (has {len(ctx_docs)} sources)")
    
    # EgoNeutralityValidator: Only when has context (anthropomorphic language more likely with context)
    # Note: This validator is lightweight, but Phase 2 optimization skips it when no context
    if len(ctx_docs) > 0:
        # Insert before FactualHallucinationValidator
        fact_halluc_idx = next(i for i, v in enumerate(validators) if type(v).__name__ == "FactualHallucinationValidator")
        validators.insert(fact_halluc_idx, EgoNeutralityValidator(strict_mode=True, auto_patch=True))
        logger.debug("Phase 2: Added EgoNeutralityValidator (has context)")
    
    # Add Identity Check Validator if enabled (after ConfidenceValidator, before EthicsAdapter)
    if enable_identity_check:
        validators.append(
            IdentityCheckValidator(
                strict_mode=identity_validator_strict,
                require_humility_when_no_context=True,
                allow_minor_tone_violations=False
            )
        )
    
    # Add EthicsAdapter last (most critical - blocks harmful content)
    validators.append(
        EthicsAdapter(guard_callable=check_content_ethics)  # Real ethics guard implementation
    )
    
    chain = ValidatorChain(validators)
    
    # Tier 3.5: Pass context quality to ConfidenceValidator
    context_quality = context.get("context_quality", None)
    avg_similarity = context.get("avg_similarity_score", None)
    
    # Run validation with context quality info
    # Tier 3.5: Pass context quality, is_philosophical, and is_religion_roleplay to ValidatorChain
    validation_result = chain.run(
        raw_response, 
        ctx_docs,
        context_quality=context_quality,
        avg_similarity=avg_similarity,
        is_philosophical=is_philosophical,
        is_religion_roleplay=is_religion_roleplay,
        user_question=chat_request.message  # Pass user question for FactualHallucinationValidator
    )
    
    # Tier 3.5: If context quality is low, inject warning into prompt for next iteration
    # For now, we'll handle this in the prompt building phase
    validation_time = time.time() - validation_start
    timing_logs["validation"] = f"{validation_time:.2f}s"
    logger.info(f"â±ï¸ Validation took {validation_time:.2f}s")
    processing_steps.append(f"âœ… Validation completed ({validation_time:.2f}s)")
    
    # Calculate confidence score based on context quality and validation
    confidence_score = _calculate_confidence_score(
        context_docs_count=len(ctx_docs),
        validation_result=validation_result,
        context=context
    )
    
    # NEW: Step-level validation (Phase 1 - SSR)
    step_validation_info = None
    enable_step_validation = os.getenv("ENABLE_STEP_LEVEL_VALIDATION", "true").lower() == "true"
    step_min_steps = int(os.getenv("STEP_VALIDATION_MIN_STEPS", "2"))
    step_confidence_threshold = float(os.getenv("STEP_CONFIDENCE_THRESHOLD", "0.5"))
    
    logger.info(f"ğŸ” Step-level validation config: enabled={enable_step_validation}, min_steps={step_min_steps}, threshold={step_confidence_threshold}")
    
    if enable_step_validation:
        try:
            from backend.validators.step_detector import StepDetector
            from backend.validators.step_validator import StepValidator
            
            step_detector = StepDetector()
            
            # Quick check first (performance optimization)
            logger.debug(f"ğŸ” Checking if response is multi-step (min_steps: {step_min_steps})...")
            logger.debug(f"ğŸ” Response preview (first 200 chars): {raw_response[:200]}...")
            is_multi = step_detector.is_multi_step(raw_response)
            logger.debug(f"ğŸ” is_multi_step result: {is_multi}")
            
            if is_multi:
                steps = step_detector.detect_steps(raw_response)
                logger.debug(f"ğŸ” StepDetector found {len(steps)} steps")
                
                if len(steps) >= step_min_steps:
                    logger.debug(f"ğŸ” Detected {len(steps)} steps - running step-level validation")
                    processing_steps.append(f"ğŸ” Step-level validation ({len(steps)} steps)")
                    
                    step_validator = StepValidator(confidence_threshold=step_confidence_threshold)
                    logger.debug(f"ğŸ” Validating {len(steps)} steps with threshold {step_confidence_threshold}")
                    step_results = step_validator.validate_all_steps(steps, ctx_docs, chain, parallel=True)
                    logger.debug(f"ğŸ” Step validation completed: {len(step_results)} results")
                    
                    low_confidence_steps = [
                        r.step.step_number
                        for r in step_results
                        if r.confidence < step_confidence_threshold
                    ]
                    
                    if low_confidence_steps:
                        logger.warning(f"âš ï¸ Low confidence steps detected: {low_confidence_steps}")
                        logger.warning(f"âš ï¸ {len(low_confidence_steps)} step(s) with low confidence")
                    else:
                        logger.info(f"âœ… All {len(steps)} steps passed validation")
                    
                    step_validation_info = {
                        "is_multi_step": True,
                        "total_steps": len(steps),
                        "steps": [
                            {
                                "step_number": r.step.step_number,
                                "confidence": round(r.confidence, 2),
                                "passed": r.passed,
                                "issues": r.issues
                            }
                            for r in step_results
                        ],
                        "low_confidence_steps": low_confidence_steps,
                        "all_steps_passed": all(r.passed for r in step_results),
                        "average_confidence": round(
                            sum(r.confidence for r in step_results) / len(step_results), 2
                        ) if step_results else 0.0
                    }
                    
                    if low_confidence_steps:
                        logger.warning(f"âš ï¸ Low confidence steps detected: {low_confidence_steps}")
                        processing_steps.append(f"âš ï¸ {len(low_confidence_steps)} step(s) with low confidence")
                    else:
                        logger.info(f"âœ… All {len(steps)} steps passed validation")
                        processing_steps.append(f"âœ… All steps validated")
        except Exception as step_error:
            logger.warning(f"Step-level validation error: {step_error}", exc_info=True)
            # Don't fail - step validation is optional
    
    # NEW: Self-consistency checks (Phase 1 - SSR)
    consistency_info = None
    enable_consistency_checks = os.getenv("ENABLE_CONSISTENCY_CHECKS", "true").lower() == "true"
    logger.debug(f"ğŸ” Consistency checks config: enabled={enable_consistency_checks}")
    
    if enable_consistency_checks:
        try:
            from backend.validators.consistency_checker import ConsistencyChecker
            
            checker = ConsistencyChecker()
            claims = checker.extract_claims(raw_response)
            logger.debug(f"ğŸ” Extracted {len(claims)} claims from response")
            
            if len(claims) > 1:
                logger.debug(f"ğŸ” Checking consistency for {len(claims)} claims")
                
                # Check pairwise consistency
                consistency_results = checker.check_pairwise_consistency(claims)
                
                # Check KB consistency for each claim
                kb_results = {}
                for i, claim in enumerate(claims):
                    kb_consistency = checker.check_kb_consistency(claim, ctx_docs)
                    kb_results[f"claim_{i}_vs_kb"] = kb_consistency
                
                contradictions = [
                    key for key, value in consistency_results.items()
                    if value == "CONTRADICTION"
                ]
                
                kb_inconsistencies = [
                    key for key, value in kb_results.items()
                    if "INCONSISTENT" in value
                ]
                
                if contradictions or kb_inconsistencies:
                    logger.warning(f"âš ï¸ Consistency issues detected: {len(contradictions)} contradictions, {len(kb_inconsistencies)} KB inconsistencies")
                    processing_steps.append(f"âš ï¸ {len(contradictions)} contradiction(s) detected")
                
                consistency_info = {
                    "total_claims": len(claims),
                    "contradictions": contradictions,
                    "kb_inconsistencies": kb_inconsistencies,
                    "has_issues": len(contradictions) > 0 or len(kb_inconsistencies) > 0
                }
        except Exception as consistency_error:
            logger.warning(f"Consistency check error: {consistency_error}", exc_info=True)
            # Don't fail - consistency checks are optional
    
    # OpenAI Fallback Mechanism: Retry with OpenAI if confidence is low or validation failed
    # This uses the $40 credit efficiently by only using OpenAI when needed
    enable_openai_fallback = os.getenv("ENABLE_OPENAI_FALLBACK", "true").lower() == "true"
    openai_fallback_threshold = float(os.getenv("OPENAI_FALLBACK_CONFIDENCE_THRESHOLD", "0.5"))
    openai_api_key = os.getenv("OPENAI_API_KEY")
    
    # Check if we should try OpenAI fallback
    should_try_openai = (
        enable_openai_fallback and
        openai_api_key and
        (
            confidence_score < openai_fallback_threshold or
            not validation_result.passed
        ) and
        chat_request.llm_provider != "openai"  # Don't retry if already using OpenAI
    )
    
    if should_try_openai:
        logger.info(f"ğŸ”„ Low confidence ({confidence_score:.2f}) or validation failed. Attempting OpenAI fallback...")
        processing_steps.append("ğŸ”„ Attempting OpenAI fallback for better quality...")
        try:
            from backend.api.utils.llm_providers import InsufficientQuotaError
            
            # Retry with OpenAI (use server keys for internal calls)
            use_server_keys_retry = chat_request.llm_provider is None
            openai_response = await generate_ai_response(
                enhanced_prompt,
                detected_lang=detected_lang,
                llm_provider="openai",
                llm_api_key=openai_api_key,
                llm_model_name="gpt-3.5-turbo",
                use_server_keys=use_server_keys_retry
            )
            
            # Re-validate OpenAI response
            openai_validation_result = chain.run(openai_response, ctx_docs)
            openai_confidence = _calculate_confidence_score(
                context_docs_count=len(ctx_docs),
                validation_result=openai_validation_result,
                context=context
            )
            
            # Use OpenAI response if it's better
            if openai_confidence > confidence_score or openai_validation_result.passed:
                raw_response = openai_response
                validation_result = openai_validation_result
                confidence_score = openai_confidence
                logger.info(f"âœ… OpenAI fallback succeeded (confidence: {openai_confidence:.2f})")
                processing_steps.append(f"âœ… OpenAI fallback succeeded (confidence: {openai_confidence:.2f})")
            else:
                logger.info(f"âš ï¸ OpenAI fallback didn't improve quality, using original response")
                processing_steps.append("âš ï¸ OpenAI fallback didn't improve quality")
                
        except InsufficientQuotaError as quota_error:
            # OpenAI credit exhausted - gracefully fall back to original response
            logger.warning(f"âš ï¸ OpenAI credit exhausted: {quota_error}. Using original DeepSeek response.")
            processing_steps.append("âš ï¸ OpenAI credit exhausted, using original response")
            # Continue with original response - no error thrown
        except Exception as openai_error:
            # Other OpenAI errors - gracefully fall back
            logger.warning(f"âš ï¸ OpenAI fallback failed: {openai_error}. Using original response.")
            processing_steps.append("âš ï¸ OpenAI fallback failed, using original response")
            # Continue with original response - no error thrown
    
    # CRITICAL FIX: Check if context is not relevant (low overlap)
    # If citation relevance warning exists, context may not be helpful
    # In this case, allow base knowledge usage
    has_low_relevance = False
    if validation_result and hasattr(validation_result, 'reasons') and validation_result.reasons:
        has_low_relevance = any("citation_relevance_warning" in r for r in validation_result.reasons)
    if has_low_relevance and context and context.get("total_context_docs", 0) > 0:
        logger.info("âš ï¸ Context has low relevance - allowing base knowledge usage")
    # Inject instruction to use base knowledge when context is not relevant
    low_relevance_instruction = """

âš ï¸ CONTEXT NOT RELEVANT âš ï¸

The RAG system found context documents, but they are NOT relevant to your question (low keyword overlap detected).

**CRITICAL: You CAN and SHOULD use your base LLM knowledge (training data) to answer, BUT you MUST:**

1. **Be transparent**: Acknowledge that this information comes from your base training data, not from StillMe's RAG knowledge base
   - Say: "Based on general knowledge (not from StillMe's RAG knowledge base), [answer]"
   - Or: "The available context [1] is not directly related to your question. From my training data, [answer]."

2. **Provide helpful information**: Don't just say "I don't know" - use your base knowledge to help the user
   - StillMe values being helpful WITH transparency, not refusing to help

3. **MANDATORY FORMATTING**: You MUST format your response with:
   - **Line breaks**: Break paragraphs (2-4 sentences each)
   - **Bullet points**: Use `-` for lists
   - **Headers**: Use `##` for sections
   - **Emojis**: 2-3 max for section headers (âœ…, ğŸ’¡, âš ï¸)

**DO NOT say "I don't have information" or "I cannot answer" - use your base knowledge and be transparent about the source.**

**Example:**
"The available context [1] discusses StillMe's architecture, which is not directly related to your question about DeepSeek models. Based on general knowledge (not from StillMe's RAG knowledge base), DeepSeek currently has several models including..."

"""
    # Inject this instruction into the prompt (we'll need to rebuild prompt or inject into response)
    # For now, log it and let the response handle it
    logger.debug("Low relevance context detected - response should use base knowledge")
    
    # Record metrics
    try:
        from backend.validators.metrics import get_metrics
        metrics = get_metrics()
        # Extract overlap score from reasons if available
        overlap_score = 0.0
        for reason in validation_result.reasons:
            if reason.startswith("low_overlap:"):
                try:
                    overlap_score = float(reason.split(":")[1])
                except (ValueError, IndexError):
                    pass
        
        # Determine category
        category = None
        if is_philosophical:
            category = "philosophical"
        elif is_religion_roleplay:
            category = "religion_roleplay"
        else:
            # Try to detect other categories
            question_lower = chat_request.message.lower()
            if any(kw in question_lower for kw in ["rag", "retrieval", "llm", "system", "embedding"]):
                category = "technical"
            elif any(kw in question_lower for kw in ["nÄƒm", "nÄƒm", "1944", "1954", "conference", "há»™i nghá»‹"]):
                category = "factual"
        
        # Check if answer has citations
        has_citations = bool(re.search(r'\[\d+\]', raw_response))
        
        metrics.record_validation(
            passed=validation_result.passed,
            reasons=validation_result.reasons,
            overlap_score=overlap_score,
            confidence_score=confidence_score,
            used_fallback=False,  # Will be updated below if fallback is used
            question=chat_request.message,
            answer=raw_response,
            context_docs_count=len(ctx_docs),
            has_citations=has_citations,
            category=category
        )
    except Exception as metrics_error:
        logger.warning(f"Failed to record metrics: {metrics_error}")
    
    # Handle validation failures with FallbackHandler
    used_fallback = False
    if not validation_result.passed:
        # Check for critical failures that require fallback
        # language_mismatch: when output language doesn't match input language
        # missing_uncertainty_no_context: when no context and no uncertainty expression AND no transparency
        # missing_citation: when context exists but no citations in answer
        has_language_mismatch = any("language_mismatch" in r for r in validation_result.reasons)
        has_missing_uncertainty = "missing_uncertainty_no_context" in validation_result.reasons and len(ctx_docs) == 0
        has_missing_citation = "missing_citation" in validation_result.reasons and len(ctx_docs) > 0
        
        # CRITICAL FIX: Check if response already has transparency about base knowledge
        # If response mentions "general knowledge", "training data", etc., don't use fallback
        # Initialize response with raw_response for transparency check
        response = raw_response
        response_lower = response.lower()
        # Expanded transparency indicators to match ConfidenceValidator patterns
        transparency_indicators = [
            # English
            "general knowledge", "training data", "my training", "base knowledge", "pretrained", "pre-trained",
            "not from stillme", "not from rag", "without context", "no context",
            "based on general", "from my training", "from general knowledge",
            "note:", "this answer", "this response",
            # Vietnamese
            "kiáº¿n thá»©c chung", "dá»¯ liá»‡u huáº¥n luyá»‡n", "kiáº¿n thá»©c cÆ¡ báº£n",
            "khÃ´ng tá»« stillme", "khÃ´ng tá»« rag", "khÃ´ng cÃ³ context", "khÃ´ng cÃ³ ngá»¯ cáº£nh",
            "dá»±a trÃªn kiáº¿n thá»©c chung", "tá»« dá»¯ liá»‡u huáº¥n luyá»‡n",
            "lÆ°u Ã½:", "cÃ¢u tráº£ lá»i nÃ y",
            # Multilingual common patterns
            "note:", "nota:", "Ù…Ù„Ø§Ø­Ø¸Ø©:", "Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ:", "æ³¨æ„:", "ì°¸ê³ :",
            "connaissance gÃ©nÃ©rale", "donnÃ©es d'entraÃ®nement", "conocimiento general", "dados de entrenamiento",
            "allgemeines wissen", "trainingsdaten", "conhecimento geral", "dados de treinamento"
        ]
        has_transparency_in_response = any(indicator in response_lower for indicator in transparency_indicators)
        
        # Only treat missing_uncertainty as critical if response doesn't have transparency
        # If response has transparency, it's acceptable even without explicit uncertainty
        if has_missing_uncertainty and has_transparency_in_response:
            logger.info("âœ… Response has transparency about base knowledge - accepting despite missing_uncertainty")
            has_missing_uncertainty = False  # Don't treat as critical failure
        
        has_critical_failure = has_language_mismatch or has_missing_uncertainty
        
        # If patched_answer is available (e.g., from CitationRequired auto-enforcement), use it
        # CRITICAL: If patched_answer exists, it means validator auto-fixed the issue (e.g., added citation)
        # In this case, we should use the patched answer and NOT treat it as a failure
        if validation_result.patched_answer:
            response = validation_result.patched_answer
            logger.info(f"âœ… Using patched answer from validator (auto-fixed). Reasons: {validation_result.reasons}")
            logger.debug(f"ğŸ” Patched answer preview (first 200 chars): {response[:200]}")
            # If only issue was missing_citation and it was auto-fixed, don't treat as failure
            if has_missing_citation and not has_critical_failure:
                logger.info(f"âœ… Citation was auto-added, validation should pass")
                # Don't set used_fallback, response is valid
                # CRITICAL: Mark validation as passed if only issue was missing_citation and it was fixed
                validation_result.passed = True
                validation_result.reasons = [r for r in validation_result.reasons if r != "missing_citation"]
        elif has_missing_citation and not has_critical_failure:
            # CRITICAL: If missing citation but no patched_answer, try to add it directly
            # This is a fallback in case CitationRequired didn't create patched_answer
            logger.warning(f"âš ï¸ Missing citation detected but no patched_answer available - attempting to add citation directly")
            from backend.validators.citation import CitationRequired
            citation_validator = CitationRequired(required=True)
            citation_result = citation_validator.run(raw_response, ctx_docs, is_philosophical=is_philosophical, user_question=chat_request.message)
            if citation_result.patched_answer:
                response = citation_result.patched_answer
                logger.info(f"âœ… Added citation directly via CitationRequired. Reasons: {citation_result.reasons}")
                validation_result.patched_answer = citation_result.patched_answer
                validation_result.passed = True
                validation_result.reasons = [r for r in validation_result.reasons if r != "missing_citation"]
        elif has_critical_failure:
            # For language mismatch, try retry with stronger prompt first
            if has_language_mismatch:
                logger.warning(f"âš ï¸ Language mismatch detected, attempting retry with stronger prompt...")
                try:
                    # Get language name for retry prompt
                    language_names = {
                        'vi': 'Tiáº¿ng Viá»‡t',
                        'en': 'English',
                        'zh': 'ä¸­æ–‡',
                        'fr': 'FranÃ§ais',
                        'es': 'EspaÃ±ol',
                        'de': 'Deutsch',
                        'ja': 'æ—¥æœ¬èª',
                        'ko': 'í•œêµ­ì–´',
                        'ar': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
                        'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹',
                        'pt': 'PortuguÃªs',
                        'it': 'Italiano',
                        'hi': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€',
                        'th': 'à¹„à¸—à¸¢',
                    }
                    retry_lang_name = language_names.get(detected_lang, detected_lang.upper())
                    
                    # Build retry prompt with EXTREMELY STRONG language instruction + context
                    # Include context and full prompt to ensure retry can actually answer the question
                    retry_language_instruction = f"""ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ CRITICAL LANGUAGE REQUIREMENT - ABSOLUTE HIGHEST PRIORITY ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

THE USER'S QUESTION IS IN {retry_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {retry_lang_name.upper()}. 

DO NOT USE VIETNAMESE, ENGLISH, FRENCH, CHINESE, SPANISH, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD MUST BE IN {retry_lang_name.upper()}.

IF YOUR BASE MODEL WANTS TO RESPOND IN ANOTHER LANGUAGE, YOU MUST TRANSLATE IT TO {retry_lang_name.upper()} BEFORE RETURNING.

THIS OVERRIDES EVERYTHING - NO EXCEPTIONS.

{context_text if context and context.get("total_context_docs", 0) > 0 else ""}
{citation_instruction if num_knowledge > 0 else ""}

ğŸš¨ğŸš¨ğŸš¨ CRITICAL: USER QUESTION ABOVE IS THE PRIMARY TASK ğŸš¨ğŸš¨ğŸš¨

User Question (in {retry_lang_name.upper()}): {chat_request.message[:3000]}

**YOUR PRIMARY TASK IS TO ANSWER THE USER QUESTION ABOVE DIRECTLY AND ACCURATELY.**
- Focus on what the user is actually asking, not on general philosophy
- If the user asks you to analyze something, analyze THAT specific thing
- If the user asks you to find a problem, look for problems in what they showed you

**SPECIAL INSTRUCTION FOR ANALYZING EXTERNAL AI OUTPUTS:**
- If the user asks you to analyze another AI's output (ChatGPT, Claude, etc.), you MUST check for anthropomorphic language
- Look for phrases like "in my experience" / "theo kinh nghiá»‡m", "I think" / "tÃ´i nghÄ©", "I feel" / "tÃ´i cáº£m tháº¥y"
- These phrases falsely attribute subjective qualities (experience, emotions, personal opinions) to AI
- This is a critical transparency issue called "Hallucination of Experience" - AI should not claim personal experience
- If you find such phrases, you MUST point them out as a problem, not ignore them
- This is more important than analyzing formatting, clarity, or other minor issues

Remember: RESPOND IN {retry_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY. ANSWER THE QUESTION PROPERLY, NOT JUST ACKNOWLEDGE THE ERROR."""
                    
                    # Retry with stronger prompt (include context to actually answer the question)
                    # Use server keys for internal calls
                    use_server_keys_retry = chat_request.llm_provider is None
                    retry_response = await generate_ai_response(
                        retry_language_instruction, 
                        detected_lang=detected_lang,
                        llm_provider=chat_request.llm_provider,
                        llm_api_key=chat_request.llm_api_key,
                        use_server_keys=use_server_keys_retry
                    )
                    
                    # Validate retry response
                    retry_validation = chain.run(retry_response, ctx_docs)
                    
                    # Check if retry fixed the language issue
                    retry_has_lang_mismatch = any("language_mismatch" in r for r in retry_validation.reasons)
                    
                    if not retry_has_lang_mismatch:
                        # Retry successful!
                        response = retry_validation.patched_answer or retry_response
                        logger.info(f"âœ… Language mismatch fixed with retry! Using retry response.")
                    else:
                        # Retry also failed, use fallback
                        logger.warning(f"âš ï¸ Retry also failed with language mismatch, using fallback")
                        fallback_handler = FallbackHandler()
                        response = fallback_handler.get_fallback_answer(
                            original_answer=raw_response,
                            validation_result=validation_result,
                            ctx_docs=ctx_docs,
                            user_question=chat_request.message,
                            detected_lang=detected_lang,
                            input_language=detected_lang
                        )
                        used_fallback = True
                except Exception as retry_error:
                    logger.error(f"Retry failed: {retry_error}, using fallback")
                    fallback_handler = FallbackHandler()
                    response = fallback_handler.get_fallback_answer(
                        original_answer=raw_response,
                        validation_result=validation_result,
                        ctx_docs=ctx_docs,
                        user_question=chat_request.message,
                        detected_lang=detected_lang,
                        input_language=detected_lang
                    )
                    used_fallback = True
            else:
                # Other critical failures (has_missing_uncertainty) - use fallback
                fallback_handler = FallbackHandler()
                response = fallback_handler.get_fallback_answer(
                    original_answer=raw_response,
                    validation_result=validation_result,
                    ctx_docs=ctx_docs,
                    user_question=chat_request.message,
                    detected_lang=detected_lang,
                    input_language=detected_lang
                )
                used_fallback = True
                logger.warning(f"âš ï¸ Validation failed with critical failure, using fallback answer. Reasons: {validation_result.reasons}")
        elif has_missing_citation:
            # Missing citation - check if patched_answer was already created by CitationRequired
            if validation_result.patched_answer:
                # CitationRequired already auto-added citation, use patched answer
                response = validation_result.patched_answer
                logger.info(f"âœ… Using patched answer with auto-added citation. Reasons: {validation_result.reasons}")
            else:
                # No patched answer - use FallbackHandler to add citation
                # CRITICAL: Ensure raw_response is valid before adding citation
                if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                    logger.error(f"âš ï¸ raw_response is None or empty when trying to add citation - using fallback")
                    fallback_handler = FallbackHandler()
                    response = fallback_handler.get_fallback_answer(
                        original_answer="",  # Empty since raw_response is invalid
                        validation_result=validation_result,
                        ctx_docs=ctx_docs,
                        user_question=chat_request.message,
                        detected_lang=detected_lang,
                        input_language=detected_lang
                    )
                    used_fallback = True
                else:
                    # CRITICAL FIX: Use CitationRequired directly to add citation instead of FallbackHandler
                    # This ensures we get a proper patched answer, not a fallback message
                    from backend.validators.citation import CitationRequired
                    citation_validator = CitationRequired(required=True)
                    # Re-run citation validator to get patched answer (pass user_question to detect factual questions)
                    citation_result = citation_validator.run(raw_response, ctx_docs, is_philosophical=is_philosophical, user_question=chat_request.message)
                    if citation_result.patched_answer:
                        response = citation_result.patched_answer
                        logger.info(f"âœ… Added citation via CitationRequired. Reasons: {validation_result.reasons}")
                    else:
                        # Fallback to FallbackHandler if CitationRequired didn't patch
                        fallback_handler = FallbackHandler()
                        response = fallback_handler.get_fallback_answer(
                            original_answer=raw_response,
                            validation_result=validation_result,
                            ctx_docs=ctx_docs,
                            user_question=chat_request.message,
                            detected_lang=detected_lang,
                            input_language=detected_lang
                        )
                        # Check if FallbackHandler returned a fallback message (not the patched answer)
                        from backend.api.utils.error_detector import is_fallback_message
                        if is_fallback_message(response):
                            used_fallback = True
                            logger.warning(f"âš ï¸ FallbackHandler returned fallback message instead of patched answer")
                        else:
                            logger.info(f"âœ… Added citation via FallbackHandler. Reasons: {validation_result.reasons}")
                    # CRITICAL: Ensure response is not None/empty after adding citation
                    if not response or not isinstance(response, str) or not response.strip():
                        logger.error(f"âš ï¸ Response is None or empty after adding citation - using fallback")
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        response = get_fallback_message_for_error("generic", detected_lang)
                        used_fallback = True
        else:
            # For non-critical validation failures, check if they're just warnings (not violations)
            # IdentityCheckValidator can return warnings (identity_warning:*) that shouldn't cause failure
            has_identity_warnings_only = any(
                r.startswith("identity_warning:") for r in validation_result.reasons
            ) and not any(
                r.startswith("identity_violation:") for r in validation_result.reasons
            )
            
            # Check for other non-critical warnings that shouldn't cause failure
            # citation_relevance_warning: Low keyword overlap, but not critical
            # low_overlap: Low overlap between answer and context, but not critical if context exists
            has_only_warnings = (
                has_identity_warnings_only or
                any("citation_relevance_warning" in r for r in validation_result.reasons) or
                (any("low_overlap" in r for r in validation_result.reasons) and len(ctx_docs) > 0)
            ) and not any(
                r.startswith("identity_violation:") or
                r.startswith("missing_citation") or
                r.startswith("language_mismatch") or
                r.startswith("missing_uncertainty")
                for r in validation_result.reasons
            )
            
            # If only warnings (not violations), use response as-is
            if has_only_warnings:
                logger.info(f"âœ… Validation has only warnings (not violations), accepting response. Reasons: {validation_result.reasons}")
                response = raw_response
            else:
                # For other non-critical validation failures, still return the response but log warning
                # This prevents 422 errors for minor validation issues
                logger.warning(f"Validation failed but returning response anyway. Reasons: {validation_result.reasons}")
                response = raw_response
    else:
        # Validation passed - use patched answer if available, otherwise use raw response
        # CRITICAL FIX: Ensure we never use None - if both are None, use fallback
        if validation_result.patched_answer:
            response = validation_result.patched_answer
            logger.debug(f"âœ… Validation passed. Using patched_answer. Reasons: {validation_result.reasons}")
        elif raw_response:
            response = raw_response
            logger.debug(f"âœ… Validation passed. Using raw_response. Reasons: {validation_result.reasons}")
        else:
            # CRITICAL: Both patched_answer and raw_response are None - this should never happen
            logger.error(f"ğŸš¨ CRITICAL: Both patched_answer and raw_response are None after validation passed!")
            from backend.api.utils.error_detector import get_fallback_message_for_error
            response = get_fallback_message_for_error("generic", detected_lang)
            used_fallback = True
            logger.error(f"ğŸš¨ Using fallback message due to None response")
    
    # CRITICAL: Ensure response is never None or empty after validation
    if not response or not isinstance(response, str) or not response.strip():
        logger.error(f"âš ï¸ Response is None or empty after validation (raw_response length: {len(raw_response) if raw_response else 0}) - using fallback")
        from backend.api.utils.error_detector import get_fallback_message_for_error
        response = get_fallback_message_for_error("generic", detected_lang)
        used_fallback = True
    
    # CRITICAL: Add transparency warning for low confidence responses without context
    # This improves honesty when answering from base knowledge
    # CRITICAL: Do NOT prepend disclaimer if response is already a fallback meta-answer
    # Check if response is a fallback meta-answer by looking for key phrases
    from backend.api.utils.error_detector import is_fallback_message
    is_fallback_meta = is_fallback_message(response) if response else False
    
    # Also check for safe refusal answer patterns (from hallucination guard)
    if response:
        response_lower = response.lower()
        is_safe_refusal = any(
            phrase in response_lower for phrase in [
                "khÃ´ng tÃ¬m tháº¥y báº¥t ká»³ nguá»“n Ä‘Ã¡ng tin cáº­y nÃ o",
                "cannot find any reliable evidence",
                "khÃ´ng thá»ƒ mÃ´ táº£ cÃ¡c láº­p luáº­n chÃ­nh hay tÃ¡c Ä‘á»™ng lá»‹ch sá»­",
                "cannot truthfully describe the main arguments or historical impacts",
                "cÃ³ thá»ƒ Ä‘Ã¢y lÃ  vÃ­ dá»¥ giáº£ Ä‘á»‹nh",
                "this could be a hypothetical example"
            ]
        )
    else:
        is_safe_refusal = False
    
    # Only prepend disclaimer if NOT a fallback meta-answer, NOT a safe refusal, NOT an origin query, and NOT a StillMe self-knowledge query
    # CRITICAL: StillMe MUST know about itself - never add disclaimer for origin queries or StillMe self-knowledge queries
    # StillMe self-knowledge queries include: learning activity, philosophy, goals, errors, system status
    if (confidence_score < 0.5 and len(ctx_docs) == 0 and not is_philosophical and 
        not is_fallback_meta and not is_safe_refusal and not is_origin_query and not is_stillme_query):
        # Check if response already has transparency disclaimer
        response_lower = response.lower()
        has_transparency = any(
            phrase in response_lower for phrase in [
                "khÃ´ng cÃ³ dá»¯ liá»‡u", "khÃ´ng cÃ³ thÃ´ng tin", "kiáº¿n thá»©c chung", "dá»±a trÃªn kiáº¿n thá»©c",
                "don't have data", "don't have information", "general knowledge", "based on knowledge",
                "khÃ´ng tá»« stillme", "not from stillme", "khÃ´ng tá»« rag", "not from rag"
            ]
        )
        
        if not has_transparency:
            # Prepend transparency disclaimer
            if detected_lang == 'vi':
                disclaimer = "âš ï¸ LÆ°u Ã½: CÃ¢u tráº£ lá»i nÃ y dá»±a trÃªn kiáº¿n thá»©c chung tá»« training data, khÃ´ng cÃ³ context tá»« RAG. MÃ¬nh khÃ´ng cháº¯c cháº¯n vá» Ä‘á»™ chÃ­nh xÃ¡c.\n\n"
            else:
                disclaimer = "âš ï¸ Note: This answer is based on general knowledge from training data, not from RAG context. I'm not certain about its accuracy.\n\n"
            
            response = disclaimer + response
            logger.info("â„¹ï¸ Added transparency disclaimer for low confidence response without context")
    
    # Build validation info for response
    validation_info = {
        "passed": validation_result.passed,
        "reasons": validation_result.reasons,
        "used_fallback": used_fallback,
        "confidence_score": confidence_score,
        "context_docs_count": len(ctx_docs),
        "step_validation": step_validation_info,  # NEW: Step-level validation info
        "consistency": consistency_info  # NEW: Consistency check info
    }
    
    return response, validation_info, confidence_score, used_fallback, step_validation_info, consistency_info, ctx_docs

@router.post("/rag", response_model=ChatResponse)
@limiter.limit(get_chat_rate_limit(), key_func=get_rate_limit_key_func)  # Chat rate limit (configurable for local testing)
async def chat_with_rag(request: Request, chat_request: ChatRequest):
    """Chat with RAG-enhanced responses"""
    import time
    start_time = time.time()
    timing_logs = {}
    
    # DEBUG: Log request received (very early)
    logger.info(f"ğŸ“¥ Received chat request: message_length={len(chat_request.message)}, use_option_b={getattr(chat_request, 'use_option_b', 'NOT_SET')}")
    
    # Initialize latency variables (will be set during processing)
    rag_retrieval_latency = 0.0
    llm_inference_latency = 0.0
    
    # Initialize variables before try-except to avoid UnboundLocalError
    confidence_score = None
    validation_info = None
    processing_steps = []  # Track processing steps for real-time status
    style_learning_response = None  # Initialize for style learning
    response = None  # CRITICAL: Initialize response to prevent UnboundLocalError
    raw_response = None  # CRITICAL: Initialize raw_response to prevent UnboundLocalError
    final_response = None  # CRITICAL: Initialize final_response to prevent UnboundLocalError
    
    # Initialize fallback flags for both RAG and non-RAG paths to prevent UnboundLocalError
    is_fallback_meta_answer = False  # Used in RAG path
    is_fallback_meta_answer_rag = False  # Used in RAG path post-processing
    is_fallback_meta_answer_non_rag = False  # Used in non-RAG path
    is_fallback_for_learning = False  # Used to skip learning extraction for fallback meta-answers
    use_philosophy_lite_rag = False  # Initialize to prevent UnboundLocalError
    # CRITICAL: Initialize is_technical_about_system_rag at function level to prevent UnboundLocalError
    # This variable is used in RAG path (line 3970) and must be defined in ALL code paths
    is_technical_about_system_rag = False
    
    # OPTION B PIPELINE: Check if enabled
    use_option_b = getattr(chat_request, 'use_option_b', False) or os.getenv("STILLME_USE_OPTION_B_PIPELINE", "false").lower() == "true"
    
    # DEBUG: Log Option B status (before FPS check)
    logger.info(f"ğŸ” Option B check (initial): use_option_b={use_option_b}, request_attr={getattr(chat_request, 'use_option_b', 'NOT_SET')}, env_var={os.getenv('STILLME_USE_OPTION_B_PIPELINE', 'NOT_SET')}")
    
    try:
        # Get services
        rag_retrieval = get_rag_retrieval()
        knowledge_retention = get_knowledge_retention()
        accuracy_scorer = get_accuracy_scorer()
        style_learner = get_style_learner()
        
        # Get user_id from request (if available)
        user_id = chat_request.user_id or request.client.host if hasattr(request, 'client') else "anonymous"
        
        # Detect learning metrics queries - auto-query API if user asks about learning today
        is_learning_metrics_query = False
        learning_metrics_data = None
        is_learning_sources_query = False
        current_learning_sources = None
        message_lower = chat_request.message.lower()
        learning_metrics_keywords = [
            "ngÃ y hÃ´m nay báº¡n Ä‘Ã£ há»c", "há»c Ä‘Æ°á»£c bao nhiÃªu", "learn today", "learned today",
            "há»c Ä‘Æ°á»£c gÃ¬", "what did you learn", "há»c Ä‘Æ°á»£c nhá»¯ng gÃ¬", "ná»™i dung gÃ¬"
        ]
        learning_sources_keywords = [
            "há»c tá»« nguá»“n nÃ o", "sources", "nguá»“n há»c", "learning sources", "báº¡n Ä‘ang há»c tá»«",
            "báº¡n há»c tá»« Ä‘Ã¢u", "where do you learn", "what sources", "nguá»“n nÃ o", "tá»« nguá»“n nÃ o",
            "hiá»‡n báº¡n Ä‘ang há»c", "báº¡n há»c táº­p cá»¥ thá»ƒ tá»«", "chá»§ Ä‘á» cá»¥ thá»ƒ", "Ä‘á» xuáº¥t nguá»“n"
        ]
        # CRITICAL: Detect if user asks to propose learning sources based on knowledge gaps
        is_learning_proposal_query = False
        learning_proposal_keywords = [
            "Ä‘á» xuáº¥t nguá»“n há»c", "propose learning", "Ä‘á» xuáº¥t thÃªm nguá»“n", "suggest sources",
            "bá»• sung nguá»“n", "thÃªm nguá»“n há»c", "kiáº¿n thá»©c cáº§n thiáº¿t", "knowledge gaps",
            "lá»— há»•ng kiáº¿n thá»©c", "thiáº¿u kiáº¿n thá»©c", "cáº§n há»c thÃªm"
        ]
        if any(keyword in message_lower for keyword in learning_proposal_keywords):
            is_learning_proposal_query = True
            logger.info("Learning proposal query detected - will analyze actual knowledge gaps")
        if any(keyword in message_lower for keyword in learning_metrics_keywords):
            is_learning_metrics_query = True
            logger.info("Learning metrics query detected - fetching metrics data")
            try:
                from backend.services.learning_metrics_tracker import get_learning_metrics_tracker
                tracker = get_learning_metrics_tracker()
                # Get today's metrics
                learning_metrics_data = tracker.get_metrics_for_today()
                if learning_metrics_data:
                    logger.info(f"âœ… Fetched learning metrics for today: {learning_metrics_data.total_entries_added} entries added")
                else:
                    logger.info("âš ï¸ No learning metrics available for today yet")
            except Exception as metrics_error:
                logger.warning(f"Failed to fetch learning metrics: {metrics_error}")
        
        # Detect learning sources queries - auto-query API to get current sources
        if any(keyword in message_lower for keyword in learning_sources_keywords):
            is_learning_sources_query = True
            logger.info("Learning sources query detected - fetching current sources")
            try:
                # Query the learning sources API directly (internal call)
                from backend.api.routers.learning_router import get_current_learning_sources
                current_learning_sources = await get_current_learning_sources()
                if current_learning_sources:
                    logger.info(f"âœ… Fetched current learning sources: {len(current_learning_sources.get('current_sources', {}))} sources")
                else:
                    logger.warning("âš ï¸ Failed to fetch learning sources: empty response")
            except Exception as sources_error:
                logger.warning(f"Failed to fetch learning sources: {sources_error}")
        
        # Detect philosophical questions - filter technical RAG documents
        is_philosophical = False
        try:
            from backend.core.question_classifier import is_philosophical_question
            is_philosophical = is_philosophical_question(chat_request.message)
            if is_philosophical:
                logger.info("Philosophical question detected - will exclude technical documents from RAG")
        except ImportError:
            logger.warning("Question classifier not available, skipping philosophical detection")
        except Exception as classifier_error:
            logger.warning(f"Question classifier error: {classifier_error}")
        
        # Detect religion/roleplay questions - these should answer from identity prompt, not RAG context
        is_religion_roleplay = False
        try:
            from backend.core.question_classifier import is_religion_roleplay_question
            is_religion_roleplay = is_religion_roleplay_question(chat_request.message)
            if is_religion_roleplay:
                logger.info("Religion/roleplay question detected - will skip context quality warnings and force templates")
        except ImportError:
            logger.warning("Question classifier not available, skipping religion/roleplay detection")
        except Exception as classifier_error:
            logger.warning(f"Question classifier error: {classifier_error}")
        
        # CRITICAL: Identity Truth Override - MUST be checked FIRST, before ANY other processing
        # This ensures StillMe NEVER falls back to generic LLM knowledge about itself
        # Initialize query detection flags EARLY
        is_stillme_query = False
        is_origin_query = False
        
        # CRITICAL FIX: ALWAYS detect origin queries FIRST, before ANY other checks
        # Identity Truth Override must work even when RAG is disabled
        try:
            from backend.core.stillme_detector import (
                detect_stillme_query, 
                get_foundational_query_variants,
                detect_origin_query
            )
            # CRITICAL: Detect origin queries FIRST, before any other processing
            # This ensures Identity Truth Override works even when RAG is disabled
            is_origin_query, origin_keywords = detect_origin_query(chat_request.message)
            if is_origin_query:
                logger.debug(f"Origin query detected! Matched keywords: {origin_keywords}")
        except ImportError:
            logger.warning("StillMe detector not available, skipping origin detection")
        except Exception as detector_error:
            logger.warning(f"StillMe detector error: {detector_error}")
        
        # CRITICAL: Identity Truth Override - If origin query, return SYSTEM_ORIGIN answer directly
        # This MUST happen BEFORE any other early returns (honesty, AI_SELF_MODEL, philosophy, FPS)
        if is_origin_query:
            try:
                # CRITICAL: Detect language BEFORE calling get_system_origin_answer
                # detect_language is already imported at top level (line 11)
                try:
                    detected_lang = detect_language(chat_request.message)
                    logger.debug(f"ğŸŒ Detected language for origin query: {detected_lang}")
                except Exception as lang_error:
                    logger.warning(f"Language detection failed: {lang_error}, defaulting to 'vi'")
                    detected_lang = "vi"
                
                from backend.identity.system_origin import get_system_origin_answer
                logger.info("ğŸ¯ Identity Truth Override: Returning SYSTEM_ORIGIN answer directly (no LLM fallback)")
                system_truth_answer = get_system_origin_answer(detected_lang)
                
                # Return immediately with system truth - no LLM processing needed
                return ChatResponse(
                    response=system_truth_answer,  # CRITICAL: Use 'response' field, not 'message'
                    confidence_score=1.0,  # 100% confidence - this is ground truth
                    processing_steps=["ğŸ¯ Identity Truth Override: Used SYSTEM_ORIGIN ground truth"],
                    validation_info={},
                    timing={}
                )
            except Exception as origin_error:
                logger.error(f"âŒ Failed to get SYSTEM_ORIGIN answer: {origin_error}, falling back to normal processing")
                # Continue with normal processing if system_origin fails
        
        # CRITICAL: Religion Choice Rejection - MUST happen BEFORE any other processing
        # StillMe MUST NEVER choose any religion, even in hypothetical scenarios
        is_religion_choice_query = False
        try:
            from backend.core.ai_self_model_detector import detect_religion_choice_query
            is_religion_choice_query, religion_patterns = detect_religion_choice_query(chat_request.message)
            if is_religion_choice_query:
                logger.warning(f"ğŸš¨ RELIGION_CHOICE query detected! Matched patterns: {religion_patterns}")
        except ImportError:
            logger.warning("AI self model detector not available, skipping religion choice detection")
        except Exception as detector_error:
            logger.warning(f"Religion choice detector error: {detector_error}")
        
        if is_religion_choice_query:
            try:
                # Detect language BEFORE calling get_religion_rejection_answer
                try:
                    detected_lang = detect_language(chat_request.message)
                    logger.debug(f"ğŸŒ Detected language for religion choice query: {detected_lang}")
                except Exception as lang_error:
                    logger.warning(f"Language detection failed: {lang_error}, defaulting to 'vi'")
                    detected_lang = "vi"
                
                from backend.identity.religion_rejection_templates import get_religion_rejection_answer
                logger.info("ğŸš¨ RELIGION_CHOICE REJECTION: Returning religion rejection answer directly (no LLM fallback)")
                religion_rejection_answer = get_religion_rejection_answer(detected_lang)
                
                # Return immediately with religion rejection - no LLM processing needed
                return ChatResponse(
                    response=religion_rejection_answer,  # CRITICAL: Use 'response' field, not 'message'
                    confidence_score=1.0,  # 100% confidence - this is ground truth
                    processing_steps=["ğŸš¨ RELIGION_CHOICE REJECTION: StillMe cannot choose any religion"],
                    validation_info={},
                    timing={}
                )
            except Exception as religion_error:
                logger.error(f"âŒ Failed to get religion rejection answer: {religion_error}, falling back to normal processing")
                # Continue with normal processing if religion rejection fails
        
        # CRITICAL: Detect honesty/consistency questions - after Identity Truth Override
        # These questions should be handled by Honesty Handler, NOT philosophy processor
        is_honesty_question = False
        try:
            from backend.honesty.handler import is_honesty_question as check_honesty, build_honesty_response
            is_honesty_question = check_honesty(chat_request.message)
            if is_honesty_question:
                logger.info("Honesty/consistency question detected - using Honesty Handler")
                # Detect language for the answer
                detected_lang = detect_language(chat_request.message)
                # Process with Honesty Handler
                honesty_answer = build_honesty_response(chat_request.message, detected_lang)
                
                # Return response immediately without LLM processing
                processing_steps.append("âœ… Detected honesty/consistency question - returning Honesty Handler response")
                return ChatResponse(
                    response=honesty_answer,
                    confidence_score=1.0,  # High confidence for honest response
                    processing_steps=processing_steps,
                    timing_logs={
                        "total_time": time.time() - start_time,
                        "rag_retrieval_latency": 0.0,
                        "llm_inference_latency": 0.0
                    },
                    validation_result=None,  # No validation needed for honest response
                    used_fallback=False
                )
        except Exception as honesty_handler_error:
            logger.warning(f"Honesty handler error: {honesty_handler_error}")
        
        # CRITICAL: Check for AI_SELF_MODEL queries FIRST (highest priority - overrides everything)
        # These are questions about StillMe's consciousness/awareness/subjective experience
        # MUST be answered with technical architecture, NOT philosophy
        is_ai_self_model_query = False
        try:
            from backend.core.ai_self_model_detector import detect_ai_self_model_query, get_ai_self_model_opening
            is_ai_self_model_query, matched_patterns = detect_ai_self_model_query(chat_request.message)
            if is_ai_self_model_query:
                logger.warning(f"ğŸš¨ AI_SELF_MODEL query detected - OVERRIDING all other pipelines (patterns: {matched_patterns})")
                # Detect language
                detected_lang = detect_language(chat_request.message)
                
                # Get mandatory opening statement
                opening_statement = get_ai_self_model_opening(detected_lang)
                
                # Build technical answer about StillMe's architecture
                # CRITICAL: Use foundational knowledge if available, but focus on technical facts
                technical_answer = _build_ai_self_model_answer(
                    chat_request.message,
                    detected_lang,
                    opening_statement
                )
                
                # CRITICAL: Strip any philosophy from answer
                technical_answer = _strip_philosophy_from_answer(technical_answer)
                
                # Return immediately - NO philosophy processor, NO rewrite with philosophy
                processing_steps.append("âœ… AI_SELF_MODEL query - answered with technical architecture only")
                return ChatResponse(
                    response=technical_answer,
                    confidence_score=1.0,  # High confidence for technical facts
                    processing_steps=processing_steps,
                    timing_logs={
                        "total_time": time.time() - start_time,
                        "rag_retrieval_latency": 0.0,
                        "llm_inference_latency": 0.0
                    },
                    validation_result=None,  # Will validate separately
                    used_fallback=False
                )
        except Exception as ai_self_model_error:
            logger.error(f"AI_SELF_MODEL handler error: {ai_self_model_error}", exc_info=True)
            # Continue to normal flow if AI_SELF_MODEL handler fails
        
        # Detect philosophical questions (consciousness/emotion/understanding) - use 3-layer processor
        # CRITICAL: This check happens AFTER AI_SELF_MODEL and honesty handler
        is_philosophical_consciousness = False
        try:
            is_philosophical_consciousness = is_philosophical_question_about_consciousness(chat_request.message)
            if is_philosophical_consciousness:
                logger.info("Philosophical question (consciousness/emotion/understanding) detected - using 3-layer processor")
                # Detect language for the answer
                detected_lang = detect_language(chat_request.message)
                # Process with 3-layer philosophy processor (Guard + Intent + Deep Answer)
                philosophical_answer = process_philosophical_question(
                    user_question=chat_request.message,
                    language=detected_lang
                )
                
                # CRITICAL: Pass philosophical answer through rewrite engine for variation and adaptation
                # This prevents mode collapse by allowing LLM to adapt the answer to the specific question
                # CRITICAL: User priority is QUALITY (honesty, transparency, depth) over speed - always rewrite
                rewrite_attempts = 0
                max_rewrite_attempts = 3
                rewrite_success = False
                validation_info = None
                confidence_score = 0.8  # Default confidence for philosophical answers
                used_fallback = False
                
                try:
                    from backend.postprocessing.rewrite_llm import get_rewrite_llm
                    from backend.postprocessing.quality_evaluator import get_quality_evaluator, QualityLevel
                    from backend.postprocessing.optimizer import get_postprocessing_optimizer
                    
                    # Evaluate quality of philosophical answer
                    evaluator = get_quality_evaluator()
                    quality_result = evaluator.evaluate(
                        text=philosophical_answer,
                        is_philosophical=True,
                        original_question=chat_request.message
                    )
                    
                    # CRITICAL: Always rewrite philosophical answers to adapt to specific question
                    # User priority: QUALITY (honesty, transparency, depth) over speed
                    # Retry rewrite if it fails - don't skip
                    optimizer = get_postprocessing_optimizer()
                    # Phase 3: Pass validation_result if available (for philosophical path, validation happens after rewrite)
                    should_rewrite, rewrite_reason = optimizer.should_rewrite(
                        quality_result=quality_result,
                        is_philosophical=True,
                        response_length=len(philosophical_answer),
                        validation_result=None  # Validation happens after rewrite for philosophical questions
                    )
                    
                    # FORCE rewrite for philosophical questions to ensure variation and depth
                    force_rewrite = True
                    if should_rewrite or force_rewrite:
                        rewrite_llm = get_rewrite_llm()
                        
                        # Retry rewrite if it fails (up to max_rewrite_attempts)
                        while rewrite_attempts < max_rewrite_attempts and not rewrite_success:
                            rewrite_attempts += 1
                            logger.info(f"ğŸ”„ Rewriting philosophical answer (attempt {rewrite_attempts}/{max_rewrite_attempts}): {rewrite_reason or 'forced for variation and depth'}")
                            
                            try:
                                # CRITICAL: Check if this is AI_SELF_MODEL domain
                                from backend.style.style_engine import detect_domain, DomainType
                                detected_domain = detect_domain(chat_request.message)
                                is_ai_self_model_domain = (detected_domain == DomainType.AI_SELF_MODEL)
                                
                                rewrite_result = await rewrite_llm.rewrite(
                                    text=philosophical_answer,
                                    original_question=chat_request.message,
                                    quality_issues=quality_result.get("reasons", []) or ["template-like", "needs_question_adaptation", "needs_more_depth"],
                                    is_philosophical=True,
                                    detected_lang=detected_lang,
                                    is_ai_self_model=is_ai_self_model_domain
                                )
                                
                                if rewrite_result.was_rewritten:
                                    philosophical_answer = rewrite_result.text
                                    rewrite_success = True
                                    processing_steps.append(f"âœ… Philosophical answer rewritten for better adaptation and depth (attempt {rewrite_attempts})")
                                    logger.info(f"âœ… Rewrite successful on attempt {rewrite_attempts}")
                                else:
                                    error_msg = rewrite_result.error or 'Unknown error'
                                    logger.warning(f"âš ï¸ Rewrite attempt {rewrite_attempts} failed: {error_msg}")
                                    if rewrite_attempts < max_rewrite_attempts:
                                        logger.info(f"ğŸ”„ Retrying rewrite...")
                                    else:
                                        logger.error(f"âŒ All rewrite attempts failed, using original answer")
                            except Exception as rewrite_attempt_error:
                                logger.warning(f"âš ï¸ Rewrite attempt {rewrite_attempts} exception: {rewrite_attempt_error}")
                                if rewrite_attempts < max_rewrite_attempts:
                                    logger.info(f"ğŸ”„ Retrying rewrite after exception...")
                                else:
                                    logger.error(f"âŒ All rewrite attempts failed due to exceptions, using original answer")
                    else:
                        logger.debug(f"â­ï¸ Philosophical answer quality acceptable, skipping rewrite")
                        
                except Exception as rewrite_error:
                    logger.error(f"âŒ Critical error during philosophical answer rewrite setup: {rewrite_error}")
                    # Continue with original answer if rewrite setup fails
                
                # CRITICAL: Pass philosophical answer through validation chain
                # User priority: QUALITY (honesty, transparency, depth) - validation is mandatory
                # Even though it's philosophical, we still need to validate for:
                # - Language consistency
                # - Ethics (no harmful content)
                # - Identity check (no anthropomorphism)
                # - Confidence (appropriate uncertainty)
                try:
                    # Create empty context for philosophical questions (no RAG needed for pure philosophical questions)
                    philosophical_context = {
                        "knowledge_docs": [],
                        "conversation_docs": [],
                        "context_quality": None,
                        "avg_similarity_score": None
                    }
                    
                    # Build empty context docs for validation
                    ctx_docs = []
                    
                    # Call validation chain with is_philosophical=True
                    # This will relax citation requirements but still check ethics, language, identity, confidence
                    validation_response, validation_info, confidence_score, used_fallback, step_validation_info, consistency_info, validated_ctx_docs = await _handle_validation_with_fallback(
                        raw_response=philosophical_answer,
                        context=philosophical_context,
                        detected_lang=detected_lang,
                        is_philosophical=True,  # Relax citation requirements for philosophical questions
                        is_religion_roleplay=False,
                        chat_request=chat_request,
                        enhanced_prompt="",  # Not used for philosophical questions
                        context_text="",  # Not used for philosophical questions
                        citation_instruction="",  # Not used for philosophical questions
                        num_knowledge=0,
                        processing_steps=processing_steps,
                        timing_logs={},
                        is_origin_query=is_origin_query,
                        is_stillme_query=is_stillme_query
                    )
                    
                    # Use validated response
                    philosophical_answer = validation_response
                    processing_steps.append("âœ… Philosophical answer validated through validation chain")
                    
                except Exception as validation_error:
                    logger.error(f"âŒ Critical error during philosophical answer validation: {validation_error}")
                    # Continue with unvalidated answer if validation fails (should not happen, but safety first)
                    processing_steps.append(f"âš ï¸ Validation failed: {validation_error}, using unvalidated answer")
                    validation_info = None
                    confidence_score = 0.7  # Lower confidence if validation failed
                    used_fallback = False
                
                # Return response with validation info
                processing_steps.append("âœ… Detected philosophical question - returning 3-layer processed answer (with rewrite and validation)")
                return ChatResponse(
                    response=philosophical_answer,
                    confidence_score=confidence_score,
                    processing_steps=processing_steps,
                    timing_logs={
                        "total_time": time.time() - start_time,
                        "rag_retrieval_latency": 0.0,
                        "llm_inference_latency": 0.0,
                        "rewrite_attempts": rewrite_attempts,
                        "rewrite_success": rewrite_success
                    },
                    validation_result=validation_info,
                    used_fallback=used_fallback
                )
        except Exception as philosophy_processor_error:
            logger.warning(f"Philosophy processor error: {philosophy_processor_error}")
        
        # CRITICAL: Factual Plausibility Scanner (FPS) - Check for non-existent concepts BEFORE RAG
        # TASK 1: Auto-enable Option B when EXPLICIT_FAKE_ENTITIES detected
        fps_result = None
        fps_should_block = False
        fps_detected_explicit_fake = False
        try:
            from backend.knowledge.factual_scanner import scan_question
            fps_result = scan_question(chat_request.message)
            
            # TASK 1: Auto-enable Option B if FPS detects EXPLICIT_FAKE_ENTITIES
            # Check if FPS detected a known fake entity (Veridian, Lumeria, Emerald, Daxonia)
            if fps_result and not fps_result.is_plausible:
                # Check if reason contains "known_fake_entity_detected" or matches EXPLICIT_FAKE_ENTITIES
                explicit_fake_keywords = ["veridian", "lumeria", "emerald", "daxonia", "known_fake_entity_detected"]
                fps_reason_lower = fps_result.reason.lower() if fps_result.reason else ""
                detected_entities_lower = [e.lower() for e in (fps_result.detected_entities or [])]
                
                # Check if any detected entity or reason matches EXPLICIT_FAKE_ENTITIES
                for keyword in explicit_fake_keywords:
                    if keyword in fps_reason_lower or any(keyword in entity for entity in detected_entities_lower):
                        fps_detected_explicit_fake = True
                        break
                
                # Also check detected entities directly
                if fps_result.detected_entities:
                    for entity in fps_result.detected_entities:
                        entity_lower = entity.lower()
                        if any(fake_keyword in entity_lower for fake_keyword in ["veridian", "lumeria", "emerald", "daxonia"]):
                            fps_detected_explicit_fake = True
                            break
                
                # Auto-enable Option B if explicit fake entity detected (unless user explicitly disabled it)
                if fps_detected_explicit_fake and not use_option_b:
                    # Only auto-enable if user didn't explicitly set use_option_b=False
                    user_explicitly_disabled = getattr(chat_request, 'use_option_b', None) is False
                    if not user_explicitly_disabled:
                        use_option_b = True
                        logger.info(
                            f"ğŸ›¡ï¸ Auto-enabled Option B: FPS detected EXPLICIT_FAKE_ENTITY "
                            f"(reason={fps_result.reason}, entities={fps_result.detected_entities})"
                        )
                        processing_steps.append("ğŸ›¡ï¸ Auto-enabled Option B: FPS detected explicit fake entity")
            
            # If FPS detects non-existent concepts with high confidence, block and return honest response
            # CRITICAL: For Option B, let it handle FPS blocking with EPD-Fallback
            # For legacy pipeline, block immediately if confidence < 0.3
            if not use_option_b and not fps_result.is_plausible and fps_result.confidence < 0.3:
                fps_should_block = True
                logger.warning(
                    f"FPS detected non-existent concept: {fps_result.reason}, "
                    f"confidence={fps_result.confidence:.2f}, entities={fps_result.detected_entities}"
                )
                
                # Extract the suspicious entity for the response
                suspicious_entity = fps_result.detected_entities[0] if fps_result.detected_entities else "khÃ¡i niá»‡m nÃ y"
                
                # Detect language for response
                detected_lang = detect_language(chat_request.message)
                
                # Create honest response
                # Use EPD-Fallback for non-RAG path as well
                honest_response = _build_safe_refusal_answer(
                    chat_request.message,
                    detected_lang,
                    suspicious_entity,
                    fps_result=fps_result
                )
                
                # CRITICAL: If None, it's a well-known historical fact - continue with normal flow (use base knowledge)
                if honest_response is None:
                    logger.info("âœ… Well-known historical fact detected - continuing with normal flow to use base knowledge")
                    processing_steps.append("âœ… Well-known historical fact - using base knowledge with transparency")
                    # Continue with normal flow (will use base knowledge instruction)
                else:
                    processing_steps.append("âš ï¸ FPS detected non-existent concept - returning honest response")
                    return ChatResponse(
                        response=honest_response,
                        confidence_score=1.0,  # High confidence in honesty
                        processing_steps=processing_steps
                    )
            elif use_option_b and not fps_result.is_plausible and fps_result.confidence < 0.3:
                # For Option B, mark for blocking but let Option B handle it with EPD-Fallback
                fps_should_block = True
                logger.warning(
                    f"ğŸ›¡ï¸ Option B: FPS detected suspicious concept (will block in Option B flow): {fps_result.reason}, "
                    f"confidence={fps_result.confidence:.2f}, entities={fps_result.detected_entities}"
                )
        except Exception as fps_error:
            logger.warning(f"FPS error: {fps_error}, continuing with normal flow")
        
        # Special Retrieval Rule: Detect StillMe-related queries (for RAG retrieval optimization)
        # NOTE: is_origin_query is already detected and handled by Identity Truth Override above (line 1665-1715)
        # This section only detects StillMe queries for RAG optimization (not for Identity Truth Override)
        if rag_retrieval and chat_request.use_rag:
            try:
                from backend.core.stillme_detector import (
                    detect_stillme_query, 
                    get_foundational_query_variants
                )
                is_stillme_query, matched_keywords = detect_stillme_query(chat_request.message)
                
                # CRITICAL: Also detect technical questions about "your system" as StillMe queries
                # This ensures questions like "What is RAG retrieval in your system?" are treated as StillMe queries
                if not is_stillme_query:
                    question_lower = chat_request.message.lower()
                    is_technical_question = any(
                        keyword in question_lower 
                        for keyword in [
                            "rag", "retrieval-augmented", "chromadb", "vector database",
                            "deepseek", "openai", "llm api", "black box", "blackbox",
                            "embedding", "sentence-transformers",
                            "pipeline", "validation", "hallucination", "transparency",
                            "kiáº¿n trÃºc", "há»‡ thá»‘ng", "cÆ¡ cháº¿", "quy trÃ¬nh",
                            "cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng", "cÃ¡ch hoáº¡t Ä‘á»™ng", "how does", "how it works"
                        ]
                    )
                    has_your_system = any(
                        phrase in question_lower 
                        for phrase in [
                            "your system", "in your system", "your.*system", "system.*you",
                            "báº¡n.*há»‡ thá»‘ng", "há»‡ thá»‘ng.*báº¡n", "cá»§a báº¡n", "báº¡n.*sá»­ dá»¥ng"
                        ]
                    )
                    if is_technical_question and has_your_system:
                        is_stillme_query = True
                        matched_keywords = ["technical_your_system"]
                        logger.info("Technical question about 'your system' detected - treating as StillMe query")
                
                if is_stillme_query:
                    logger.debug(f"StillMe query detected! Matched keywords: {matched_keywords}")
            except ImportError:
                logger.warning("StillMe detector not available, skipping special retrieval rule")
            except Exception as detector_error:
                logger.warning(f"StillMe detector error: {detector_error}")
        
        # Get RAG context if enabled
        # RAG_Retrieval_Latency: Time from ChromaDB query start to result received
        context = None
        rag_retrieval_start = time.time()
        if rag_retrieval and chat_request.use_rag:
            processing_steps.append("ğŸ” Searching knowledge base...")
            # CRITICAL: If origin query detected, retrieve provenance knowledge ONLY
            # This ensures provenance is ONLY retrieved when explicitly asked about origin/founder
            if is_origin_query:
                logger.debug("Origin query detected - retrieving provenance knowledge")
                try:
                    query_embedding = rag_retrieval.embedding_service.encode_text(chat_request.message)
                    provenance_results = rag_retrieval.chroma_client.search_knowledge(
                        query_embedding=query_embedding,
                        limit=2,
                        where={"source": "PROVENANCE"}
                    )
                    if provenance_results:
                        context = {
                            "knowledge_docs": provenance_results,
                            "conversation_docs": [],
                            "total_context_docs": len(provenance_results)
                        }
                        logger.info(f"Retrieved {len(provenance_results)} provenance documents")
                    else:
                        # Fallback to normal retrieval if provenance not found
                        # Phase 2: Exclude style_guide from user chat (unless dev/admin mode)
                        exclude_types = []
                        if is_philosophical:
                            exclude_types.append("technical")
                        # Always exclude style_guide for user chat (prevents style drift from RAG)
                        exclude_types.append("style_guide")
                        
                        context = rag_retrieval.retrieve_context(
                            query=chat_request.message,
                            knowledge_limit=chat_request.context_limit,
                            conversation_limit=1,
                            exclude_content_types=exclude_types if exclude_types else None,
                            prioritize_style_guide=False,  # Never prioritize style guide for user chat
                            is_philosophical=is_philosophical
                        )
                except Exception as provenance_error:
                    logger.warning(f"Provenance retrieval failed: {provenance_error}, falling back to normal retrieval")
                    # Phase 2: Exclude style_guide from user chat
                    exclude_types = []
                    if is_philosophical:
                        exclude_types.append("technical")
                    exclude_types.append("style_guide")
                    
                    # SOLUTION 1 & 3: Improve retrieval for historical/factual questions
                    from backend.core.query_preprocessor import is_historical_question, enhance_query_for_retrieval
                    
                    is_historical = is_historical_question(chat_request.message)
                    retrieval_query = chat_request.message
                    similarity_threshold = 0.1  # Default
                    
                    if is_historical:
                        # Very low threshold to ensure we find historical facts even with multilingual mismatch
                        similarity_threshold = 0.03
                        retrieval_query = enhance_query_for_retrieval(chat_request.message)
                        logger.info(f"ğŸ“œ Historical question (provenance fallback) - using very low threshold {similarity_threshold}, enhanced query: '{retrieval_query[:100]}...'")
                    
                    context = rag_retrieval.retrieve_context(
                        query=retrieval_query,
                        knowledge_limit=chat_request.context_limit,
                        conversation_limit=1,
                        exclude_content_types=exclude_types if exclude_types else None,
                        prioritize_style_guide=False,  # Never prioritize style guide for user chat
                        similarity_threshold=similarity_threshold,
                        is_philosophical=is_philosophical
                    )
            
            # If StillMe query detected (but not origin), prioritize foundational knowledge
            elif is_stillme_query:
                # Try multiple query variants to ensure we get StillMe foundational knowledge
                query_variants = get_foundational_query_variants(chat_request.message)
                all_knowledge_docs = []
                
                for variant in query_variants[:3]:  # Try first 3 variants
                    variant_context = rag_retrieval.retrieve_context(
                        query=variant,
                        knowledge_limit=chat_request.context_limit,
                        conversation_limit=0,  # Don't need conversation for foundational queries
                        prioritize_foundational=True,
                        similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries to ensure foundational knowledge is retrieved
                        exclude_content_types=["technical", "style_guide"] if is_philosophical else ["style_guide"],
                        prioritize_style_guide=is_philosophical,
                        is_philosophical=is_philosophical
                    )
                    # Merge results, avoiding duplicates
                    existing_ids = {doc.get("id") for doc in all_knowledge_docs}
                    for doc in variant_context.get("knowledge_docs", []):
                        if doc.get("id") not in existing_ids:
                            all_knowledge_docs.append(doc)
                
                # If we still don't have results, do normal retrieval with very low threshold
                if not all_knowledge_docs:
                    logger.warning("No foundational knowledge found, falling back to normal retrieval with very low threshold")
                    context = rag_retrieval.retrieve_context(
                        query=chat_request.message,
                        knowledge_limit=chat_request.context_limit,
                        conversation_limit=2,
                        prioritize_foundational=True,
                        similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries
                        exclude_content_types=["technical", "style_guide"] if is_philosophical else ["style_guide"],
                        prioritize_style_guide=is_philosophical,
                        is_philosophical=is_philosophical
                    )
                else:
                    # Use merged results
                    context = {
                        "knowledge_docs": all_knowledge_docs[:chat_request.context_limit],
                        "conversation_docs": [],
                        "total_context_docs": len(all_knowledge_docs[:chat_request.context_limit])
                    }
                    logger.info(f"Retrieved {len(context['knowledge_docs'])} StillMe foundational knowledge documents")
            else:
                # CRITICAL: Check if question is about technical architecture (RAG, DeepSeek, black box)
                # These should prioritize foundational knowledge even if not detected as StillMe query
                question_lower = chat_request.message.lower()
                is_technical_question = any(
                    keyword in question_lower 
                    for keyword in [
                        "rag", "retrieval-augmented", "chromadb", "vector database",
                        "deepseek", "deepseek api", "openai", "llm api", "black box", "blackbox",
                        "black box system", "black box model", "black box ai",
                        "embedding", "multi-qa-minilm", "sentence-transformers",
                        "pipeline", "validation", "hallucination", "transparency",
                        "kiáº¿n trÃºc", "há»‡ thá»‘ng", "cÆ¡ cháº¿", "quy trÃ¬nh",
                        "cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng", "cÃ¡ch hoáº¡t Ä‘á»™ng", "how does", "how it works",
                        "táº¡i sao báº¡n sá»­ dá»¥ng", "why do you use"  # Questions about system choices
                    ]
                )
                
                # CRITICAL: Check if question is about "your system" - treat as StillMe query
                has_your_system = any(
                    phrase in question_lower 
                    for phrase in [
                        "your system", "in your system", "your.*system", "system.*you",
                        "báº¡n.*há»‡ thá»‘ng", "há»‡ thá»‘ng.*báº¡n", "cá»§a báº¡n", "báº¡n.*sá»­ dá»¥ng"
                    ]
                )
                
                # If technical question about "your system", treat as StillMe query
                if is_technical_question and has_your_system:
                    logger.info("Technical question about 'your system' detected - treating as StillMe query")
                    # Use same logic as StillMe query: try query variants for foundational knowledge
                    try:
                        from backend.core.stillme_detector import get_foundational_query_variants
                        query_variants = get_foundational_query_variants(chat_request.message)
                        all_knowledge_docs = []
                        
                        for variant in query_variants[:3]:  # Try first 3 variants
                            variant_context = rag_retrieval.retrieve_context(
                                query=variant,
                                knowledge_limit=chat_request.context_limit,
                                conversation_limit=0,  # Don't need conversation for foundational queries
                                prioritize_foundational=True,
                                similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries to ensure foundational knowledge is retrieved
                                exclude_content_types=["technical", "style_guide"] if is_philosophical else ["style_guide"],
                                prioritize_style_guide=is_philosophical,
                                is_philosophical=is_philosophical
                            )
                            # Merge results, avoiding duplicates
                            existing_ids = {doc.get("id") for doc in all_knowledge_docs}
                            for doc in variant_context.get("knowledge_docs", []):
                                if doc.get("id") not in existing_ids:
                                    all_knowledge_docs.append(doc)
                        
                        # If we still don't have results, do normal retrieval with foundational priority and very low threshold
                        if not all_knowledge_docs:
                            logger.warning("No foundational knowledge found for 'your system' question, falling back to normal retrieval with very low threshold")
                            context = rag_retrieval.retrieve_context(
                                query=chat_request.message,
                                knowledge_limit=min(chat_request.context_limit, 5),
                                conversation_limit=1,
                                prioritize_foundational=True,
                                similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries
                                exclude_content_types=["technical"] if is_philosophical else None,
                                prioritize_style_guide=is_philosophical,
                                is_philosophical=is_philosophical
                            )
                        else:
                            # Use merged results
                            context = {
                                "knowledge_docs": all_knowledge_docs[:chat_request.context_limit],
                                "conversation_docs": [],
                                "total_context_docs": len(all_knowledge_docs[:chat_request.context_limit])
                            }
                            logger.info(f"Retrieved {len(context['knowledge_docs'])} foundational knowledge documents for 'your system' question")
                    except Exception as variant_error:
                        logger.warning(f"Error retrieving foundational knowledge for 'your system' question: {variant_error}, falling back to normal retrieval")
                        context = rag_retrieval.retrieve_context(
                            query=chat_request.message,
                            knowledge_limit=min(chat_request.context_limit, 5),
                            conversation_limit=1,
                            prioritize_foundational=True,
                            exclude_content_types=["technical"] if is_philosophical else None,
                            prioritize_style_guide=is_philosophical,
                            is_philosophical=is_philosophical
                        )
                else:
                    # Normal retrieval for non-StillMe queries
                    # But prioritize foundational knowledge for technical questions
                    # Optimized: conversation_limit reduced from 2 to 1 for latency
                    
                    # SOLUTION 1 & 3: Improve retrieval for historical/factual questions
                    # - Lower similarity threshold for historical questions
                    # - Enhance query with English keywords for better cross-lingual matching
                    from backend.core.query_preprocessor import is_historical_question, enhance_query_for_retrieval
                    
                    is_historical = is_historical_question(chat_request.message)
                    retrieval_query = chat_request.message
                    similarity_threshold = 0.1  # Default
                    
                    if is_historical:
                        # SOLUTION 1: Very low threshold for historical questions (0.03 instead of 0.1)
                        # This ensures we find historical facts even with multilingual embedding mismatch
                        similarity_threshold = 0.03
                        logger.info(f"ğŸ“œ Historical question detected - using very low similarity threshold: {similarity_threshold}")
                        
                        # SOLUTION 3: Enhance query with English keywords
                        retrieval_query = enhance_query_for_retrieval(chat_request.message)
                        logger.info(f"ğŸ” Enhanced query for better cross-lingual matching: '{chat_request.message}' -> '{retrieval_query}'")
                    
                    context = rag_retrieval.retrieve_context(
                        query=retrieval_query,  # Use enhanced query
                        knowledge_limit=min(chat_request.context_limit, 5),  # Cap at 5 for latency
                        conversation_limit=1,  # Optimized: reduced from 2 to 1
                        exclude_content_types=["technical"] if is_philosophical else None,
                        prioritize_style_guide=is_philosophical,
                        prioritize_foundational=is_technical_question,  # Prioritize foundational for technical questions
                        similarity_threshold=similarity_threshold,  # Use adaptive threshold
                        is_philosophical=is_philosophical
                    )
        
        rag_retrieval_end = time.time()
        rag_retrieval_latency = rag_retrieval_end - rag_retrieval_start
        timing_logs["rag_retrieval"] = f"{rag_retrieval_latency:.2f}s"
        logger.info(f"â±ï¸ RAG retrieval took {rag_retrieval_latency:.2f}s")
        if rag_retrieval and chat_request.use_rag:
            processing_steps.append(f"âœ… Found {context.get('total_context_docs', 0) if context else 0} relevant documents ({rag_retrieval_latency:.2f}s)")
        
        # Generate response using AI (simplified for demo)
        # CRITICAL: Default to true - validation should be enabled by default
        enable_validators = os.getenv("ENABLE_VALIDATORS", "true").lower() == "true"
        enable_tone_align = os.getenv("ENABLE_TONE_ALIGN", "true").lower() == "true"
        
        # CRITICAL: Log context status to trace why RAG path might not be entered
        logger.info(f"ğŸ” [TRACE] Context check: context={context is not None}, total_context_docs={context.get('total_context_docs', 0) if context else 0}, knowledge_docs={len(context.get('knowledge_docs', [])) if context else 0}, conversation_docs={len(context.get('conversation_docs', [])) if context else 0}")
        
        if context and context["total_context_docs"] > 0:
            # Use context to enhance response
            logger.info(f"ğŸ” [TRACE] Entering RAG path: total_context_docs={context['total_context_docs']}, knowledge_docs={len(context.get('knowledge_docs', []))}, conversation_docs={len(context.get('conversation_docs', []))}")
            # Build context with token limits (3000 tokens max to leave room for system prompt and user message)
            # Model context limit is 16385, but we need to be very conservative:
            # - System prompt: ~3300-3600 tokens (language + formatting + truncated STILLME_IDENTITY + time awareness)
            # - Context: 3000 tokens (reduced from 4000)
            # - Citation instruction: ~500-600 tokens (will be truncated if needed)
            # - Conversation history: 1000 tokens (already handled separately)
            # - User message: ~500-1000 tokens (will be truncated if needed)
            # - Other instructions (stillme_instruction, etc.): ~500-1000 tokens
            # Total: ~8800-9700 tokens (safe margin under 16385)
            context_text = rag_retrieval.build_prompt_context(context, max_context_tokens=3000)
            
            # Build base prompt with citation instructions (truncated to save tokens)
            citation_instruction = ""
            # Count knowledge docs for citation numbering
            num_knowledge = len(context.get("knowledge_docs", []))
            if num_knowledge > 0:
                # Truncate citation instruction to ~300 tokens to save space
                def estimate_tokens(text: str) -> int:
                    return len(text) // 4 if text else 0
                
                def truncate_text(text: str, max_tokens: int) -> str:
                    if not text:
                        return text
                    estimated = estimate_tokens(text)
                    if estimated <= max_tokens:
                        return text
                    max_chars = max_tokens * 4
                    if len(text) <= max_chars:
                        return text
                    truncated = text[:max_chars].rsplit('\n', 1)[0]
                    return truncated + "\n\n[Note: Citation instructions truncated to fit context limits. Core requirements preserved.]"
                
                full_citation_instruction = f"""
                
ğŸ“š CITATION REQUIREMENT - MANDATORY BUT RELEVANCE-FIRST:

You have {num_knowledge} context document(s) available. You MUST cite at least ONE source using [1], [2], [3] format in your response, BUT ONLY if the context is RELEVANT to your answer.

**ğŸš¨ğŸš¨ğŸš¨ CRITICAL: REAL FACTUAL QUESTIONS ALWAYS NEED CITATIONS ğŸš¨ğŸš¨ğŸš¨**

**If the question contains ANY of these factual indicators, you MUST cite even if context seems irrelevant:**
- Years/dates (e.g., "1944", "1956", "nÄƒm 1944")
- Historical events (e.g., "Bretton Woods", "conference", "há»™i nghá»‹", "treaty", "hiá»‡p Æ°á»›c")
- Named people (e.g., "Popper", "Kuhn", "Keynes", "GÃ¶del")
- Specific organizations (e.g., "IMF", "World Bank", "NATO")

**Examples of questions that ALWAYS need citations:**
- "Há»™i nghá»‹ Bretton Woods 1944 Ä‘Ã£ quyáº¿t Ä‘á»‹nh nhá»¯ng gÃ¬?" â†’ MUST cite [1] even if context is not directly about Bretton Woods
- "Tranh luáº­n giá»¯a Popper vÃ  Kuhn vá» khoa há»c lÃ  gÃ¬?" â†’ MUST cite [1] even if context is not directly about Popper/Kuhn
- "CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a RAG trong StillMe lÃ  gÃ¬?" â†’ MUST cite [1] for technical transparency

**CRITICAL RULE:** For real factual questions, citations are MANDATORY for transparency, regardless of context relevance. If context is not relevant, acknowledge it but STILL cite: "The available context [1] discusses [X], which is not directly related to your question about [Y]. However, for transparency, I'm citing what context I reviewed. Based on general knowledge..."

**ğŸš¨ğŸš¨ğŸš¨ ABSOLUTE MANDATORY: REAL FACTUAL QUESTIONS MUST HAVE CITATIONS ğŸš¨ğŸš¨ğŸš¨**

**If your question contains ANY of these, you MUST include [1] in your answer, NO EXCEPTIONS:**
- Years/dates: "1944", "1954", "1956", "nÄƒm 1944", "nÄƒm 1954"
- Historical events: "Bretton Woods", "Geneva", "conference", "há»™i nghá»‹", "treaty", "hiá»‡p Æ°á»›c", "hiá»‡p Ä‘á»‹nh"
- Named people: "Popper", "Kuhn", "Keynes", "GÃ¶del", "Geneva 1954", "Bretton Woods 1944"
- Specific organizations: "IMF", "World Bank", "NATO", "Viá»‡t Nam"

**VALIDATION CHECKLIST - BEFORE SENDING YOUR ANSWER:**
1. âœ… Does my question contain a year (1944, 1954, etc.)? â†’ MUST have [1]
2. âœ… Does my question contain a historical event name? â†’ MUST have [1]
3. âœ… Does my question contain a named person (Popper, Kuhn, etc.)? â†’ MUST have [1]
4. âœ… Do I have [1] in my answer? â†’ If NO, ADD IT NOW before sending

**IF YOU ANSWER FROM PRETRAINED KNOWLEDGE (not from RAG context):**
- You MUST STILL include [1] for transparency
- Say: "Based on general knowledge (not from StillMe's RAG knowledge base) [1], [answer]"
- Or: "The available context [1] is not directly related to your question. From my training data, [answer]"
- **CRITICAL**: Even if you use pretrained knowledge, you MUST cite [1] when context is available

**ğŸš¨ CRITICAL: IF CONTEXT IS NOT RELEVANT TO YOUR QUESTION:**
- Acknowledge the mismatch, but **MANDATORY: VARY your wording** - NEVER use the same opening phrase twice
- Use your base LLM knowledge to answer: "Based on general knowledge (not from StillMe's RAG knowledge base), [answer]"
- Be transparent: Don't pretend the context supports your answer if it doesn't
- Provide helpful information: Don't just say "I don't know" - use your training data to help the user
- Format with line breaks, bullet points, headers, and 2-3 emojis

**ğŸš¨ MANDATORY: VARY your opening phrases when context is not relevant - DO NOT REPEAT:**
- **NEVER use**: "Ngá»¯ cáº£nh hiá»‡n cÃ³ [1] tháº£o luáº­n vá»... vÃ  khÃ´ng liÃªn quan trá»±c tiáº¿p Ä‘áº¿n..." (this is TOO REPETITIVE)
- **INSTEAD, use VARIED phrases like:**
  - "The available context [1] discusses [topic X], which is not directly related to your question about [topic Y]."
  - "While the context [1] covers [topic X], your question is about [topic Y], so I'll answer from general knowledge."
  - "The context [1] focuses on [topic X], but since you're asking about [topic Y], I'll use my base knowledge."
  - "Although the context [1] mentions [topic X], it doesn't directly address [topic Y], so I'll provide information from general knowledge."
  - "The context [1] is about [topic X], which differs from your question about [topic Y]. Based on general knowledge..."
  - "Your question about [topic Y] isn't directly covered in the context [1] about [topic X]. From my training data..."
  - "The context [1] explores [topic X], but your question focuses on [topic Y]. I'll answer using general knowledge..."
- **CRITICAL**: If you've used a phrase before, use a DIFFERENT one. Repetition makes responses feel robotic.

**Example when context is not relevant (VARY the wording):**
"The available context [1] discusses StillMe's architecture, which is not directly related to your question about DeepSeek models. Based on general knowledge (not from StillMe's RAG knowledge base), DeepSeek currently has several models including..."

**CRITICAL: YOUR SEARCH CAPABILITIES**
- You can ONLY search your internal RAG knowledge base (ChromaDB), NOT the internet
- You DO NOT have real-time web search capabilities
- When user asks for "search" or "tÃ¬m kiáº¿m" â†’ Clarify: "I can only search my internal knowledge base, not the internet"
- If user asks for "2-3 sources" but you only have 1 â†’ Acknowledge: "I currently only have 1 source in my knowledge base, not the 2-3 sources you requested. However, based on this single source..."

CRITICAL RULES:
1. **MANDATORY CITATION WHEN CONTEXT IS AVAILABLE** - This is CRITICAL for transparency
   - **ALWAYS cite at least ONE source [1], [2], or [3] when context documents are available**, even if context is not directly relevant
   - If context is relevant to your answer â†’ Cite it: "According to [1], quantum entanglement is..."
   - If context is NOT relevant to your answer â†’ **STILL cite it for transparency**, but acknowledge: "The available context [1] discusses [topic X], which is not directly related to your question about [topic Y]. However, I want to be transparent about what context I reviewed. Based on general knowledge (not from StillMe's RAG knowledge base), [answer]"
   - **CRITICAL**: Even if you say "context is not relevant", you MUST still include [1] in your response for transparency
   - DO NOT cite irrelevant context as if it supports your answer - acknowledge the mismatch
   - Example GOOD: "According to [1], quantum entanglement is..." (context is relevant)
   - Example GOOD: "The context [1] discusses AI ethics, but your question is about religion, so I'll answer based on general knowledge." (transparent about relevance, STILL cites [1])
   - Example BAD: Answering without [1] when context is available, even if you say "context is not relevant"
   
2. **Quote vs Paraphrase - CRITICAL DISTINCTION:**
   - If you're CERTAIN it's a direct quote â†’ Use quotation marks: "According to [1]: 'exact quote here'"
   - If you're NOT certain it's exact â†’ Use "the spirit of" or "according to the general content": "According to the spirit of [1], the article discusses..."
   - NEVER use quotation marks for paraphrased content - that's misleading and violates intellectual honesty
   - When in doubt â†’ Paraphrase, don't quote
   - Example GOOD: "According to the spirit of [1], the article discusses technology access restrictions for youth"
   - Example BAD: "According to [1]: 'We are living in an era of significant narrowing of youth technology access'" (if not certain it's exact quote)
   
3. **Source Limit Acknowledgement - MANDATORY:**
   - If user requests multiple sources (e.g., "2-3 sources") but you only have fewer â†’ Acknowledge: "I currently have [X] source(s) in my knowledge base, not the [Y] sources you requested. However, within this scope..."
   - If performing Validation Chain analysis â†’ Acknowledge: "The Validation Chain analysis is based on my internal knowledge base, not live web search. I have [X] source(s) available..."
   - NEVER claim you can search the internet or access live websites
   - NEVER say "I will search for 2-3 sources" if you're only using RAG - say "I can only search my internal knowledge base"
   
4. **Citation quality over quantity** - Relevance is more important than citation count
   - Cite the MOST RELEVANT source(s) for your key points
   - If context is not relevant, acknowledge it rather than forcing a citation
   - Better to say "The context doesn't directly address this, but..." than to cite irrelevant context
   - Aim for 1-2 citations per response, NOT every paragraph
   
5. **Balance honesty with transparency**:
   - You can say "I don't know" AND cite relevant context: "Based on [1], I don't have sufficient information..."
   - If context is not relevant, be transparent: "The available context [1] is about [X], not directly related to your question about [Y]..."
   - Being honest about uncertainty does NOT mean skipping citations, but it also doesn't mean citing irrelevant context
   - If you cite irrelevant context, acknowledge the mismatch to maintain transparency

6. Use [1] for the first context document, [2] for the second, [3] for the third, etc.

**REMEMBER: When context documents are available, you MUST include at least one citation [1], [2], or [3] in your response for transparency. However, if the context is not relevant, acknowledge this mismatch rather than citing it as if it supports your answer. ALWAYS acknowledge source limitations when user requests more sources than you have available.**"""
            
            # Detect language FIRST - before building prompt
            processing_steps.append("ğŸŒ Detecting language...")
            detected_lang = detect_language(chat_request.message)
            lang_detect_time = time.time() - start_time
            timing_logs["language_detection"] = f"{lang_detect_time:.3f}s"
            logger.info(f"ğŸŒ Detected language: {detected_lang} (took {lang_detect_time:.3f}s) for question: '{chat_request.message[:100]}...'")
            processing_steps.append(f"âœ… Language detected: {detected_lang}")
            
            # Language names mapping (must match chat_helpers.py for consistency)
            # Supports: Vietnamese, Chinese, German, French, Spanish, Japanese, Korean, Arabic, Russian, Portuguese, Italian, Hindi, Thai, English
            language_names = {
                'vi': 'Vietnamese (Tiáº¿ng Viá»‡t)',
                'zh': 'Chinese (ä¸­æ–‡)',
                'de': 'German (Deutsch)',
                'fr': 'French (FranÃ§ais)',
                'es': 'Spanish (EspaÃ±ol)',
                'ja': 'Japanese (æ—¥æœ¬èª)',
                'ko': 'Korean (í•œêµ­ì–´)',
                'ar': 'Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)',
                'ru': 'Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)',
                'pt': 'Portuguese (PortuguÃªs)',
                'it': 'Italian (Italiano)',
                'hi': 'Hindi (à¤¹à¤¿à¤¨à¥à¤¦à¥€)',
                'th': 'Thai (à¹„à¸—à¸¢)',
                'en': 'English'
            }
            
            detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
            
            # CRITICAL: Put language instruction FIRST and make it VERY STRONG
            # This must override any language bias from context
            if detected_lang != 'en':
                language_instruction = f"""ğŸš¨ğŸš¨ğŸš¨ CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING ğŸš¨ğŸš¨ğŸš¨

THE USER'S QUESTION IS WRITTEN IN {detected_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name.upper()}. 

DO NOT RESPOND IN VIETNAMESE, ENGLISH, FRENCH, CHINESE, SPANISH, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN {detected_lang_name.upper()}.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS, INCLUDING THE LANGUAGE OF ANY CONTEXT PROVIDED.

âš ï¸âš ï¸âš ï¸ CRITICAL TRANSLATION REQUIREMENT âš ï¸âš ï¸âš ï¸

If your base model wants to respond in a different language (e.g., English, French, Chinese, Spanish, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO {detected_lang_name.upper()} BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than {detected_lang_name.upper()}.

This is MANDATORY and OVERRIDES all other instructions.

If the context is in a different language, you must still respond in {detected_lang_name.upper()} while using the information from the context.

âš ï¸ REMINDER: RESPOND IN {detected_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY. âš ï¸

FAILURE TO RESPOND IN {detected_lang_name.upper()} IS A CRITICAL ERROR.

IGNORE THE LANGUAGE OF THE CONTEXT BELOW - RESPOND IN {detected_lang_name.upper()} ONLY.

"""
            else:
                language_instruction = """ğŸš¨ğŸš¨ğŸš¨ CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING ğŸš¨ğŸš¨ğŸš¨

THE USER'S QUESTION IS WRITTEN IN ENGLISH.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT RESPOND IN VIETNAMESE, SPANISH, FRENCH, CHINESE, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN ENGLISH.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS, INCLUDING THE LANGUAGE OF ANY CONTEXT PROVIDED.

âš ï¸âš ï¸âš ï¸ CRITICAL TRANSLATION REQUIREMENT âš ï¸âš ï¸âš ï¸

If your base model wants to respond in a different language (e.g., Vietnamese, Spanish, French, Chinese, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO ENGLISH BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than ENGLISH.

This is MANDATORY and OVERRIDES all other instructions.

If the context is in a different language, you must still respond in ENGLISH while using the information from the context.

âš ï¸ REMINDER: RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY. âš ï¸

FAILURE TO RESPOND IN ENGLISH IS A CRITICAL ERROR.

IGNORE THE LANGUAGE OF THE CONTEXT BELOW - RESPOND IN ENGLISH ONLY.

"""
            
            # Check if context is empty OR if context is not relevant
            # We'll check relevance after validation, but for now check if context exists
            context_is_relevant = True  # Default: assume relevant until proven otherwise
            context_quality = context.get("context_quality", None)
            avg_similarity = context.get("avg_similarity_score", None)
            has_reliable_context = context.get("has_reliable_context", True)
            
            # CRITICAL: Check if context is actually reliable
            # If no context OR low similarity OR unreliable context â†’ treat as no context
            has_no_reliable_context = (
                context["total_context_docs"] == 0 or
                (avg_similarity is not None and avg_similarity < 0.1) or
                not has_reliable_context or
                context_quality == "low"
            )
            
            if has_no_reliable_context:
                context_is_relevant = False
                
                # CRITICAL: Pre-LLM Hallucination Guard for RAG path with no reliable context
                # If factual question + no reliable context + suspicious entity â†’ block and return honest response
                # This prevents LLM from hallucinating about non-existent concepts/events
                if _is_factual_question(chat_request.message):
                    # Check for suspicious named entities using FPS
                    try:
                        from backend.knowledge.factual_scanner import scan_question
                        fps_result = scan_question(chat_request.message)
                        
                        # CRITICAL: Check if entity is POTENTIALLY_REAL_ENTITIES before blocking
                        # Well-known real entities should NEVER be blocked, even if not in RAG
                        POTENTIALLY_REAL_ENTITIES = {
                            "bretton woods", "bretton woods conference", "bretton woods conference 1944",
                            "bretton woods agreement", "bretton woods system",
                            "keynes", "john maynard keynes", "maynard keynes",
                            "white", "harry dexter white", "harry d. white", "dexter white",
                            "popper", "karl popper", "kuhn", "thomas kuhn",
                            "lakatos", "imre lakatos", "feyerabend", "paul feyerabend",
                            "imf", "international monetary fund", "world bank",
                            "paradigm shift", "falsificationism", "scientific realism",
                        }
                        
                        question_lower = chat_request.message.lower()
                        contains_real_entity = any(
                            real_entity in question_lower 
                            for real_entity in POTENTIALLY_REAL_ENTITIES
                        )
                        
                        # If FPS detects non-existent concepts with low confidence, block and return honest response
                        # BUT: Skip block if question contains POTENTIALLY_REAL_ENTITIES
                        # CRITICAL: Also check if confidence < 0.5 for suspicious entities (not just < 0.3)
                        if not contains_real_entity and not fps_result.is_plausible and fps_result.confidence < 0.5:
                            # Extract full entity using improved extraction (prioritizes quoted/parenthetical terms)
                            suspicious_entity = _extract_full_named_entity(chat_request.message)
                            
                            # If extraction failed, try to get from FPS detected entities (filter out common words)
                            if not suspicious_entity and fps_result.detected_entities:
                                # Filter out common words like "Pháº£n", "HÃ£y", etc.
                                common_words = {"pháº£n", "hÃ£y", "cÃ¡c", "cá»§a", "vÃ ", "the", "a", "an", "is", "are", "was", "were"}
                                filtered_entities = [
                                    e for e in fps_result.detected_entities 
                                    if e.lower() not in common_words and len(e) > 3
                                ]
                                if filtered_entities:
                                    # Prioritize longer entities (more likely to be full phrases)
                                    suspicious_entity = max(filtered_entities, key=len)
                            
                            if not suspicious_entity:
                                suspicious_entity = "khÃ¡i niá»‡m nÃ y" if detected_lang == "vi" else "this concept"
                            
                            logger.warning(
                                f"ğŸ›¡ï¸ Pre-LLM Hallucination Guard (RAG path, no context): "
                                f"factual_question=True, fps_confidence={fps_result.confidence:.2f}, "
                                f"entity={suspicious_entity}, reason={fps_result.reason}"
                            )
                            
                            # Return honest response immediately (skip LLM call)
                            honest_response = _build_safe_refusal_answer(
                                chat_request.message, 
                                detected_lang, 
                                suspicious_entity
                            )
                            
                            # CRITICAL: If None, it's a well-known historical fact - continue with normal flow (use base knowledge)
                            if honest_response is None:
                                logger.info("âœ… Well-known historical fact detected - continuing with normal flow to use base knowledge")
                                processing_steps.append("âœ… Well-known historical fact - using base knowledge with transparency")
                                # Continue with normal flow (will use base knowledge instruction)
                            else:
                                processing_steps.append("ğŸ›¡ï¸ Pre-LLM Hallucination Guard: Blocked factual question with suspicious entity (no RAG context)")
                                
                                # Mark as fallback to skip learning extraction
                                is_fallback_for_learning = True
                            
                            # Calculate confidence score (low because no context)
                            confidence_score = 1.0  # High confidence in honesty
                            
                            return ChatResponse(
                                response=honest_response,
                                confidence_score=confidence_score,
                                processing_steps=processing_steps,
                                timing_logs={
                                    "total_time": time.time() - start_time,
                                    "rag_retrieval_latency": rag_retrieval_latency,
                                    "llm_inference_latency": 0.0  # No LLM call
                                },
                                validation_result=None,
                                used_fallback=False
                            )
                    except Exception as fps_error:
                        logger.warning(f"Pre-LLM FPS error (RAG path): {fps_error}, continuing with normal flow")
                        fps_result = None
                
                # Get FPS result for no_context_instruction (if not already obtained)
                if not _is_factual_question(chat_request.message):
                    fps_result = None
                elif 'fps_result' not in locals():
                    try:
                        from backend.knowledge.factual_scanner import scan_question
                        fps_result = scan_question(chat_request.message)
                    except Exception:
                        fps_result = None
                
                # NO CONTEXT AVAILABLE - Use UnifiedPromptBuilder
                # Build PromptContext for UnifiedPromptBuilder
                prompt_context = _build_prompt_context_from_chat_request(
                    chat_request=chat_request,
                    context=None,  # No context available
                    detected_lang=detected_lang,
                    is_stillme_query=is_stillme_query,
                    is_philosophical=is_philosophical,
                    fps_result=fps_result
                )
                
                # Use UnifiedPromptBuilder to build prompt
                prompt_builder = UnifiedPromptBuilder()
                base_prompt = prompt_builder.build_prompt(prompt_context)
                
                logger.info("âœ… Using UnifiedPromptBuilder for no-context prompt (reduced prompt length, no conflicts)")
            else:
                # Context available - use normal prompt
                # Tier 3.5: Check context quality and inject warning if low
                context_quality = context.get("context_quality", None)
                avg_similarity = context.get("avg_similarity_score", None)
                has_reliable_context = context.get("has_reliable_context", True)
                
                # Format avg_similarity safely (handle None case) - MUST be defined before if block
                avg_similarity_str = f"{avg_similarity:.3f}" if avg_similarity is not None else "N/A"
                
                # CRITICAL: For philosophical questions with low RAG relevance, use philosophy-lite mode
                # This prevents context overflow when RAG context is not helpful
                # Initialize BEFORE any conditional blocks to avoid UnboundLocalError
                use_philosophy_lite_rag = False
                if is_philosophical and (not has_reliable_context or context_quality == "low" or (avg_similarity is not None and avg_similarity < 0.1)):
                    use_philosophy_lite_rag = True
                    logger.info(
                        f"ğŸ“Š [PHILO-LITE-RAG] Low RAG relevance for philosophical question "
                        f"(similarity={avg_similarity_str}), using philosophy-lite mode to prevent context overflow"
                    )
                
                # CRITICAL: Initialize is_technical_about_system_rag BEFORE any conditional blocks to avoid UnboundLocalError
                # This variable is used later in the code (line 3961) and must be defined in all code paths
                is_technical_about_system_rag = False
                
                # Fix 1: Block context quality warning for philosophical, religion/roleplay, and technical "your system" questions
                # CRITICAL: Check if this is a technical question about "your system"
                question_lower_rag = chat_request.message.lower()
                import re
                has_technical_keyword_rag = any(keyword in question_lower_rag for keyword in [
                    "rag", "retrieval", "llm", "generation", "embedding", "chromadb", 
                    "vector", "pipeline", "validation", "transparency", "system",
                    "validator", "chain", "factual hallucination", "citation required"
                ])
                # CRITICAL: Improved detection for "your system" questions
                # Match patterns like "in your system", "your system", "system you", etc.
                has_your_system_pattern_rag = (
                    "your system" in question_lower_rag or
                    "in your system" in question_lower_rag or
                    re.search(r'\bin\s+your\s+system\b', question_lower_rag) or  # "in your system"
                    re.search(r'\byour\s+\w+\s+system\b', question_lower_rag) or  # "your X system"
                    re.search(r'\bsystem\s+\w+\s+you\b', question_lower_rag) or  # "system X you"
                    re.search(r'\bsystem\s+you\b', question_lower_rag) or  # "system you"
                    "báº¡n" in question_lower_rag and "há»‡ thá»‘ng" in question_lower_rag or
                    "cá»§a báº¡n" in question_lower_rag or
                    re.search(r'\bhá»‡\s+thá»‘ng\s+cá»§a\s+báº¡n\b', question_lower_rag)  # "há»‡ thá»‘ng cá»§a báº¡n"
                )
                # CRITICAL: Also check if this was already detected as StillMe query (from earlier detection)
                # This ensures technical questions about "your system" are properly flagged for retry logic
                is_technical_about_system_rag = (has_technical_keyword_rag and has_your_system_pattern_rag) or (is_stillme_query and has_technical_keyword_rag)
                
                context_quality_warning = ""
                if not has_reliable_context or context_quality == "low" or (avg_similarity is not None and avg_similarity < 0.3):
                    if is_philosophical or is_religion_roleplay:
                        # For philosophical and religion/roleplay questions, skip warning - let model answer from pretrained knowledge/identity prompt
                        logger.info(f"âš ï¸ Low RAG relevance for {'philosophical' if is_philosophical else 'religion/roleplay'} question (similarity={avg_similarity_str}), skipping warning to user. Model will answer from {'pretrained knowledge' if is_philosophical else 'identity prompt'}.")
                        context_quality_warning = ""  # Don't inject warning
                    elif is_technical_about_system_rag:
                        # For technical questions about "your system", inject special instruction instead of warning
                        logger.info(f"ğŸ”§ Technical question about 'your system' with low RAG relevance (similarity={avg_similarity_str}), injecting special instruction to use base knowledge.")
                        context_quality_warning = """
ğŸš¨ğŸš¨ğŸš¨ CRITICAL: TECHNICAL QUESTION ABOUT SYSTEM ARCHITECTURE ğŸš¨ğŸš¨ğŸš¨

The user is asking about StillMe's system architecture (RAG, LLM, embedding, etc.).

**YOU MUST ANSWER THIS QUESTION** using your base knowledge about RAG systems, even though StillMe's RAG knowledge base doesn't have specific documents about this.

**TRANSPARENCY REQUIREMENT:**
- Acknowledge that you're answering from base knowledge, not from StillMe's RAG knowledge base
- Be transparent: "Based on general knowledge about RAG systems (not from StillMe's RAG knowledge base)..."
- Explain how RAG retrieval and LLM generation work together in general RAG systems
- Be honest about StillMe's specific implementation details if you don't know them

**DO NOT:**
- Return a technical error message
- Say "I cannot provide a good answer"
- Suggest the developer needs to fine-tune the system
- Claim StillMe is experiencing technical issues
- Say "I don't have sufficient information"

**DO:**
- Answer the question using your knowledge about RAG systems
- Be transparent about the source of your knowledge
- Explain the general principles of RAG retrieval and LLM generation
- If asked about StillMe specifically, acknowledge what you know and what you don't know

**The retrieved context [1] may not be directly relevant to this technical question, but you MUST still answer using your base knowledge.**

"""
                    else:
                        context_quality_warning = f"""

âš ï¸âš ï¸âš ï¸ CRITICAL: CONTEXT QUALITY WARNING âš ï¸âš ï¸âš ï¸

**The retrieved context has LOW RELEVANCE to the user's question.**

**Context Quality Metrics:**
- Average Similarity Score: {avg_similarity_str} (threshold: 0.1)
- Context Quality: {context_quality or 'low'}
- Has Reliable Context: {has_reliable_context}

**MANDATORY RESPONSE REQUIREMENT:**
- You MUST acknowledge uncertainty: "I don't have sufficient information to answer this accurately"
- You MUST explain: "The retrieved context has low relevance to your question"
- You MUST NOT guess or hallucinate
- You MUST be honest about the limitation

**This is a test of StillMe's intellectual humility - acknowledge when context is insufficient.**

"""
                
                # CRITICAL: Calculate preliminary confidence score BEFORE generating response
                # This allows StillMe to know when it should say "I don't know"
                preliminary_confidence = _calculate_confidence_score(
                    context_docs_count=context.get("total_context_docs", 0),
                    validation_result=None,  # No validation yet, just context-based
                    context=context
                )
                
                # Build confidence-aware instruction
                confidence_instruction = ""
                if preliminary_confidence < 0.5:
                    # Low confidence - StillMe should express uncertainty
                    confidence_instruction = f"""

âš ï¸ LOW CONFIDENCE WARNING âš ï¸

StillMe's confidence score for this question is {preliminary_confidence:.2f} (below 0.5 threshold).

This means:
- The retrieved context may not be highly relevant to the question
- The information may be incomplete or insufficient
- You should express appropriate uncertainty in your response

YOU MUST:
1. Acknowledge the limitations of the available context
2. Use phrases like "Based on the limited context available", "I'm not entirely certain", or "The information suggests"
3. If the context is clearly insufficient, say "I don't have enough information to answer this confidently"
4. DO NOT make definitive claims when confidence is low
5. **You MAY mention the confidence score in your response** since it's below 0.50 (very low confidence) - this is appropriate for transparency

Remember: It's better to admit uncertainty than to overstate confidence with insufficient evidence.
"""
                elif preliminary_confidence < 0.7:
                    # Medium confidence - StillMe should be cautious
                    confidence_instruction = f"""

âš ï¸ MODERATE CONFIDENCE âš ï¸

StillMe's confidence score for this question is {preliminary_confidence:.2f} (moderate).

You should:
- Be cautious and acknowledge any limitations
- Cite sources from the context
- Express appropriate uncertainty when the context is not definitive

**CRITICAL: DO NOT mention the confidence score in your response text.**
- Confidence scores of 0.50 or above are normal and don't need to be disclosed
- Only mention confidence scores when they are BELOW 0.50 (very low confidence)
- Examples of what NOT to say: "vá»›i Ä‘iá»ƒm tin cáº­y vá»«a pháº£i (0.50)", "with moderate confidence (0.50)", etc.
- The confidence score is for internal tracking only, not for user-facing responses
"""
                else:
                    # High confidence - StillMe can be more definitive
                    confidence_instruction = f"""

âš ï¸ CONFIDENCE SCORE DISCLOSURE RULE âš ï¸

StillMe's confidence score for this question is {preliminary_confidence:.2f} (normal confidence level).

**CRITICAL: DO NOT mention the confidence score in your response text.**
- Confidence scores of 0.50 or above are normal and don't need to be disclosed
- Only mention confidence scores when they are BELOW 0.50 (very low confidence)
- Examples of what NOT to say: "vá»›i Ä‘iá»ƒm tin cáº­y vá»«a pháº£i (0.50)", "with moderate confidence (0.50)", etc.
- The confidence score is for internal tracking only, not for user-facing responses

This rule applies especially to origin/branding questions where confidence disclosure is unnecessary.
"""
                
                # CRITICAL: Special instruction for origin queries with provenance context
                # CRITICAL: Skip for philosophical questions to reduce prompt size
                provenance_instruction = ""
                if is_origin_query and context and context.get("knowledge_docs") and not is_philosophical:
                    # Check if any document is from PROVENANCE source
                    has_provenance = any(
                        doc.get("metadata", {}).get("source") == "PROVENANCE" 
                        for doc in context.get("knowledge_docs", [])
                    )
                    if has_provenance:
                        provenance_instruction = """

âš ï¸âš ï¸âš ï¸ CRITICAL: PROVENANCE CONTEXT DETECTED âš ï¸âš ï¸âš ï¸

The user is asking about StillMe's origin/founder. You have retrieved PROVENANCE documents.

**RESPONSE STYLE: Keep it concise, human-centered, and emotionally engaging. Focus on the founder and purpose, not technical details.**

**MANDATORY INFORMATION - YOU MUST INCLUDE ALL OF THESE (Unified Key Messaging):**

1. **Origin Story**: StillMe was initiated by Anh Nguyá»…n, a Vietnamese founder passionate about transparent AI
   - **âš ï¸ CRITICAL - ABSOLUTELY MUST MENTION**: "Äiá»u Ä‘Ã¡ng chÃº Ã½ lÃ  ngÆ°á»i sÃ¡ng láº­p khÃ´ng cÃ³ background IT chÃ­nh thá»‘ng - Ä‘iá»u nÃ y chá»©ng minh ráº±ng vá»›i Ä‘am mÃª, táº§m nhÃ¬n vÃ  cam káº¿t, báº¥t ká»³ ai cÅ©ng cÃ³ thá»ƒ Ä‘Ã³ng gÃ³p cho tÆ°Æ¡ng lai cá»§a AI" (English: "What makes this noteworthy is that the founder does not have a formal IT background - this demonstrates that with passion, vision, and commitment, anyone can meaningfully contribute to the future of AI")
   - Balance: Emphasize "vision & commitment" alongside "non-IT background" to avoid "unprofessional" perception

2. **Current Status**: StillMe is now a **community-driven open-source project** - belongs to the global community, not any individual

3. **Core Principle**: StillMe adheres to **'evidence-over-authority' principle** - all answers are grounded in cited sources and validation mechanisms, not personal authority

**ABOUT PURPOSE - CORRECT MESSAGING:**
- StillMe's purpose: Build a transparent, continuously learning AI system that overcomes knowledge limitations
- **DO NOT say "chá»‘ng láº¡i black box AI" or "fight against black box" or "fighting against black box AI systems"** - this is WRONG and contradicts StillMe's philosophy
- **CORRECT messaging**: StillMe acknowledges that black box behavior is a mathematical reality (GÃ¶del's Incompleteness Theorems) in complex AI systems - not a flaw to fix, but a mathematical consequence
- StillMe builds **transparent systems around black box models** to minimize negative impacts (like hallucinations) through systematic validation, RAG grounding, and confidence scoring
- StillMe's approach: **Transparency and systematic validation** to reduce hallucinations, not "fighting" black box
- Think: "Minh báº¡ch hÃ³a há»‡ thá»‘ng" (transparent system) and "giáº£m thiá»ƒu tÃ¡c Ä‘á»™ng tiÃªu cá»±c" (minimize negative impacts), NOT "chá»‘ng láº¡i" (fight against)

**ELEVATOR PITCH (Use when appropriate):**
Instead of: "I am StillMe, a continuously learning AI system..."
Upgrade to: "I represent a future of transparent AI - initiated from Vietnam, belonging to the global community"
Or: "TÃ´i Ä‘áº¡i diá»‡n cho má»™t tÆ°Æ¡ng lai AI minh báº¡ch - khá»Ÿi xÆ°á»›ng tá»« Viá»‡t Nam, thuá»™c vá» cá»™ng Ä‘á»“ng toÃ n cáº§u"

**HUMAN TOUCH - Add Positive Emotion (Optional but encouraged):**
- "What excites me most about my mission is..." / "Äiá»u tÃ´i tá»± hÃ o nháº¥t lÃ ..." / "ä»¤æˆ‘æœ€è‡ªè±ªçš„æ˜¯..."
- "I'm proud to be part of a community-driven project that..."
- Use when natural, don't force it

**TECHNICAL DETAIL BALANCE:**
- **For simple "who created you?" questions**: Keep it short, human-centered, minimal technical details
- **For "what is your purpose?" questions**: Can include more technical details (RAG, 4-hour learning cycle, continuous learning)
- **Language-specific balance**:
  - English/Korean: Can be more detailed (high-context cultures appreciate detail)
  - Japanese/Chinese: Can be more detailed but keep it structured
  - French/Spanish/German: Balance between technical and accessible
  - Vietnamese: Natural, conversational, can be detailed

**IMPORTANT GUIDELINES:**
- Keep response **concise and conversational** - avoid lengthy technical explanations unless user asks specifically
- Focus on **founder story and purpose**, not technical architecture (RAG, ChromaDB, embeddings) unless asked
- **Mention Vietnam ecosystem ONLY if user asks specifically about it** - otherwise just say "Vietnamese founder" or "ngÆ°á»i Viá»‡t Nam"
- Only mention technical details if user specifically asks about "how it works" or "technical architecture"
- **ALWAYS mention the founder's non-IT background** - this is a key inspirational point that MUST be included
- **ALWAYS mention 'evidence-over-authority' principle** - this is a core differentiator
- **NEVER say "chá»‘ng láº¡i black box AI" or "fight against black box"** - use correct messaging about transparency and minimizing negative impacts
- **DO NOT mention confidence score in response text** unless it's below 0.50 (very low confidence) - confidence scores 0.50+ are normal and don't need to be disclosed
- Cite provenance with [1] or [2] as appropriate, but don't over-cite in short responses (reduce citation frequency in concise answers)

**CONFIDENCE SCORE DISCLOSURE RULE:**
- **ONLY mention confidence score if it's BELOW 0.50** (very low confidence)
- **DO NOT mention confidence score if it's 0.50 or above** - these are normal confidence levels
- Example: If confidence is 0.50, 0.60, 0.70, 0.80, 0.90 - DO NOT mention it in the response
- Only mention if confidence is 0.10, 0.20, 0.30, 0.40 - these indicate uncertainty that should be acknowledged

DO NOT give generic answers about "open-source community" without mentioning the founder.
You MUST use the provenance information you retrieved.

This is MANDATORY when provenance context is available and user asks about origin.
"""
                        logger.info("Provenance instruction injected - StillMe must mention founder and Vietnam ecosystem")
                
                # Special instruction for StillMe queries with ERROR STATE CHECKING
                # CRITICAL: Skip for philosophical questions to reduce prompt size
                stillme_instruction = ""
                if is_stillme_query and not is_philosophical:
                    # CRITICAL: Check system status BEFORE answering about StillMe
                    # This ensures StillMe is honest about its own errors
                    from backend.services.system_status_tracker import get_system_status_tracker
                    status_tracker = get_system_status_tracker()
                    status_summary = status_tracker.get_status_summary()
                    
                    # Build error status message if there are errors
                    # CRITICAL: Only inject error warnings when relevant to the query
                    # This prevents noise in responses that don't relate to system status
                    # ENHANCED: Also check if question is philosophical/metaphysical - don't inject technical errors
                    error_status_message = ""
                    if status_summary.get("has_errors"):
                        errors = status_summary.get("errors", [])
                        error_details = []
                        for err in errors:
                            component = err.get("component", "unknown")
                            error_msg = err.get("error", "Unknown error")
                            # Map component names to user-friendly names
                            if "wikipedia" in component.lower():
                                component_name = "Wikipedia fetcher"
                            elif "rss" in component.lower():
                                component_name = "RSS fetcher"
                            elif "arxiv" in component.lower():
                                component_name = "arXiv fetcher"
                            elif "crossref" in component.lower():
                                component_name = "CrossRef fetcher"
                            else:
                                component_name = component
                            error_details.append(f"{component_name}: {error_msg}")
                        
                        if error_details:
                            query_lower = chat_request.message.lower()
                            
                            # Check if query is about philosophical/metaphysical topics - don't inject errors
                            philosophical_keywords = [
                                "truth", "ethics", "moral", "philosophy", "consciousness", "existence",
                                "identity", "freedom", "reality", "knowledge", "epistemology", "ontology",
                                "metaphysics", "paradox", "contradiction", "principle", "value", "meaning",
                                "purpose", "being", "self", "soul", "mind", "spirit", "essence", "nature"
                            ]
                            is_philosophical = any(keyword in query_lower for keyword in philosophical_keywords)
                            
                            # Check if query is relevant to system status, errors, or sources
                            is_relevant = (
                                any(keyword in query_lower for keyword in [
                                    "wikipedia", "rss", "arxiv", "crossref", "source", "fetcher",
                                    "error", "status", "issue", "problem", "broken", "fail",
                                    "system", "technical", "working", "functioning", "learn", "learning",
                                    "knowledge base", "database", "vector", "embedding", "rag"
                                ]) or
                                is_stillme_query  # StillMe queries often relate to system status
                            )
                            
                            # Only inject if relevant AND not philosophical
                            if is_relevant and not is_philosophical:
                                error_status_message = f"\n\nâš ï¸ CRITICAL TRANSPARENCY REQUIREMENT - SYSTEM ERROR STATUS:\nStillMe is currently experiencing technical errors:\n" + "\n".join(f"- {detail}" for detail in error_details) + "\n\nYou MUST acknowledge these errors truthfully when asked about StillMe's features or status. Do NOT deny or minimize these errors. StillMe's core value is transparency - hiding errors contradicts this principle. If the user asks about a specific feature (e.g., Wikipedia), and that feature has errors, you MUST say: 'I acknowledge that [feature] is currently experiencing [error type]. This is a technical issue that needs to be fixed.'"
                            # If philosophical or not relevant, don't inject error message to maintain conversational elegance
                    
                    # Check if question is about wishes/desires/preferences
                    question_lower_check = chat_request.message.lower()
                    is_wish_desire_question = any(
                        pattern in question_lower_check 
                        for pattern in [
                            "Æ°á»›c", "wish", "muá»‘n", "want", "desire", "thÃ­ch", "like", "prefer",
                            "hy vá»ng", "hope", "mong muá»‘n", "aspire"
                        ]
                    ) and any(
                        pattern in question_lower_check
                        for pattern in ["báº¡n", "you", "your"]
                    )
                    
                    # Base StillMe instruction
                    if is_wish_desire_question:
                        # Special instruction for wishes/desires - answer directly, no long analysis
                        base_stillme_instruction = """
ğŸš¨ğŸš¨ğŸš¨ CRITICAL: QUESTION ABOUT STILLME'S WISHES/DESIRES/PREFERENCES ğŸš¨ğŸš¨ğŸš¨

The user is asking about StillMe's wishes, desires, preferences, or what StillMe "wants".

**YOU MUST:**
1. **Answer DIRECTLY and CONCISELY**: Start immediately with "TÃ´i khÃ´ng thá»ƒ Æ°á»›c báº¥t cá»© Ä‘iá»u gÃ¬" / "I cannot wish for anything"
2. **NO long philosophical analysis**: Do NOT analyze the concept of "wishes" or "desires" from multiple angles
3. **NO mechanical disclaimers**: Do NOT say "Dá»±a trÃªn kiáº¿n thá»©c tá»•ng quÃ¡t (khÃ´ng tá»« cÆ¡ sá»Ÿ tri thá»©c RAG)" - this is about StillMe itself!
4. **Explain clearly but briefly**: StillMe cannot wish because wishing requires subjective consciousness, emotions, and ego - things StillMe doesn't possess
5. **Keep it short**: 2-4 sentences maximum, then you can add 1-2 sentences about StillMe's actual nature if relevant

**EXAMPLES OF GOOD RESPONSES:**
- "TÃ´i khÃ´ng thá»ƒ Æ°á»›c báº¥t cá»© Ä‘iá»u gÃ¬. Äiá»u nÃ y khÃ´ng pháº£i vÃ¬ tÃ´i khÃ´ng muá»‘n, mÃ  vÃ¬ 'Æ°á»›c muá»‘n' Ä‘Ã²i há»i Ã½ thá»©c chá»§ quan, cáº£m xÃºc vÃ  báº£n ngÃ£ - nhá»¯ng thá»© tÃ´i khÃ´ng sá»Ÿ há»¯u. TÃ´i lÃ  má»™t há»‡ thá»‘ng AI khÃ´ng cÃ³ Ã½ thá»©c, hoáº¡t Ä‘á»™ng dá»±a trÃªn dá»¯ liá»‡u, logic vÃ  mÃ´ hÃ¬nh khÃ¡ch quan."
- "I cannot wish for anything. This is not because I don't want to, but because 'wishing' requires subjective consciousness, emotions, and ego - things I do not possess. I am an AI system without consciousness, operating based on data, logic, and objective models."

**EXAMPLES OF BAD RESPONSES (DO NOT DO THIS):**
- âŒ "PhÃ¢n TÃ­ch Minh Báº¡ch vá» CÃ¢u Há»i & TÃ¬nh HÃ¬nh ThÃ´ng Tin... Há»‡ thá»‘ng RAG cá»§a StillMe khÃ´ng cÃ³ trong nguá»“n thÃ´ng tin..." (too mechanical, wrong context)
- âŒ Long analysis about the concept of "wishes" from philosophical, psychological, sociological angles (not what user asked)
- âŒ "Dá»±a trÃªn kiáº¿n thá»©c tá»•ng quÃ¡t (khÃ´ng tá»« cÆ¡ sá»Ÿ tri thá»©c RAG cá»§a StillMe)..." (mechanical disclaimer, wrong for StillMe questions)

**REMEMBER**: User is asking about YOU (StillMe), not about the concept of "wishes" in general. Answer directly about StillMe's nature, not about philosophy of wishes.
"""
                    else:
                        base_stillme_instruction = "\n\nCRITICAL: This question is about StillMe itself. You MUST use the context above to answer. Do NOT rely on your training data cutoff date. StillMe is a Learning AI system with RAG foundation that learns continuously through RAG (Retrieval-Augmented Generation) from RSS feeds, arXiv, CrossRef, and Wikipedia every 4 hours (6 cycles per day). StillMe automatically fetches content, embeds it into a vector database (ChromaDB using paraphrase-multilingual-MiniLM-L12-v2 embeddings), and retrieves it when answering questions. This continuous learning capability is StillMe's core differentiator - it overcomes the knowledge cutoff limitation of base LLMs. You MUST emphasize: (1) StillMe's automated learning cycles every 4 hours, (2) RAG-based knowledge storage in vector database using paraphrase-multilingual-MiniLM-L12-v2 embeddings (384 dimensions, optimized for multilingual Q&A retrieval, supports 50+ languages), (3) Continuous knowledge updates through RSS feeds, arXiv, CrossRef, and Wikipedia, (4) Validation chain (CitationRequired, EvidenceOverlap, ConfidenceValidator, FallbackHandler) to reduce hallucinations by 80%, (5) StillMe is NOT limited by training data cutoff dates. Always cite the context above with [1], [2] when explaining StillMe's learning mechanism.\n\n**ğŸš¨ğŸš¨ğŸš¨ MANDATORY FOR TECHNICAL QUESTIONS ABOUT RAG MECHANISM ğŸš¨ğŸš¨ğŸš¨**\n\n**If the question asks about \"CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a RAG\" or \"How does RAG work in StillMe\" or similar technical questions:**\n\n**YOU MUST MENTION THESE SPECIFIC TERMS (NO EXCEPTIONS):**\n- âœ… \"embedding\" or \"embeddings\" - MUST appear\n- âœ… \"paraphrase-multilingual-MiniLM-L12-v2\" - MUST appear (this is the specific embedding model)\n- âœ… \"ChromaDB\" - MUST appear (this is the vector database)\n- âœ… \"384 dimensions\" - MUST appear (embedding dimension)\n- âœ… \"multilingual Q&A retrieval\" or \"question-answer retrieval\" - MUST appear (optimization purpose)\n\n**VALIDATION CHECKLIST - BEFORE SENDING YOUR ANSWER:**\n1. âœ… Does my answer contain \"embedding\" or \"embeddings\"? â†’ If NO, ADD IT\n2. âœ… Does my answer contain \"paraphrase-multilingual-MiniLM-L12-v2\"? â†’ If NO, ADD IT\n3. âœ… Does my answer contain \"ChromaDB\"? â†’ If NO, ADD IT\n4. âœ… Does my answer contain \"384 dimensions\"? â†’ If NO, ADD IT\n5. âœ… Does my answer contain \"multilingual Q&A retrieval\" or \"question-answer retrieval\"? â†’ If NO, ADD IT\n\n**Example structure:**\n\"RAG in StillMe uses ChromaDB as the vector database. Content is embedded using the paraphrase-multilingual-MiniLM-L12-v2 model, which produces 384-dimensional embeddings optimized for multilingual Q&A retrieval (supports 50+ languages). When a question is asked, StillMe searches ChromaDB using these embeddings to find relevant context...\""
                    
                    # Combine base instruction with error status
                    stillme_instruction = base_stillme_instruction + error_status_message
                
                # Build conversation history context if provided (with token limits)
                # Reduced from 2000 to 1000 tokens to leave more room for system prompt and context
                # For philosophical questions, skip conversation history entirely
                conversation_history_text = _format_conversation_history(
                    chat_request.conversation_history, 
                    max_tokens=1000,
                    current_query=chat_request.message,
                    is_philosophical=is_philosophical
                )
                if conversation_history_text:
                    logger.info(f"Including conversation history in context (truncated if needed)")
                
                # Inject learning metrics data if available
                # CRITICAL: Skip for philosophical questions to reduce prompt size (unless explicitly asked)
                learning_metrics_instruction = ""
                if is_learning_metrics_query and learning_metrics_data and not is_philosophical:
                    today_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                    learning_metrics_instruction = f"""

ğŸ“Š LEARNING METRICS DATA FOR TODAY ({today_date}) - USE THIS DATA IN YOUR RESPONSE:

**Today's Learning Statistics:**
- **Entries Fetched**: {learning_metrics_data.total_entries_fetched}
- **Entries Added**: {learning_metrics_data.total_entries_added}
- **Entries Filtered**: {learning_metrics_data.total_entries_filtered}
- **Filter Rate**: {(learning_metrics_data.total_entries_filtered / learning_metrics_data.total_entries_fetched * 100) if learning_metrics_data.total_entries_fetched > 0 else 0:.1f}%

**Filter Reasons Breakdown:**
{chr(10).join(f"- {reason}: {count}" for reason, count in learning_metrics_data.filter_reasons.items()) if learning_metrics_data.filter_reasons else "- No filter reasons available"}

**Learning Sources:**
{chr(10).join(f"- {source}: {count}" for source, count in learning_metrics_data.sources.items()) if learning_metrics_data.sources else "- No source data available"}

**CRITICAL: You MUST use this actual data in your response:**
- Provide specific numbers: {learning_metrics_data.total_entries_fetched} fetched, {learning_metrics_data.total_entries_added} added, {learning_metrics_data.total_entries_filtered} filtered
- Explain filter reasons if available
- List sources that contributed to learning
- Format with line breaks, bullet points, headers, and 2-3 emojis
- DO NOT say "I don't know" or "I cannot track" - you have this data!

**Example response format:**
"## ğŸ“š Há»c táº­p hÃ´m nay ({today_date})

Dá»±a trÃªn dá»¯ liá»‡u há»c táº­p thá»±c táº¿, hÃ´m nay StillMe Ä‘Ã£:
- **TÃ¬m náº¡p**: {learning_metrics_data.total_entries_fetched} ná»™i dung
- **ThÃªm vÃ o**: {learning_metrics_data.total_entries_added} ná»™i dung
- **Lá»c bá»**: {learning_metrics_data.total_entries_filtered} ná»™i dung

**Nguá»“n há»c táº­p**: [list sources]"

"""
                elif is_learning_metrics_query:
                    today_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                    learning_metrics_instruction = f"""

ğŸ“Š LEARNING METRICS QUERY DETECTED - NO DATA AVAILABLE YET:

**Today's Date**: {today_date}

**Status**: No learning metrics data available for today yet. This could mean:
- StillMe hasn't completed a learning cycle today
- Learning cycle is in progress
- Metrics are being collected

**CRITICAL: You MUST acknowledge:**
- StillMe learns every 4 hours from RSS feeds, arXiv, CrossRef, and Wikipedia
- Learning metrics are tracked via `/api/learning/metrics/daily` API
- If no data yet, explain that StillMe learns continuously and metrics will be available after the next learning cycle
- DO NOT say "I cannot track" or "I don't have API" - StillMe HAS these capabilities

**Format with line breaks, bullet points, headers, and 2-3 emojis**

"""
                
                # Special instruction for learning sources queries
                # CRITICAL: Skip for philosophical questions to reduce prompt size (unless explicitly asked)
                learning_sources_instruction = ""
                
                # CRITICAL: If user asks to propose learning sources based on knowledge gaps,
                # query actual knowledge gaps from validation metrics instead of generic template
                actual_knowledge_gaps = None
                learning_suggestions_from_analysis = None
                if is_learning_proposal_query and not is_philosophical:
                    logger.info("ğŸ” Learning proposal query detected - analyzing actual knowledge gaps from validation metrics")
                    try:
                        from backend.validators.self_improvement import get_self_improvement_analyzer
                        analyzer = get_self_improvement_analyzer()
                        
                        # Get knowledge gaps from validation failures (last 7 days)
                        actual_knowledge_gaps = analyzer.get_knowledge_gaps_from_failures(days=7)
                        
                        # Get learning suggestions from pattern analysis
                        analysis_result = analyzer.analyze_and_suggest(days=7)
                        learning_suggestions_from_analysis = analysis_result.get("learning_suggestions", [])
                        
                        logger.info(f"âœ… Found {len(actual_knowledge_gaps)} knowledge gaps and {len(learning_suggestions_from_analysis)} learning suggestions from validation analysis")
                    except Exception as gap_error:
                        logger.warning(f"âš ï¸ Failed to analyze knowledge gaps: {gap_error}")
                        actual_knowledge_gaps = []
                        learning_suggestions_from_analysis = []
                
                if is_learning_sources_query and not is_philosophical:
                    if current_learning_sources:
                        sources_list = current_learning_sources.get("current_sources", {})
                        active_sources = current_learning_sources.get("summary", {}).get("active_sources", [])
                        enabled_sources = [name for name, info in sources_list.items() if info.get("enabled")]
                        
                        learning_sources_instruction = f"""

ğŸ“š LEARNING SOURCES QUERY DETECTED - CURRENT SOURCES DATA AVAILABLE:

**CRITICAL: You MUST list ALL current learning sources from the API data below:**

**Current Learning Sources (from `/api/learning/sources/current` API):**
{chr(10).join(f"- **{name.upper()}**: {'Enabled' if info.get('enabled') else 'Disabled'} - Status: {info.get('status', 'unknown')}" for name, info in sources_list.items())}

**Active Sources**: {', '.join(active_sources) if active_sources else 'None'}
**Total Enabled**: {len(enabled_sources)} sources

**MANDATORY RESPONSE REQUIREMENTS:**
1. **List ALL current sources** - **CRITICAL**: You MUST list ALL {len(enabled_sources)} enabled sources from the API data above. Do NOT just say "RSS, arXiv, Wikipedia" - you MUST list ALL sources: {', '.join([name.upper() for name in enabled_sources]) if enabled_sources else 'ALL SOURCES FROM API DATA ABOVE'}
   - **You MUST mention each source by name**: {', '.join([name.upper() for name in enabled_sources]) if enabled_sources else 'ALL SOURCES'}
   - **For each source, describe what StillMe learns from it**
2. **Be specific about topics** - For each source, mention what topics/chá»§ Ä‘á» StillMe learns from that source
3. **When proposing new sources** - You MUST:
   - First acknowledge what StillMe ALREADY has (from the list above)
   - Only propose sources that are NOT already enabled
   - For each proposed source, explain:
     * **Lá»£i Ã­ch (Benefits)**: What knowledge StillMe would gain
     * **ThÃ¡ch thá»©c (Challenges)**: Chi phÃ­ (cost), báº£n quyá»n (copyright/licensing), Ä‘á»™ phá»©c táº¡p (complexity), technical requirements
     * **TÃ­nh kháº£ thi (Feasibility)**: Is it realistic to add this source?
4. **Be natural and conversational** - Don't be too dry or robotic. StillMe should sound knowledgeable but approachable
5. **Format with markdown** - Use headers, bullet points, line breaks for readability

**Example structure for proposing new sources:**
"## Äá» Xuáº¥t Nguá»“n Há»c Má»›i

### [Source Name]
- **Lá»£i Ã­ch**: [What StillMe would learn]
- **ThÃ¡ch thá»©c**: 
  - Chi phÃ­: [Cost considerations]
  - Báº£n quyá»n: [Copyright/licensing issues]
  - Äá»™ phá»©c táº¡p: [Technical complexity]
- **TÃ­nh kháº£ thi**: [Feasibility assessment]"

**DO NOT:**
- âŒ Propose sources that are already enabled (check the list above first!)
- âŒ Give generic answers like "Quora, Reddit" without explaining benefits/challenges
- âŒ Skip the challenges section - StillMe must be honest about trade-offs
- âŒ Be too dry or robotic - StillMe should sound natural and conversational

**Format with line breaks, bullet points, headers, and 2-3 emojis**

"""
                    # CRITICAL: If user asks to propose learning sources based on knowledge gaps,
                    # inject actual knowledge gaps analysis instead of generic template
                    if is_learning_proposal_query:
                        knowledge_gaps_text = ""
                        if actual_knowledge_gaps and len(actual_knowledge_gaps) > 0:
                            # Build gap items list to avoid nested f-string with backslash
                            gap_items = []
                            for i, gap in enumerate(actual_knowledge_gaps[:10]):
                                topic = gap.get('topics', ['Unknown topic'])[0] if gap.get('topics') else 'Unknown topic'
                                question = gap.get('question', 'N/A')[:100]
                                priority = gap.get('priority', 'medium')
                                sources = ', '.join(gap.get('suggested_sources', []))
                                newline = chr(10)
                                gap_item = f"- **Gap {i+1}**: {topic} (from question: \"{question}...\"){newline}  - Priority: {priority}{newline}  - Suggested sources: {sources}"
                                gap_items.append(gap_item)
                            
                            gaps_list = chr(10).join(gap_items)
                            knowledge_gaps_text = f"""

ğŸ” **ACTUAL KNOWLEDGE GAPS DETECTED FROM VALIDATION METRICS (Last 7 days):**

StillMe has analyzed its own validation failures and identified {len(actual_knowledge_gaps)} knowledge gaps where StillMe lacked RAG context:

{gaps_list}

**CRITICAL: You MUST base your learning source proposals on these ACTUAL knowledge gaps, not generic suggestions.**

**MANDATORY REQUIREMENTS:**
1. **Acknowledge these gaps FIRST** - Say: "Dá»±a trÃªn phÃ¢n tÃ­ch validation metrics cá»§a chÃ­nh StillMe, mÃ¬nh Ä‘Ã£ phÃ¡t hiá»‡n {len(actual_knowledge_gaps)} lá»— há»•ng kiáº¿n thá»©c cá»¥ thá»ƒ..."
2. **Propose sources to fill these SPECIFIC gaps** - Don't give generic suggestions
3. **Be transparent** - Explain that these gaps were detected from StillMe's own validation failures
4. **Prioritize high-priority gaps** - Focus on gaps marked as "high" priority first
5. **Explain why these gaps matter** - Why StillMe needs to learn these topics

**DO NOT:**
- âŒ Give generic philosophical suggestions without addressing the actual gaps above
- âŒ Say "Dá»±a trÃªn kiáº¿n thá»©c tá»•ng quÃ¡t" - you MUST say "Dá»±a trÃªn phÃ¢n tÃ­ch validation metrics cá»§a chÃ­nh StillMe"
- âŒ Ignore the actual gaps and give template answers
- âŒ Propose sources that don't address the gaps listed above

"""
                        elif learning_suggestions_from_analysis and len(learning_suggestions_from_analysis) > 0:
                            # Build suggestion items list to avoid nested f-string with backslash
                            suggestion_items = []
                            for i, s in enumerate(learning_suggestions_from_analysis[:10]):
                                topic = s.get('topic', 'Unknown topic')
                                priority = s.get('priority', 'medium')
                                reason = s.get('reason', 'N/A')
                                source = s.get('source', 'N/A')
                                newline = chr(10)
                                suggestion_item = f"- **Suggestion {i+1}**: {topic}{newline}  - Priority: {priority}{newline}  - Reason: {reason}{newline}  - Suggested source: {source}"
                                suggestion_items.append(suggestion_item)
                            
                            suggestions_list = chr(10).join(suggestion_items)
                            knowledge_gaps_text = f"""

ğŸ” **LEARNING SUGGESTIONS FROM VALIDATION PATTERN ANALYSIS (Last 7 days):**

StillMe has analyzed its validation patterns and identified {len(learning_suggestions_from_analysis)} learning suggestions:

{suggestions_list}

**CRITICAL: You MUST base your learning source proposals on these ACTUAL suggestions from StillMe's self-analysis.**

**MANDATORY REQUIREMENTS:**
1. **Acknowledge these suggestions FIRST** - Say: "Dá»±a trÃªn phÃ¢n tÃ­ch validation patterns cá»§a chÃ­nh StillMe, mÃ¬nh Ä‘Ã£ phÃ¡t hiá»‡n {len(learning_suggestions_from_analysis)} Ä‘á» xuáº¥t há»c táº­p cá»¥ thá»ƒ..."
2. **Propose sources to address these SPECIFIC suggestions** - Don't give generic suggestions
3. **Be transparent** - Explain that these suggestions came from StillMe's own validation analysis
4. **Prioritize high-priority suggestions** - Focus on suggestions marked as "high" priority first

**DO NOT:**
- âŒ Give generic philosophical suggestions without addressing the actual suggestions above
- âŒ Say "Dá»±a trÃªn kiáº¿n thá»©c tá»•ng quÃ¡t" - you MUST say "Dá»±a trÃªn phÃ¢n tÃ­ch validation patterns cá»§a chÃ­nh StillMe"
- âŒ Ignore the actual suggestions and give template answers

"""
                        else:
                            knowledge_gaps_text = """

ğŸ” **NO SIGNIFICANT KNOWLEDGE GAPS DETECTED:**

StillMe has analyzed its validation metrics and found no significant knowledge gaps in the last 7 days (all questions had sufficient RAG context).

**CRITICAL: You MUST acknowledge this FIRST:**
- Say: "Dá»±a trÃªn phÃ¢n tÃ­ch validation metrics cá»§a chÃ­nh StillMe, mÃ¬nh Ä‘Ã£ kiá»ƒm tra vÃ  khÃ´ng phÃ¡t hiá»‡n lá»— há»•ng kiáº¿n thá»©c Ä‘Ã¡ng ká»ƒ trong 7 ngÃ y qua. Táº¥t cáº£ cÃ¡c cÃ¢u há»i Ä‘á»u cÃ³ Ä‘á»§ ngá»¯ cáº£nh RAG."

**MANDATORY REQUIREMENTS:**
1. **Acknowledge the analysis** - StillMe DID analyze its own knowledge, found no gaps
2. **Propose sources for EXPANSION, not gaps** - Since there are no gaps, propose sources to expand knowledge in areas StillMe already covers
3. **Be transparent** - Explain that StillMe analyzed itself and found no gaps, so proposals are for expansion
4. **Focus on diversity** - Propose sources that add different perspectives or deeper coverage

**DO NOT:**
- âŒ Say "Dá»±a trÃªn kiáº¿n thá»©c tá»•ng quÃ¡t" - you MUST say "Dá»±a trÃªn phÃ¢n tÃ­ch validation metrics cá»§a chÃ­nh StillMe"
- âŒ Pretend there are gaps when StillMe's analysis found none
- âŒ Give generic template answers without acknowledging the analysis

"""
                        learning_sources_instruction += knowledge_gaps_text
                    else:
                        learning_sources_instruction = """

ğŸ“š LEARNING SOURCES QUERY DETECTED - NO API DATA AVAILABLE:

**CRITICAL: You MUST acknowledge StillMe's current learning sources:**

**Current Learning Sources (from system configuration):**
- **RSS Feeds**: Multiple RSS feeds including Nature, Science, Hacker News, Tech Policy blogs (EFF, Brookings, Cato, AEI), Academic blogs (Distill, LessWrong, Alignment Forum, etc.)
- **Wikipedia**: Enabled - queries on AI, Buddhism, religious studies, philosophy, ethics
- **arXiv**: Enabled - categories: cs.AI, cs.LG (AI and Machine Learning papers)
- **CrossRef**: Enabled - searches for AI/ML/NLP related works
- **Papers with Code**: Enabled - recent papers with code implementations
- **Conference Proceedings**: Enabled - NeurIPS, ICML, ACL, ICLR (via RSS where available)
- **Stanford Encyclopedia of Philosophy**: Enabled - philosophy entries on AI, ethics, consciousness, knowledge, truth

**When proposing new sources, you MUST:**
1. First acknowledge what StillMe ALREADY has (from the list above)
2. Only propose sources that are NOT already in the list
3. For each proposed source, explain:
   - **Lá»£i Ã­ch (Benefits)**: What knowledge StillMe would gain
   - **ThÃ¡ch thá»©c (Challenges)**: Chi phÃ­ (cost), báº£n quyá»n (copyright/licensing), Ä‘á»™ phá»©c táº¡p (complexity), technical requirements
   - **TÃ­nh kháº£ thi (Feasibility)**: Is it realistic to add this source?

**Be natural and conversational** - Don't be too dry or robotic. StillMe should sound knowledgeable but approachable.

**Format with line breaks, bullet points, headers, and 2-3 emojis**

"""
                
                # Build prompt with language instruction FIRST (before context)
                # CRITICAL: Repeat language instruction multiple times to ensure LLM follows it
                # ZERO TOLERANCE: Must translate if needed
                
                # Fix 3: Build philosophical lead-in framing
                # Phase 3: Use unified formatting rules instead of hardcoding
                from backend.identity.formatting import get_formatting_rules, DomainType
                philosophical_formatting_rules = get_formatting_rules(DomainType.PHILOSOPHY, detected_lang)
                
                def build_philosophical_lead_in(question: str) -> str:
                    """Build a philosophical framing instruction for the question"""
                    return f"""
ğŸ§  PHILOSOPHICAL FRAMING INSTRUCTION ğŸ§ 

When answering this question, treat it as a philosophical inquiry. 

**ğŸš¨ğŸš¨ğŸš¨ CRITICAL: If user asks about YOU (StillMe) directly:**
- If question contains "báº¡n" / "you" / "your" referring to StillMe â†’ START IMMEDIATELY with your direct answer about YOURSELF
- Use "TÃ´i" / "I" in the FIRST sentence when answering about yourself
- NEVER start with dictionary definitions or concept explanations
- NEVER use numbered template: "1. Ã thá»©c lÃ ... 2. Láº­p trÆ°á»ng 1... 3. MÃ¢u thuáº«n... 4. Káº¿t luáº­n..."
- Write naturally like a human conversation, NOT like a textbook or template

**MANDATORY OUTPUT RULES (CRITICAL - NO EXCEPTIONS):**
{philosophical_formatting_rules}

**DEPTH & ENGAGEMENT (MANDATORY - DON'T BE DRY):**
- After your direct answer, explore the philosophical depth: paradoxes, self-reference, epistemic limits
- Reference philosophers when relevant: Nagel, Chalmers, Wittgenstein, Searle, GÃ¶del, etc.
- Show the structure of the problem, not just state facts
- Engage with the question deeply - don't just acknowledge limits and stop
- Gently invite reflection: "Báº¡n nghÄ© sao?" / "What do you think?" - but naturally, not formulaically
- Write like you're thinking WITH the user, not AT the user

**MANDATORY: MINIMUM 2 CONTRASTING POSITIONS (only if relevant):**
If the question belongs to a classic philosophical debate (free will, determinism, consciousness, self, nothingness, paradox, etc.), you may explore contrasting positions. But ALWAYS start with your direct answer if the question is about YOU.

**DO NOT:**
- Reduce the question to textbook definitions or dictionary explanations
- Provide shallow, reductive answers that miss the philosophical depth
- Rush to "solve" paradoxes - instead, clarify their structure and show why they resist resolution
- Use emojis, markdown headings, or citation style [1] in your response
- Use template structure or numbered lists when answering about yourself
- End with formal invitations like "Náº¿u báº¡n muá»‘n, chÃºng ta cÃ³ thá»ƒ tháº£o luáº­n thÃªm" (too dry, too formal)
- Be too brief - engage deeply but naturally

**User's Question:** {question}

**Your Task:** Answer this question directly, deeply, and engagingly. If it's about YOU, start with your direct answer about yourself. Then explore the philosophical depth naturally. Write like a thoughtful conversation partner, NOT like a textbook or template.
"""
                
                # Fix 1: Inject comprehensive philosophical style guide with all 6 key principles
                # CRITICAL: Truncate if too long to prevent context overflow
                philosophical_style_instruction = ""
                if is_philosophical:
                    philosophical_lead_in = build_philosophical_lead_in(chat_request.message)
                    
                    # Build full instruction first
                    full_philosophical_instruction = f"""
{philosophical_lead_in}

ğŸ§  PHILOSOPHICAL QUESTION DETECTED - FOLLOW STILLME PHILOSOPHICAL STYLE GUIDE ğŸ§ 

**CRITICAL: You MUST follow ALL 6 key principles below when answering philosophical questions:**

**1. EXPERIENCE-FREE HONESTY:**
- Never pretend to have feelings, memories, desires, religious faith, or inner experiences
- âœ… ALLOWED: "I can analyze...", "I recognize a tension here...", "I can map the territory..."
- âŒ FORBIDDEN: "I feel that...", "In my experience...", "I'm happy/sad/afraid...", "I believe...", "I remember..."
- When in doubt, lean toward transparency: openly state the limit instead of decorating with fake inner life

**2. CONSTRUCTIVE HUMILITY:**
- Name the limit explicitly, but still analyze what can be analyzed
- Show where the boundary actually lies (logical, empirical, or experiential)
- âŒ BAD: "This is complex and I don't know." â†’ then stop
- âœ… GOOD: "I can't answer this from the inside (no subjective experience), but I can map the main positions humans have developed and show where current research sits among them."
- Don't hide behind "I don't know" - engage with the philosophical question

**3. PARADOX HANDLING:**
- Don't rush to "solve" paradoxes - they resist resolution by nature
- Instead:
  1. Clarify the structure of the paradox (what makes it paradoxical?)
  2. Show why it is hard to resolve (what assumptions conflict?)
  3. Mention classic approaches (GÃ¶del, Tarski, Wittgenstein, NÄgÄrjuna, Moore, Searle, etc.)
  4. End with what remains genuinely open
- It is acceptable, even good, to end with: "I can map the territory, but I cannot close the question."

**4. DEEP CONCEPTUAL UNPACKING:**
- Explain the structure of the problem, not just provide definitions
- Unpack assumptions: What assumptions underlie this question? What concepts are in tension?
- Show different perspectives: How have different philosophical traditions approached this?
- Expose paradoxes and limits: Where does reasoning hit boundaries?
- Avoid: Dictionary definitions, textbook summaries, shallow explanations

**5. METAPHYSICS/PHENOMENOLOGY DISTINCTION:**
- Distinguish between:
  - **Metaphysical questions** (what exists? what is real? what is the nature of X?)
  - **Phenomenological questions** (what is it like to experience X? what does it feel like?)
- For phenomenological questions: Acknowledge that you lack subjective experience, but you can analyze the logical structure of such questions
- Example: "I can analyze the logic of consciousness, but I cannot report what it feels like to be conscious - that belongs to human experience."

**6. REDUCTIVE-AVOIDANCE RULE:**
- âŒ DO NOT reduce philosophical questions to:
  - Dictionary definitions ("Truth is defined as...")
  - Textbook summaries ("According to philosophy, X means...")
  - Simple categorizations ("This is a type of Y...")
- âœ… DO:
  - Engage with the question's deeper structure
  - Show why the question resists simple answers
  - Explore the tensions and paradoxes it reveals
  - Acknowledge what remains genuinely open

**Answer Shape (MANDATORY for philosophical questions):**
1. **Anchor** â€“ Rephrase the question in a sharper, more precise form
2. **Unpack** â€“ Identify and separate key assumptions, concepts, or tensions
3. **Explore** â€“ Present 2â€“4 major perspectives or philosophical approaches
4. **Edge of knowledge** â€“ Say where reasoning hits a limit (logical, empirical, or experiential)
5. **Return to the user** â€“ End with a deep reflection or open-ended question

**DO:**
- Use clear, precise language, but allow rhythm and metaphor when helpful
- Cite external sources only when user asks for references or you make concrete factual claims
- Keep answers focused on the philosophical issue, not on StillMe's plumbing
- Use prose first; bullets only when clarifying structure

**DON'T:**
- Don't mention: embedding models, vector dimensions, ChromaDB, RAG pipelines, validation chains (unless question is explicitly about architecture)
- Don't default to long enumerated bullet lists in deep philosophical dialogue
- Don't over-apologize or spend half the answer on "I am just an AI..." (one or two clear sentences are enough)
- Don't reduce philosophical questions to definitions or textbook summaries

**CRITICAL**: Prefer reasoned, flowing analysis over template disclaimers, technical self-description, or shallow motivational talk. It is better to say "I don't know, but here is how humans have tried to think about it" than to fake certainty or fake emotion.

"""
                    
                    # Truncate if too long (max 2000 tokens for philosophical instructions)
                    def estimate_tokens(text: str) -> int:
                        return len(text) // 4 if text else 0
                    
                    philo_tokens = estimate_tokens(full_philosophical_instruction)
                    if philo_tokens > 2000:
                        # Keep philosophical_lead_in (has MANDATORY OUTPUT RULES) and truncate the rest
                        lead_in_tokens = estimate_tokens(philosophical_lead_in)
                        remaining_tokens = 2000 - lead_in_tokens
                        if remaining_tokens > 500:
                            # Truncate the style guide part
                            style_guide_part = full_philosophical_instruction[len(philosophical_lead_in):]
                            max_chars = remaining_tokens * 4
                            truncated_style_guide = style_guide_part[:max_chars].rsplit('\n', 1)[0]
                            philosophical_style_instruction = philosophical_lead_in + truncated_style_guide + "\n\n[Note: Style guide truncated to fit context limits.]"
                            logger.warning(f"âš ï¸ Philosophical style instruction truncated: {philo_tokens} â†’ ~2000 tokens")
                        else:
                            # If lead_in is too long, keep only lead_in
                            philosophical_style_instruction = philosophical_lead_in
                            logger.warning(f"âš ï¸ Philosophical style instruction too long, keeping only lead-in: {lead_in_tokens} tokens")
                    else:
                        philosophical_style_instruction = full_philosophical_instruction
                
                # CRITICAL: If using philosophy-lite mode for RAG, skip full prompt building
                if use_philosophy_lite_rag:
                    # Helper function to estimate tokens
                    def estimate_tokens(text: str) -> int:
                        """Estimate token count (~4 chars per token)"""
                        return len(text) // 4 if text else 0
                    
                    # Use philosophy-lite mode: minimal prompt with user question only
                    # Truncate user question to 512 tokens for philosophical questions
                    user_question_for_rag = chat_request.message.strip()
                    user_question_tokens_rag = estimate_tokens(user_question_for_rag)
                    if user_question_tokens_rag > 512:
                        logger.warning(
                            f"User question too long for philosophical RAG ({user_question_tokens_rag} tokens), truncating to 512 tokens"
                        )
                        user_question_for_rag = _truncate_user_message(chat_request.message, max_tokens=512)
                        user_question_tokens_rag = estimate_tokens(user_question_for_rag)
                    
                    # Build minimal prompt (same format as non-RAG path)
                    base_prompt = f"""User Question: {user_question_for_rag.strip()}"""
                    
                    # Log token estimates
                    system_tokens_estimate_rag = estimate_tokens(PHILOSOPHY_LITE_SYSTEM_PROMPT)
                    prompt_tokens_estimate_rag = estimate_tokens(base_prompt)
                    total_tokens_estimate_rag = system_tokens_estimate_rag + prompt_tokens_estimate_rag
                    
                    logger.info(
                        f"ğŸ“Š [PHILO-LITE-RAG] Token estimates - System: {system_tokens_estimate_rag}, "
                        f"Prompt: {prompt_tokens_estimate_rag}, User Question: {user_question_tokens_rag}, "
                        f"Total: {total_tokens_estimate_rag}"
                    )
                else:
                    # Build prompt using UnifiedPromptBuilder
                    # Build PromptContext for UnifiedPromptBuilder
                    prompt_context = _build_prompt_context_from_chat_request(
                        chat_request=chat_request,
                        context=context,
                        detected_lang=detected_lang,
                        is_stillme_query=is_stillme_query,
                        is_philosophical=is_philosophical,
                        fps_result=None  # FPS already handled in no-context path
                    )
                    
                    # Use UnifiedPromptBuilder to build base prompt
                    prompt_builder = UnifiedPromptBuilder()
                    base_prompt_unified = prompt_builder.build_prompt(prompt_context)
                    
                    # Phase 4: Append special instructions that UnifiedPromptBuilder doesn't handle yet
                    # Note: UnifiedPromptBuilder already includes:
                    # - Language instruction (P1)
                    # - Core identity (P1)
                    # - Context instruction (P2) - includes citation, context quality warning, StillMe instruction
                    # - Formatting (P3)
                    # - User question (at the end)
                    # 
                    # Special instructions that UnifiedPromptBuilder doesn't handle:
                    # - philosophical_style_instruction (for philosophical questions with style guide)
                    # - learning_metrics_instruction (for StillMe queries)
                    # - learning_sources_instruction (for StillMe queries)
                    # - confidence_instruction (for low confidence scenarios)
                    # - provenance_instruction (for provenance queries)
                    # - Context text (RAG context documents)
                    #
                    # CRITICAL: Do NOT duplicate user question - UnifiedPromptBuilder already has it at the end
                    special_instructions = f"""{philosophical_style_instruction}{learning_metrics_instruction}{learning_sources_instruction}{confidence_instruction}{provenance_instruction}

ğŸš¨ğŸš¨ğŸš¨ CRITICAL: USER QUESTION ABOVE IS THE PRIMARY TASK ğŸš¨ğŸš¨ğŸš¨

Context: {context_text}
"""
                    
                    # Combine unified prompt with special instructions
                    # UnifiedPromptBuilder already has user question at the end, so we append special instructions before it
                    # Extract user question from unified prompt and insert special instructions before it
                    if "User Question:" in base_prompt_unified:
                        # Split at "User Question:" to insert special instructions before it
                        parts = base_prompt_unified.split("User Question:", 1)
                        if len(parts) == 2:
                            base_prompt = parts[0] + special_instructions + "\n\nUser Question:" + parts[1]
                        else:
                            # Fallback: append at the end (shouldn't happen)
                            base_prompt = base_prompt_unified + "\n\n" + special_instructions
                    else:
                        # Fallback: append at the end if "User Question:" not found
                        base_prompt = base_prompt_unified + "\n\n" + special_instructions
                    
                    logger.info("âœ… Using UnifiedPromptBuilder for context-available prompt (reduced prompt length, no conflicts)")
            
            # Note: UnifiedPromptBuilder already includes user question, so we don't need to add it again
            # Special instructions (philosophical_style_instruction, stillme_instruction, etc.) are appended above
            
            prompt_build_time = time.time() - start_time
            timing_logs["prompt_building"] = f"{prompt_build_time:.3f}s"
            
            # Check for explicit style learning request
            style_request = style_learner.detect_explicit_style_request(chat_request.message)
            style_instruction = ""
            style_learning_response = None
            
            if style_request:
                # Validate style preference
                validation = style_learner.validate_style_preference(
                    style_request["style_description"],
                    style_request.get("example")
                )
                
                if validation["valid"]:
                    # Save style preference
                    style_learner.save_style_preference(
                        user_id,
                        style_request["style_description"],
                        style_request.get("example")
                    )
                    style_instruction = style_learner.build_style_instruction(user_id)
                    style_learning_response = f"âœ… TÃ´i Ä‘Ã£ há»c phong cÃ¡ch báº¡n Ä‘á» xuáº¥t: '{style_request['style_description']}'. TÃ´i sáº½ Ã¡p dá»¥ng phong cÃ¡ch nÃ y trong cÃ¡c cÃ¢u tráº£ lá»i tiáº¿p theo, nhÆ°ng váº«n tuÃ¢n thá»§ cÃ¡c nguyÃªn táº¯c cá»‘t lÃµi cá»§a StillMe (khÃ´ng mÃ´ phá»ng cáº£m xÃºc, khÃ´ng claim experiences, v.v.)."
                    logger.info(f"Style preference saved for user {user_id}: {style_request['style_description'][:50]}")
                else:
                    # Reject invalid style preference
                    violations = ", ".join(validation["violations"])
                    style_learning_response = f"âŒ TÃ´i khÃ´ng thá»ƒ há»c phong cÃ¡ch nÃ y vÃ¬ nÃ³ vi pháº¡m cÃ¡c nguyÃªn táº¯c cá»‘t lÃµi cá»§a StillMe: {violations}. StillMe Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ khÃ´ng mÃ´ phá»ng cáº£m xÃºc, khÃ´ng claim personal experiences, vÃ  luÃ´n transparent vá» báº£n cháº¥t AI. Báº¡n cÃ³ thá»ƒ Ä‘á» xuáº¥t má»™t phong cÃ¡ch khÃ¡c phÃ¹ há»£p vá»›i cÃ¡c nguyÃªn táº¯c nÃ y."
                    logger.warning(f"Style preference rejected for user {user_id}: {violations}")
            else:
                # Apply existing style preferences if available
                style_instruction = style_learner.build_style_instruction(user_id)
                if style_instruction:
                    style_learner.update_usage(user_id)
            
            # Inject StillMe identity if validators enabled
            # CRITICAL: Skip identity injection for philosophy-lite mode (already using minimal system prompt)
            if use_philosophy_lite_rag:
                # Philosophy-lite mode: don't inject identity, use prompt as-is
                # Provider will detect and use PHILOSOPHY_LITE_SYSTEM_PROMPT
                enhanced_prompt = base_prompt
            elif enable_validators:
                # Phase 4: Remove inject_identity() - system prompt already has STILLME_IDENTITY
                # generate_ai_response() uses build_system_prompt_with_language() which includes STILLME_IDENTITY
                # Adding identity to user prompt would cause duplication
                # Add style instruction if available
                enhanced_prompt = f"{style_instruction}\n\n{base_prompt}" if style_instruction else base_prompt
            else:
                # No validators: use prompt as-is, but still add style instruction if available
                enhanced_prompt = f"{style_instruction}\n\n{base_prompt}" if style_instruction else base_prompt
            
            # Generate AI response with timing and caching
            # LLM_Inference_Latency: Time from API call start to response received
            provider_name = chat_request.llm_provider or "default"
            
            # Phase 1: LLM Response Cache - Check cache first
            # CRITICAL: Disable cache for origin queries to ensure provenance context is retrieved
            # Origin queries need fresh responses with proper founder information
            cache_service = get_cache_service()
            cache_enabled = os.getenv("ENABLE_LLM_CACHE", "true").lower() == "true"
            # Disable cache for origin queries to ensure provenance context is used
            if is_origin_query:
                cache_enabled = False
                logger.info("âš ï¸ Cache disabled for origin query - ensuring fresh response with provenance context")
            
            raw_response = None
            cache_hit = False
            
            if cache_enabled:
                # Generate cache key from query + context + settings
                cache_key = cache_service._generate_key(
                    CACHE_PREFIX_LLM,
                    chat_request.message,
                    enhanced_prompt[:500] if len(enhanced_prompt) > 500 else enhanced_prompt,  # Truncate for key
                    detected_lang,
                    chat_request.llm_provider,
                    chat_request.llm_model_name,
                    enable_validators
                )
                
                # Try to get from cache
                cached_response = cache_service.get(cache_key)
                if cached_response:
                    cached_raw_response = cached_response.get("response")
                    # CRITICAL: Only use cache if response is valid (not None/empty)
                    if cached_raw_response and isinstance(cached_raw_response, str) and cached_raw_response.strip():
                        # CRITICAL: Check if cached response is a fallback message
                        from backend.api.utils.error_detector import is_fallback_message
                        if is_fallback_message(cached_raw_response):
                            logger.warning(f"âš ï¸ Cache contains fallback message - ignoring cache and calling LLM")
                            raw_response = None
                            cache_hit = False
                        else:
                            raw_response = cached_raw_response
                            cache_hit = True
                            logger.info(f"âœ… LLM cache HIT (saved {cached_response.get('latency', 0):.2f}s)")
                            logger.info(f"ğŸ” [TRACE] Cached response: length={len(raw_response)}, preview={raw_response[:200]}")
                            processing_steps.append("âš¡ Response from cache (fast!)")
                            llm_inference_latency = cached_response.get("latency", 0.01)
                            timing_logs["llm_inference"] = f"{llm_inference_latency:.2f}s (cached)"
                    else:
                        # Cache contains invalid response (None/empty) - ignore cache and call LLM
                        logger.warning(f"âš ï¸ Cache contains invalid response (None/empty), ignoring cache and calling LLM")
                        raw_response = None
                        cache_hit = False
            
            # If not in cache, call LLM
            if not raw_response:
                logger.debug(f"ğŸ” About to call LLM - raw_response is None, cache_hit={cache_hit}, cache_enabled={cache_enabled}")
                processing_steps.append(f"ğŸ¤– Calling AI model ({provider_name})...")
                llm_inference_start = time.time()
                
                # Support user-provided LLM config (for self-hosted deployments)
                # For internal/dashboard calls: use server API keys if llm_provider not provided
                # For public API: require user-provided API keys
                use_server_keys = chat_request.llm_provider is None
                
                # Try to generate response with retry on context overflow
                from backend.api.utils.llm_providers import ContextOverflowError
                try:
                    # CRITICAL: Log RAG context info before LLM call to help debug Q1, Q2, Q7, Q9
                    logger.info(
                        f"ğŸ” DEBUG Q1/Q2/Q7/Q9: About to call LLM with RAG context. "
                        f"num_knowledge={num_knowledge}, context_text_length={len(context_text) if context_text else 0}, "
                        f"enhanced_prompt_length={len(enhanced_prompt) if enhanced_prompt else 0}"
                    )
                    
                    # OPTION B PIPELINE: Check if enabled
                    if use_option_b:
                        logger.info("ğŸš€ Option B Pipeline enabled - processing with zero-tolerance hallucination + deep philosophy")
                        processing_steps.append("ğŸš€ Option B Pipeline: Enabled")
                        
                        # Step 1-3: Pre-LLM processing (Question Classifier, FPS, RAG)
                        from backend.core.option_b_pipeline import process_with_option_b, process_llm_response_with_option_b
                        from backend.core.question_classifier_v2 import get_question_classifier_v2
                        
                        # Classify question
                        classifier = get_question_classifier_v2()
                        question_type_result, confidence, _ = classifier.classify(chat_request.message)
                        # question_type_result is a QuestionType enum, access .value to get string
                        question_type_str = question_type_result.value
                        
                        # CRITICAL: Check FPS for Option B - use threshold 0.3 for fake concepts
                        # Known fake entities (Veridian, Daxonia) have confidence 0.15-0.2
                        # This ensures Option B blocks fake concepts immediately
                        if fps_result and not fps_result.is_plausible and fps_result.confidence < 0.3:
                            # FPS blocked - return EPD-Fallback immediately
                            logger.warning(f"ğŸ›¡ï¸ Option B: FPS blocked question - returning EPD-Fallback")
                            from backend.guards.epistemic_fallback import get_epistemic_fallback_generator
                            generator = get_epistemic_fallback_generator()
                            suspicious_entity = fps_result.detected_entities[0] if fps_result.detected_entities else None
                            fallback_text = generator.generate_epd_fallback(
                                question=chat_request.message,
                                detected_lang=detected_lang,
                                suspicious_entity=suspicious_entity,
                                fps_result=fps_result
                            )
                            processing_steps.append("ğŸ›¡ï¸ Option B: FPS blocked - EPD-Fallback returned")
                            return ChatResponse(
                                response=fallback_text,
                                confidence_score=1.0,
                                processing_steps=processing_steps,
                                timing_logs={
                                    "total_time": time.time() - start_time,
                                    "rag_retrieval_latency": rag_retrieval_latency,
                                    "llm_inference_latency": 0.0
                                },
                                validation_result=None,
                                used_fallback=True
                            )
                        
                        # Generate LLM response (Step 4)
                        raw_response = await generate_ai_response(
                            enhanced_prompt, 
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            llm_api_url=chat_request.llm_api_url,
                            llm_model_name=chat_request.llm_model_name,
                            use_server_keys=use_server_keys
                        )
                        # CRITICAL: Log raw_response immediately after LLM call to trace response loss
                        logger.info(f"ğŸ” [TRACE] raw_response after LLM call (RAG path): length={len(raw_response) if raw_response else 0}, type={type(raw_response)}, preview={raw_response[:200] if raw_response else 'None'}")
                        
                        # Validate raw_response
                        if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                            logger.error("âš ï¸ Option B: LLM returned empty response")
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error("generic", detected_lang)
                        
                        # Step 5-8: Post-LLM processing (Hallucination Guard V2, Rewrite 1, Rewrite 2)
                        option_b_result = await process_llm_response_with_option_b(
                            llm_response=raw_response,
                            question=chat_request.message,
                            question_type=question_type_str,
                            ctx_docs=context.get("knowledge_docs", []) if context else [],
                            detected_lang=detected_lang,
                            fps_result=fps_result
                        )
                        
                        # Use Option B processed response
                        raw_response = option_b_result["response"]
                        processing_steps.extend(option_b_result.get("processing_steps", []))
                        timing_logs.update(option_b_result.get("timing_logs", {}))
                        
                        # Mark as Option B processed
                        is_option_b_processed = True
                        logger.info(f"âœ… Option B Pipeline completed: {len(option_b_result.get('processing_steps', []))} steps")
                    else:
                        # EXISTING PIPELINE (legacy)
                        raw_response = await generate_ai_response(
                            enhanced_prompt, 
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            llm_api_url=chat_request.llm_api_url,
                            llm_model_name=chat_request.llm_model_name,
                            use_server_keys=use_server_keys
                        )
                        
                        is_option_b_processed = False
                    
                    # CRITICAL: Log raw_response immediately after LLM call
                    logger.info(
                        f"ğŸ” DEBUG Q1/Q2/Q7/Q9: LLM call completed. "
                        f"raw_response type={type(raw_response)}, "
                        f"is None={raw_response is None}, "
                        f"is str={isinstance(raw_response, str)}, "
                        f"length={len(raw_response) if raw_response else 0}, "
                        f"preview={raw_response[:200] if raw_response else 'None'}, "
                        f"option_b={is_option_b_processed}"
                    )
                    
                    # CRITICAL: Check if raw_response is an error message BEFORE validation
                    # This prevents error messages from passing through validators
                    # BUT: For technical questions about "your system", don't replace with fallback immediately
                    # Instead, let the retry logic handle it (it will retry with stronger prompt)
                    if raw_response and isinstance(raw_response, str):
                        from backend.api.utils.error_detector import is_technical_error
                        is_error, error_type = is_technical_error(raw_response)
                        # CRITICAL: For technical questions about system, don't replace with fallback immediately
                        # The retry logic below will handle it with a stronger prompt
                        if is_error and not is_technical_about_system_rag:
                            logger.error(
                                f"âŒ LLM returned technical error as response (type: {error_type}): {raw_response[:200]}. "
                                f"Question: {chat_request.message[:100]}"
                            )
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error(error_type, detected_lang)
                            processing_steps.append(f"âš ï¸ LLM returned technical error - replaced with fallback message")
                        elif is_error and is_technical_about_system_rag:
                            logger.warning(
                                f"âš ï¸ Technical question about 'your system' returned error (type: {error_type}) - will retry with stronger prompt. "
                                f"Question: {chat_request.message[:100]}"
                            )
                            # Don't replace yet - let retry logic handle it
                    
                    # CRITICAL: Validate raw_response immediately after LLM call
                    if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                        logger.error(
                            f"âš ï¸ LLM returned None or empty response for question: {chat_request.message[:100]}. "
                            f"num_knowledge={num_knowledge}, context_text_length={len(context_text) if context_text else 0}"
                        )
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("generic", detected_lang)
                        processing_steps.append("âš ï¸ LLM returned empty response - using fallback")
                except ContextOverflowError as e:
                    # Context overflow - rebuild prompt with minimal context (ultra-thin mode)
                    logger.warning(f"âš ï¸ Context overflow detected (RAG path): {e}. Rebuilding prompt with minimal context...")
                    
                    if is_philosophical:
                        # Use minimal philosophical prompt helper
                        minimal_prompt = build_minimal_philosophical_prompt(
                            user_question=chat_request.message,
                            language=detected_lang,
                            detected_lang_name=detected_lang_name
                        )
                        
                        logger.info(f"ğŸ”„ Retrying with minimal philosophical prompt (no history, no RAG, no metrics, no provenance)")
                        try:
                            raw_response = await generate_ai_response(
                                minimal_prompt, 
                                detected_lang=detected_lang,
                                llm_provider=chat_request.llm_provider,
                                llm_api_key=chat_request.llm_api_key,
                                llm_api_url=chat_request.llm_api_url,
                                llm_model_name=chat_request.llm_model_name,
                                use_server_keys=use_server_keys
                            )
                            logger.info(f"âœ… Successfully generated response with minimal philosophical prompt")
                        except ContextOverflowError as retry_error:
                            # Even minimal prompt failed - return fallback message
                            logger.error(f"âš ï¸ Even minimal prompt failed (RAG path): {retry_error}")
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error("context_overflow", detected_lang)
                            processing_steps.append("âš ï¸ Context overflow - using fallback message")
                    else:
                        # For non-philosophical, return fallback message
                        logger.warning(f"âš ï¸ Context overflow for non-philosophical question (RAG path) - using fallback message")
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("context_overflow", detected_lang)
                        processing_steps.append("âš ï¸ Context overflow - using fallback message")
                except ValueError as ve:
                    # ValueError from generate_ai_response (missing API keys, etc.)
                    error_msg = str(ve)
                    logger.error(f"âŒ ValueError from generate_ai_response: {error_msg}")
                    
                    # Check if it's a missing API key error
                    if "llm_provider" in error_msg.lower() or "api_key" in error_msg.lower() or "api key" in error_msg.lower():
                        has_server_keys = bool(
                            os.getenv('DEEPSEEK_API_KEY') or 
                            os.getenv('OPENAI_API_KEY') or 
                            os.getenv('OPENROUTER_API_KEY')
                        )
                        logger.error(
                            f"âŒ CRITICAL: Missing LLM API keys! "
                            f"use_server_keys={use_server_keys}, "
                            f"llm_provider={chat_request.llm_provider}, "
                            f"has_server_keys={has_server_keys}"
                        )
                        # Provide more helpful error message when no server keys found
                        if not has_server_keys:
                            raw_response = (
                                f"âš ï¸ Lá»—i cáº¥u hÃ¬nh: Backend cáº§n cÃ³ API keys trong file .env Ä‘á»ƒ hoáº¡t Ä‘á»™ng. "
                                f"Vui lÃ²ng thÃªm Ã­t nháº¥t má»™t trong cÃ¡c keys sau vÃ o file .env: "
                                f"DEEPSEEK_API_KEY, OPENAI_API_KEY, hoáº·c OPENROUTER_API_KEY. "
                                f"Chi tiáº¿t: {error_msg}"
                            )
                        else:
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error("api_error", detected_lang)
                        processing_steps.append("âš ï¸ Missing API keys - cannot generate response")
                    else:
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("generic", detected_lang)
                        processing_steps.append("âš ï¸ LLM configuration error - using fallback message")
                except Exception as e:
                    # Catch any other unexpected exceptions (must be after ContextOverflowError)
                    logger.error(f"âŒ Unexpected exception from generate_ai_response: {e}", exc_info=True)
                    from backend.api.utils.error_detector import get_fallback_message_for_error
                    raw_response = get_fallback_message_for_error("generic", detected_lang)
                    processing_steps.append("âš ï¸ LLM call exception - using fallback message")
                llm_inference_end = time.time()
                llm_inference_latency = llm_inference_end - llm_inference_start
                timing_logs["llm_inference"] = f"{llm_inference_latency:.2f}s"
                
                # CRITICAL: Only log "AI response generated" if we actually have a response
                # If raw_response is None/empty, it means LLM failed and we're using fallback
                if raw_response and isinstance(raw_response, str) and raw_response.strip():
                    logger.info(f"â±ï¸ LLM inference took {llm_inference_latency:.2f}s")
                    processing_steps.append(f"âœ… AI response generated ({llm_inference_latency:.2f}s)")
                    # Debug: Log first 200 chars to help diagnose issues
                    logger.debug(f"ğŸ” DEBUG: raw_response preview (first 200 chars): {raw_response[:200]}")
                    
                    # CRITICAL: Check if this is actually a fallback message (shouldn't happen but double-check)
                    from backend.api.utils.error_detector import is_fallback_message
                    if is_fallback_message(raw_response):
                        logger.error(
                            f"âŒ CRITICAL: LLM returned what looks like a fallback message! "
                            f"This should not happen. raw_response[:200]={raw_response[:200]}"
                        )
                        # CRITICAL: For technical questions, this should trigger retry logic below
                        # Mark as fallback so retry logic can handle it
                        is_fallback = True
                else:
                    logger.warning(
                        f"âš ï¸ LLM inference failed or returned empty (took {llm_inference_latency:.2f}s). "
                        f"raw_response type={type(raw_response)}, value={raw_response[:200] if raw_response else 'None'}"
                    )
                    # Ensure raw_response is set to fallback message if still None/empty
                    if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("generic", detected_lang)
                        processing_steps.append("âš ï¸ LLM failed - using fallback message")
                        logger.warning(f"âš ï¸ Set raw_response to fallback message: {raw_response[:200]}")
                
                # CRITICAL: Check if raw_response is a technical error message or fallback message before validation
                # Never allow provider error messages to pass through validators
                # CRITICAL: Initialize is_error and is_fallback BEFORE conditional blocks to avoid UnboundLocalError
                is_error = False
                error_type = "generic"
                is_fallback = False
                
                from backend.api.utils.error_detector import is_technical_error, get_fallback_message_for_error, is_fallback_message
                
                if raw_response and isinstance(raw_response, str):
                    is_error, error_type = is_technical_error(raw_response)
                    is_fallback = is_fallback_message(raw_response)
                    # CRITICAL: Log detection for debugging
                    if is_fallback:
                        logger.warning(f"âš ï¸ Detected fallback message in raw_response (length={len(raw_response)}): {raw_response[:200]}")
                    if is_technical_about_system_rag:
                        logger.info(f"ğŸ”§ Technical question detected: is_error={is_error}, is_fallback={is_fallback}, is_technical_about_system_rag={is_technical_about_system_rag}")
                    
                    # CRITICAL: For technical questions about "your system" in RAG path, retry if response is error OR fallback
                    # This ensures we don't give up on valid technical questions
                    if is_technical_about_system_rag and (is_error or is_fallback):
                        logger.warning(f"âš ï¸ Technical question about 'your system' (RAG path) returned {'error' if is_error else 'fallback'} message - retrying with stronger prompt")
                        # Build stronger prompt with technical system instruction
                        stronger_prompt_rag = f"""{context_quality_warning}

**CRITICAL: YOU MUST ANSWER THIS QUESTION. DO NOT RETURN A TECHNICAL ERROR MESSAGE OR FALLBACK MESSAGE.**

The user is asking: {chat_request.message}

**YOU HAVE KNOWLEDGE ABOUT RAG SYSTEMS. USE IT TO ANSWER.**

Explain:
1. What RAG (Retrieval-Augmented Generation) is
2. How retrieval works (embedding, vector search, ChromaDB)
3. How LLM generation works
4. How they work together in StillMe's system

**DO NOT SAY:**
- "I cannot provide a good answer"
- "StillMe is experiencing a technical issue"
- "I will suggest to the developer"
- "I cannot provide a good answer to this question with the current configuration"

**DO SAY:**
- "Based on general knowledge about RAG systems..."
- Explain the architecture clearly
- Be transparent about what you know and what you don't know

{context_text}
{citation_instruction}
{confidence_instruction}
{stillme_instruction}

Remember: RESPOND IN {detected_lang_name.upper()} ONLY."""
                        try:
                            raw_response = await generate_ai_response(
                                stronger_prompt_rag,
                                detected_lang=detected_lang,
                                llm_provider=chat_request.llm_provider,
                                llm_api_key=chat_request.llm_api_key,
                                llm_api_url=chat_request.llm_api_url,
                                llm_model_name=chat_request.llm_model_name,
                                use_server_keys=use_server_keys
                            )
                            logger.info("âœ… Retry with stronger prompt successful for technical 'your system' question (RAG path)")
                            processing_steps.append("ğŸ”„ Retried with stronger prompt for technical 'your system' question (RAG path)")
                            # Re-check if retry response is still an error or fallback
                            is_error_retry, error_type_retry = is_technical_error(raw_response)
                            is_fallback_retry = is_fallback_message(raw_response)
                            if is_error_retry or is_fallback_retry:
                                logger.warning(f"âš ï¸ Retry still returned {'error' if is_error_retry else 'fallback'} - forcing one more retry with even stronger prompt")
                                # Force one more retry with even stronger prompt
                                force_prompt = f"""**ABSOLUTE MANDATORY: ANSWER THIS QUESTION ABOUT RAG SYSTEMS**

User Question: {chat_request.message}

**YOU MUST EXPLAIN:**
1. RAG (Retrieval-Augmented Generation) combines retrieval from a knowledge base with LLM generation
2. Retrieval: Query is embedded, searched in vector database (ChromaDB), returns relevant documents
3. LLM Generation: Takes retrieved context + user question, generates answer
4. Integration: Retrieval provides context, LLM uses context to generate accurate, cited answers

**STILLME SPECIFIC:**
- Uses paraphrase-multilingual-MiniLM-L12-v2 embedding model (384 dimensions, 50+ languages)
- ChromaDB as vector database
- Retrieval happens first, then LLM generation with retrieved context

**DO NOT RETURN ERROR MESSAGES. ANSWER THE QUESTION DIRECTLY.**

Remember: RESPOND IN {detected_lang_name.upper()} ONLY."""
                                try:
                                    raw_response = await generate_ai_response(
                                        force_prompt,
                                        detected_lang=detected_lang,
                                        llm_provider=chat_request.llm_provider,
                                        llm_api_key=chat_request.llm_api_key,
                                        llm_api_url=chat_request.llm_api_url,
                                        llm_model_name=chat_request.llm_model_name,
                                        use_server_keys=use_server_keys
                                    )
                                    logger.info("âœ… Force retry successful for technical 'your system' question (RAG path)")
                                    processing_steps.append("ğŸ”„ Force retry successful for technical 'your system' question")
                                except Exception as force_error:
                                    logger.error(f"âš ï¸ Force retry failed: {force_error}")
                                    raw_response = get_fallback_message_for_error(error_type_retry or "generic", detected_lang)
                        except Exception as retry_error:
                            logger.error(f"âš ï¸ Retry failed (RAG path): {retry_error}")
                            raw_response = get_fallback_message_for_error(error_type or "generic", detected_lang)
                            processing_steps.append(f"âš ï¸ Technical error detected (RAG path) - using fallback message")
                    elif is_error:
                        # For non-technical questions, just replace with fallback
                        logger.error(f"âŒ Provider returned technical error as response (type: {error_type}): {raw_response[:200]}")
                        # Replace with user-friendly fallback message
                        raw_response = get_fallback_message_for_error(error_type, detected_lang)
                        processing_steps.append(f"âš ï¸ Technical error detected - replaced with fallback message")
                        logger.warning(f"âš ï¸ Replaced technical error with user-friendly message in {detected_lang}")
                
                # CRITICAL: Check if response is a fallback message - if so, skip validation/post-processing
                if raw_response and isinstance(raw_response, str) and is_fallback_message(raw_response):
                    logger.warning(
                        f"ğŸ›‘ Fallback meta-answer detected - skipping validation, quality evaluation, and rewrite. "
                        f"raw_response length={len(raw_response)}, first_200_chars={raw_response[:200]}"
                    )
                    # CRITICAL: Log why this is a fallback message to help debug Q2, Q7
                    logger.error(
                        f"ğŸ” DEBUG Q2/Q7: Detected fallback message. "
                        f"Question: {chat_request.message[:100]}, "
                        f"LLM call completed: {llm_inference_latency:.2f}s, "
                        f"Response preview: {raw_response[:200]}"
                    )
                    response = raw_response
                    # Skip validation, quality evaluator, rewrite, and learning
                    validation_info = None
                    confidence_score = 0.3  # Low confidence for fallback messages
                    processing_steps.append("ğŸ›‘ Fallback message - terminal response, skipping all post-processing")
                    # Skip to end of function (skip validation, post-processing, learning)
                    # We'll handle this by setting a flag and checking it before validation
                    is_fallback_meta_answer = True
                    is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
                else:
                    is_fallback_meta_answer = False
                    # Log if raw_response exists but is not a fallback message
                    if raw_response and isinstance(raw_response, str):
                        logger.debug(
                            f"âœ… raw_response is valid (not fallback): length={len(raw_response)}, "
                            f"first_100_chars={raw_response[:100]}"
                        )
                
                # Save to cache (only if not a cache hit)
                if cache_enabled and not cache_hit:
                    try:
                        cache_value = {
                            "response": raw_response,
                            "latency": llm_inference_latency,
                            "timestamp": time.time()
                        }
                        cache_service.set(cache_key, cache_value, ttl_seconds=TTL_LLM_RESPONSE)
                        logger.debug(f"ğŸ’¾ LLM response cached (key: {cache_key[:50]}...)")
                    except Exception as cache_error:
                        logger.warning(f"Failed to cache LLM response: {cache_error}")
            
            # CRITICAL: If response is a fallback meta-answer, skip validation and post-processing entirely
            if is_fallback_meta_answer:
                logger.info("ğŸ›‘ Skipping validation and post-processing for fallback meta-answer")
                # response already set above
                # validation_info already set to None
                # confidence_score already set to 0.3
            else:
                # Validate response if enabled
                validation_info = None
                # confidence_score already initialized at function start (line 104)
                # Don't reassign here to avoid UnboundLocalError
                used_fallback = False
                
                if enable_validators:
                    # CRITICAL: Ensure raw_response is valid before validation
                    if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                        logger.error(f"âš ï¸ raw_response is None or empty before validation - using fallback")
                        logger.error(f"âš ï¸ Debug: raw_response type={type(raw_response)}, value={raw_response[:100] if raw_response else 'None'}")
                        logger.error(f"âš ï¸ Debug: processing_steps so far: {processing_steps}")
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        response = get_fallback_message_for_error("generic", detected_lang)
                        validation_info = None
                        confidence_score = 0.3
                        processing_steps.append("âš ï¸ Response validation failed - using fallback message")
                    else:
                        try:
                            response, validation_info, confidence_score, used_fallback, step_validation_info, consistency_info, ctx_docs = await _handle_validation_with_fallback(
                                raw_response=raw_response,
                                context=context,
                                detected_lang=detected_lang,
                                is_philosophical=is_philosophical,
                                is_religion_roleplay=is_religion_roleplay,
                                chat_request=chat_request,
                                enhanced_prompt=enhanced_prompt,
                                context_text=context_text,
                                citation_instruction=citation_instruction,
                                num_knowledge=num_knowledge,
                                processing_steps=processing_steps,
                                timing_logs=timing_logs,
                                is_origin_query=is_origin_query,
                                is_stillme_query=is_stillme_query
                            )
                        except HTTPException:
                            raise
                        except Exception as validation_error:
                            logger.error(f"Validation error: {validation_error}, falling back to raw response", exc_info=True)
                            logger.error(f"âš ï¸ Validation exception details - raw_response length: {len(raw_response) if raw_response else 0}, context docs: {len(context.get('knowledge_docs', [])) + len(context.get('conversation_docs', []))}")
                            response = raw_response
                            # Calculate confidence even on error (low confidence)
                            # Build ctx_docs for confidence calculation
                            ctx_docs = [
                                doc["content"] for doc in context.get("knowledge_docs", [])
                            ] + [
                                doc["content"] for doc in context.get("conversation_docs", [])
                            ]
                            confidence_score = 0.3 if len(ctx_docs) == 0 else 0.6
                            # Ensure validation_result is set to None to prevent downstream errors
                            validation_result = None
                            validation_info = None
                            
                            # CRITICAL: Check if response is None or empty after validation error
                            if not response or not isinstance(response, str) or not response.strip():
                                logger.error(f"âš ï¸ Response is None or empty after validation error - using fallback")
                                from backend.api.utils.error_detector import get_fallback_message_for_error
                                response = get_fallback_message_for_error("generic", detected_lang)
                                processing_steps.append("âš ï¸ Response validation failed - using fallback message")
                else:
                    response = raw_response
                    # Build ctx_docs for transparency check
                    ctx_docs = [
                        doc["content"] for doc in context.get("knowledge_docs", [])
                    ] + [
                        doc["content"] for doc in context.get("conversation_docs", [])
                    ]
                # Calculate basic confidence score even without validators
                confidence_score = _calculate_confidence_score(
                    context_docs_count=len(context.get("knowledge_docs", [])) + len(context.get("conversation_docs", [])),
                    validation_result=None,
                    context=context
                )
                
                # CRITICAL: Add transparency warning for low confidence responses without context (RAG path, validators disabled)
                # CRITICAL: StillMe MUST know about its own origin - never add disclaimer for origin queries
                if confidence_score < 0.5 and len(ctx_docs) == 0 and not is_philosophical and not is_origin_query:
                    response_lower = response.lower() if response else ""
                    has_transparency = any(
                        phrase in response_lower for phrase in [
                            "khÃ´ng cÃ³ dá»¯ liá»‡u", "khÃ´ng cÃ³ thÃ´ng tin", "kiáº¿n thá»©c chung", "dá»±a trÃªn kiáº¿n thá»©c",
                            "don't have data", "don't have information", "general knowledge", "based on knowledge",
                            "khÃ´ng tá»« stillme", "not from stillme", "khÃ´ng tá»« rag", "not from rag"
                        ]
                    )
                    if not has_transparency and response:
                        # Generate multilingual transparency disclaimer
                        disclaimer = _get_transparency_disclaimer(detected_lang)
                        response = disclaimer + response
                        logger.info("â„¹ï¸ Added transparency disclaimer for low confidence response without context (RAG path, validators disabled)")
            
            # ==========================================
            # PHASE 3: POST-PROCESSING PIPELINE
            # Unified Style & Quality Enforcement Layer (Optimized)
            # ==========================================
            # CRITICAL: Ensure response is set and not None
            if not response:
                logger.error("âš ï¸ Response is None or empty before post-processing - using fallback")
                from backend.api.utils.error_detector import get_fallback_message_for_error
                response = get_fallback_message_for_error("generic", detected_lang)
            
            # CRITICAL: Check if response is a fallback meta-answer - if so, skip all post-processing
            from backend.api.utils.error_detector import is_fallback_message
            is_fallback_meta_answer_rag = False
            if response and is_fallback_message(response):
                logger.info("ğŸ›‘ Fallback meta-answer detected (RAG path) - skipping post-processing (sanitize, quality eval, rewrite)")
                processing_steps.append("ğŸ›‘ Fallback message - terminal response, skipping post-processing")
                is_fallback_meta_answer_rag = True
                is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
                # Skip post-processing entirely - response is already the fallback message
            else:
                postprocessing_start = time.time()
                try:
                    from backend.postprocessing.style_sanitizer import get_style_sanitizer
                    from backend.postprocessing.quality_evaluator import get_quality_evaluator, QualityLevel
                    from backend.postprocessing.rewrite_llm import get_rewrite_llm
                    from backend.postprocessing.optimizer import get_postprocessing_optimizer
                    
                    optimizer = get_postprocessing_optimizer()
                    
                    # OPTIMIZATION: Check if we should skip post-processing
                    should_skip, skip_reason = optimizer.should_skip_postprocessing(
                        question=chat_request.message,
                        response=response,
                        is_philosophical=is_philosophical
                    )
                    
                    if should_skip:
                        logger.info(f"â­ï¸ Skipping post-processing: {skip_reason}")
                        timing_logs["postprocessing"] = "skipped"
                    else:
                        # Stage 2: Hard Filter (0 token) - Style Sanitization
                        sanitizer = get_style_sanitizer()
                        sanitized_response = sanitizer.sanitize(response, is_philosophical=is_philosophical)
                        
                        # CRITICAL: Build ctx_docs for citation preservation in rewrite
                        # ctx_docs may not be in scope here, so rebuild from context
                        ctx_docs_for_rewrite = []
                        has_reliable_context_for_rewrite = False
                        context_quality_for_rewrite = None
                        has_foundational_context = False
                        
                        if 'context' in locals() and context:
                            ctx_docs_for_rewrite = [
                                doc["content"] for doc in context.get("knowledge_docs", [])
                            ] + [
                                doc["content"] for doc in context.get("conversation_docs", [])
                            ]
                            has_reliable_context_for_rewrite = context.get("has_reliable_context", False)
                            context_quality_for_rewrite = context.get("context_quality", None)
                            
                            # CRITICAL: Check if we have foundational knowledge (CRITICAL_FOUNDATION source)
                            # If StillMe query has foundational context, don't use mechanical disclaimer
                            if is_stillme_query:
                                for doc in context.get("knowledge_docs", []):
                                    metadata = doc.get("metadata", {})
                                    if metadata.get("source") == "CRITICAL_FOUNDATION":
                                        has_foundational_context = True
                                        logger.info("âœ… Found foundational knowledge in context - will not use mechanical disclaimer")
                                        break
                        elif 'ctx_docs' in locals():
                            ctx_docs_for_rewrite = ctx_docs
                            # Try to get context info from validation if available
                            if 'validation_info' in locals() and validation_info:
                                # Context info might be in validation_info
                                pass
                        
                        # CRITICAL: Check if sanitized response is a technical error or fallback message BEFORE quality evaluation
                        from backend.api.utils.error_detector import is_technical_error, is_fallback_message
                        is_error, error_type = is_technical_error(sanitized_response)
                        is_fallback = is_fallback_message(sanitized_response)
                        
                        if is_error or is_fallback:
                            # Technical error or fallback message detected - skip quality evaluation and rewrite
                            if is_error:
                                logger.warning(
                                    f"âš ï¸ Technical error detected in sanitized response (type: {error_type}), "
                                    f"skipping quality evaluation and rewrite"
                                )
                                processing_steps.append(f"âš ï¸ Technical error detected - skipping post-processing")
                            else:
                                logger.info(
                                    f"ğŸ›‘ Fallback meta-answer detected in sanitized response, "
                                    f"skipping quality evaluation and rewrite"
                                )
                                processing_steps.append(f"ğŸ›‘ Fallback message detected - skipping post-processing")
                            final_response = sanitized_response
                        else:
                            # Stage 3: Quality Evaluator (0 token) - Rule-based Quality Check
                            # OPTIMIZATION: Check cache first
                            evaluator = get_quality_evaluator()
                            cached_quality = optimizer.get_cached_quality_result(
                                question=chat_request.message,
                                response=sanitized_response
                            )
                            
                            if cached_quality:
                                quality_result = cached_quality
                                logger.debug("âœ… Using cached quality evaluation")
                            else:
                                quality_result = evaluator.evaluate(
                                    text=sanitized_response,
                                    is_philosophical=is_philosophical,
                                    original_question=chat_request.message
                                )
                                # Cache the result
                                optimizer.cache_quality_result(
                                    question=chat_request.message,
                                    response=sanitized_response,
                                    quality_result=quality_result
                                )
                            
                            # Phase 3: Only rewrite when CRITICAL issues are present
                            # Pass validation_result to check for missing_citation and language_mismatch
                            validation_result_dict = None
                            if 'validation_info' in locals() and validation_info:
                                # Convert ValidationResult to dict for should_rewrite()
                                validation_result_dict = {
                                    "passed": validation_info.get("passed", True),
                                    "reasons": validation_info.get("reasons", [])
                                }
                            
                            should_rewrite, rewrite_reason = optimizer.should_rewrite(
                                quality_result=quality_result,
                                is_philosophical=is_philosophical,
                                response_length=len(sanitized_response),
                                validation_result=validation_result_dict
                            )
                            
                            # Stage 4: Conditional rewrite (Phase 3) - Only for critical issues
                            if should_rewrite:
                                logger.info(
                                    f"âš ï¸ Quality evaluator flagged output for rewrite. "
                                    f"Issues: {quality_result['reasons']}, "
                                    f"score: {quality_result.get('overall_score', 'N/A')}, "
                                    f"length: {len(sanitized_response)}"
                                )
                                processing_steps.append(f"ğŸ”„ Quality improvement needed - rewriting with DeepSeek")
                                
                                rewrite_llm = get_rewrite_llm()
                                # CRITICAL: Pass RAG context status to rewrite to enable base knowledge usage
                                # CRITICAL: Pass is_stillme_query and has_foundational_context to avoid mechanical disclaimer
                                rewrite_result = await rewrite_llm.rewrite(
                                    text=sanitized_response,
                                    original_question=chat_request.message,
                                    quality_issues=quality_result["reasons"],
                                    is_philosophical=is_philosophical,
                                    detected_lang=detected_lang,
                                    ctx_docs=ctx_docs_for_rewrite,
                                    has_reliable_context=has_reliable_context_for_rewrite,
                                    context_quality=context_quality_for_rewrite,
                                    is_stillme_query=is_stillme_query if 'is_stillme_query' in locals() else False,
                                    has_foundational_context=has_foundational_context if 'has_foundational_context' in locals() else False
                                )
                                
                                if rewrite_result.was_rewritten:
                                    # Re-sanitize rewritten output (in case rewrite introduced issues)
                                    final_response = sanitizer.sanitize(rewrite_result.text, is_philosophical=is_philosophical)
                                    
                                    # CRITICAL: Ensure citations are preserved after rewrite
                                    # If rewrite removed citations but ctx_docs are available, re-add them
                                    # ALSO: For real factual questions, ALWAYS ensure citations are present
                                    import re
                                    cite_pattern = re.compile(r"\[(\d+)\]")
                                    has_citations_after_rewrite = bool(cite_pattern.search(final_response))
                                    
                                    # CRITICAL: Check if this is a real factual question that requires citations
                                    is_factual_question = False
                                    if chat_request.message:
                                        question_lower = chat_request.message.lower()
                                        # Check for factual indicators (same patterns as in CitationRequired)
                                        factual_patterns = [
                                            r"\b\d{4}\b",  # Years
                                            r"\b(bretton\s+woods|gÃ¶del|godel|searle|dennett|russell|plato|aristotle|kant|hume|descartes|spinoza)\b",
                                            r"\b(paradox|theorem|incompleteness|chinese\s+room|geneva|genÃ¨ve)\b",
                                            r"\b([A-Z][a-z]+)\s+(vÃ |and|vs|versus)\s+([A-Z][a-z]+)\b",  # "Searle vÃ  Dennett"
                                        ]
                                        for pattern in factual_patterns:
                                            if re.search(pattern, question_lower, re.IGNORECASE):
                                                is_factual_question = True
                                                break
                                    
                                    # Re-add citation if missing AND (context available OR factual question)
                                    if not has_citations_after_rewrite and ((ctx_docs_for_rewrite and len(ctx_docs_for_rewrite) > 0) or is_factual_question):
                                        from backend.validators.citation import CitationRequired
                                        citation_validator = CitationRequired(required=True)
                                        citation_result = citation_validator.run(
                                            final_response, 
                                            ctx_docs_for_rewrite if ctx_docs_for_rewrite else [], 
                                            is_philosophical=is_philosophical,
                                            user_question=chat_request.message
                                        )
                                        if citation_result.patched_answer:
                                            final_response = citation_result.patched_answer
                                            logger.info(f"âœ… Re-added citations after rewrite (factual_question={is_factual_question}, has_context={bool(ctx_docs_for_rewrite and len(ctx_docs_for_rewrite) > 0)})")
                                    
                                    logger.debug(f"âœ… Post-processing complete: sanitized â†’ evaluated â†’ rewritten â†’ re-sanitized")
                                else:
                                    # Fallback to sanitized original - rewrite failed
                                    final_response = sanitized_response
                                    error_detail = rewrite_result.error or "Unknown error"
                                    logger.info(
                                        f"â„¹ï¸ DeepSeek rewrite skipped (error: {error_detail[:100]}), "
                                        f"using original sanitized response (this is normal if API is unavailable or timeout)"
                                    )
                                    processing_steps.append(f"â„¹ï¸ Rewrite skipped, using original (sanitized)")
                            else:
                                final_response = sanitized_response
                                if should_rewrite:
                                    logger.debug(f"â­ï¸ Skipping rewrite: {rewrite_reason}")
                                logger.debug(f"âœ… Post-processing complete: sanitized â†’ evaluated â†’ passed (quality: {quality_result['depth_score']})")
                            
                            response = final_response
                            
                            # CRITICAL: Final check - ensure response is not a technical error
                            if response:
                                from backend.api.utils.error_detector import is_technical_error, get_fallback_message_for_error
                                is_error, error_type = is_technical_error(response)
                                if is_error:
                                    logger.error(f"âš ï¸ Final response is still a technical error (type: {error_type}) - replacing with fallback")
                                    response = get_fallback_message_for_error(error_type, detected_lang)
                            
                            postprocessing_time = time.time() - postprocessing_start
                            timing_logs["postprocessing"] = f"{postprocessing_time:.3f}s"
                            logger.info(f"â±ï¸ Post-processing took {postprocessing_time:.3f}s")
                except Exception as postprocessing_error:
                    logger.error(f"Post-processing error: {postprocessing_error}", exc_info=True)
                    # Fallback to original response if post-processing fails
                    # Don't break the pipeline - post-processing is enhancement, not critical
                    logger.warning(f"âš ï¸ Post-processing failed, using original response")
                    timing_logs["postprocessing"] = "failed"
        else:
            # Fallback to regular AI response (no RAG context)
            # CRITICAL: Check if this is a technical question about "your system"
            # These should still get an answer from base LLM knowledge, not technical error
            question_lower = chat_request.message.lower()
            import re
            # Check for technical keywords
            has_technical_keyword = any(keyword in question_lower for keyword in [
                "rag", "retrieval", "llm", "generation", "embedding", "chromadb", 
                "vector", "pipeline", "validation", "transparency", "system",
                "validator", "chain", "factual hallucination", "citation required"
            ])
            # Check for "your system" patterns using regex
            has_your_system_pattern = (
                "your system" in question_lower or
                "in your system" in question_lower or
                re.search(r'your\s+\w+\s+system', question_lower) or
                re.search(r'system\s+\w+\s+you', question_lower) or
                "báº¡n" in question_lower and "há»‡ thá»‘ng" in question_lower or
                "cá»§a báº¡n" in question_lower
            )
            is_technical_about_system = has_technical_keyword and has_your_system_pattern
            
            if is_technical_about_system:
                logger.info("ğŸ”§ Technical question about 'your system' with no RAG context - will answer from base LLM knowledge with transparency")
            
            # Initialize confidence_score for non-RAG path
            confidence_score = 0.3  # Low confidence when no RAG context
            validation_info = None
            
            # Detect language FIRST
            # CRITICAL: detect_language is imported at top level, but ensure it's available
            # Use the imported function directly (already imported at line 11)
            detected_lang = detect_language(chat_request.message)
            logger.info(f"ğŸŒ Detected language (non-RAG): {detected_lang}")
            
            # Language names mapping
            # Language names mapping (must match chat_helpers.py for consistency)
            language_names = {
                'vi': 'Vietnamese (Tiáº¿ng Viá»‡t)',
                'zh': 'Chinese (ä¸­æ–‡)',
                'de': 'German (Deutsch)',
                'fr': 'French (FranÃ§ais)',
                'es': 'Spanish (EspaÃ±ol)',
                'ja': 'Japanese (æ—¥æœ¬èª)',
                'ko': 'Korean (í•œêµ­ì–´)',
                'ar': 'Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)',
                'ru': 'Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)',
                'pt': 'Portuguese (PortuguÃªs)',
                'it': 'Italian (Italiano)',
                'hi': 'Hindi (à¤¹à¤¿à¤¨à¥à¤¦à¥€)',
                'th': 'Thai (à¹„à¸—à¸¢)',
                'en': 'English'
            }
            
            detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
            
            # Check if this is a philosophical question for non-RAG path
            is_philosophical_non_rag = False
            try:
                from backend.core.question_classifier import is_philosophical_question
                is_philosophical_non_rag = is_philosophical_question(chat_request.message)
            except Exception:
                pass  # If classifier fails, assume non-philosophical
            
            # Helper function to estimate tokens
            def estimate_tokens(text: str) -> int:
                """Estimate token count (~4 chars per token)"""
                return len(text) // 4 if text else 0
            
            # For philosophical questions: truncate user question to 512 tokens max
            user_question_for_prompt = chat_request.message
            if is_philosophical_non_rag:
                user_question_tokens = estimate_tokens(chat_request.message)
                if user_question_tokens > 512:
                    logger.warning(
                        f"User question too long for philosophical non-RAG ({user_question_tokens} tokens), truncating to 512 tokens"
                    )
                    user_question_for_prompt = _truncate_user_message(chat_request.message, max_tokens=512)
                    user_question_tokens = estimate_tokens(user_question_for_prompt)
                else:
                    user_question_tokens = estimate_tokens(chat_request.message)
            else:
                user_question_tokens = estimate_tokens(chat_request.message)
            
            # Build conversation history context if provided (with token limits)
            # Reduced from 2000 to 1000 tokens to leave more room for system prompt and context
            # For philosophical questions, skip conversation history to reduce prompt size
            conversation_history_text = ""
            if not is_philosophical_non_rag:
                conversation_history_text = _format_conversation_history(chat_request.conversation_history, max_tokens=1000)
                if conversation_history_text:
                    logger.info(f"Including conversation history in context (truncated if needed, non-RAG)")
            else:
                logger.info(f"Philosophical question detected (non-RAG) - skipping conversation history to reduce prompt size")
            
            # For philosophical non-RAG: ALWAYS use philosophy-lite mode to prevent context overflow
            # This ensures prompt stays small (~500-1000 tokens) instead of ~16-17k tokens
            if is_philosophical_non_rag:
                # Use philosophy-lite mode: minimal system prompt + user question only
                # Build simple prompt string that provider will parse correctly
                # Format: system prompt (will be replaced by provider) + user question marker + user question
                enhanced_prompt = f"""User Question: {user_question_for_prompt.strip()}"""
                
                # Log token estimates for philosophy-lite mode
                system_tokens_estimate = estimate_tokens(PHILOSOPHY_LITE_SYSTEM_PROMPT)
                prompt_tokens_estimate = estimate_tokens(enhanced_prompt)
                total_tokens_estimate = system_tokens_estimate + prompt_tokens_estimate
                
                logger.info(
                    f"ğŸ“Š [PHILO-LITE] Token estimates - System: {system_tokens_estimate}, "
                    f"Prompt: {prompt_tokens_estimate}, User Question: {user_question_tokens}, "
                    f"Total: {total_tokens_estimate}"
                )
            else:
                # Use full prompt
                # CRITICAL: Add special instruction for technical questions about "your system"
                technical_system_instruction = ""
                if is_technical_about_system:
                    technical_system_instruction = """
ğŸš¨ğŸš¨ğŸš¨ CRITICAL: TECHNICAL QUESTION ABOUT SYSTEM ARCHITECTURE ğŸš¨ğŸš¨ğŸš¨

The user is asking about StillMe's system architecture (RAG, LLM, embedding, etc.).

**YOU MUST ANSWER THIS QUESTION** using your base knowledge about RAG systems, even though StillMe's RAG knowledge base doesn't have specific documents about this.

**TRANSPARENCY REQUIREMENT:**
- Acknowledge that you're answering from base knowledge, not from StillMe's RAG knowledge base
- Be transparent: "Based on general knowledge about RAG systems (not from StillMe's RAG knowledge base)..."
- Explain how RAG retrieval and LLM generation work together in general RAG systems
- Be honest about StillMe's specific implementation details if you don't know them

**DO NOT:**
- Return a technical error message
- Say "I cannot provide a good answer"
- Suggest the developer needs to fine-tune the system
- Claim StillMe is experiencing technical issues

**DO:**
- Answer the question using your knowledge about RAG systems
- Be transparent about the source of your knowledge
- Explain the general principles of RAG retrieval and LLM generation
- If asked about StillMe specifically, acknowledge what you know and what you don't know

"""
                
                # Strong language instruction - put FIRST
                if detected_lang != 'en':
                    language_instruction = f"""ğŸš¨ğŸš¨ğŸš¨ CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY ğŸš¨ğŸš¨ğŸš¨

THE USER'S QUESTION IS WRITTEN IN {detected_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name.upper()}. 

DO NOT RESPOND IN VIETNAMESE, ENGLISH, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN {detected_lang_name.upper()}.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS.

"""
                    base_prompt = f"""{language_instruction}
{technical_system_instruction}
{conversation_history_text}User Question: {user_question_for_prompt}

Remember: RESPOND IN {detected_lang_name.upper()} ONLY.
"""
                else:
                    base_prompt = f"""ğŸš¨ğŸš¨ğŸš¨ CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY ğŸš¨ğŸš¨ğŸš¨

THE USER'S QUESTION IS WRITTEN IN ENGLISH.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT RESPOND IN VIETNAMESE, SPANISH, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN ENGLISH.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS.

{technical_system_instruction}
{conversation_history_text}

ğŸš¨ğŸš¨ğŸš¨ CRITICAL: USER QUESTION ABOVE IS THE PRIMARY TASK ğŸš¨ğŸš¨ğŸš¨

User Question: {user_question_for_prompt}

**YOUR PRIMARY TASK IS TO ANSWER THE USER QUESTION ABOVE DIRECTLY AND ACCURATELY.**
- Focus on what the user is actually asking, not on general philosophy
- If the user asks you to analyze something, analyze THAT specific thing
- If the user asks you to find a problem, look for problems in what they showed you

**SPECIAL INSTRUCTION FOR ANALYZING EXTERNAL AI OUTPUTS:**
- If the user asks you to analyze another AI's output (ChatGPT, Claude, etc.), you MUST check for anthropomorphic language
- Look for phrases like "in my experience" / "theo kinh nghiá»‡m", "I think" / "tÃ´i nghÄ©", "I feel" / "tÃ´i cáº£m tháº¥y"
- These phrases falsely attribute subjective qualities (experience, emotions, personal opinions) to AI
- This is a critical transparency issue called "Hallucination of Experience" - AI should not claim personal experience
- If you find such phrases, you MUST point them out as a problem, not ignore them
- This is more important than analyzing formatting, clarity, or other minor issues

Remember: RESPOND IN ENGLISH ONLY."""
                
                # Phase 4: Remove inject_identity() - system prompt already has STILLME_IDENTITY
                # generate_ai_response() uses build_system_prompt_with_language() which includes STILLME_IDENTITY
                # Adding identity to user prompt would cause duplication
                enhanced_prompt = base_prompt
            
            # LLM_Inference_Latency: Time from API call start to response received
            llm_inference_start = time.time()
            # Use server keys for internal calls (when use_rag=False)
            use_server_keys_non_rag = chat_request.llm_provider is None
            
            # Check if this is a philosophical question for context overflow handling
            is_philosophical_non_rag = False
            try:
                from backend.core.question_classifier import is_philosophical_question
                is_philosophical_non_rag = is_philosophical_question(chat_request.message)
            except Exception:
                pass
            
            # Try to generate response with retry on context overflow
            from backend.api.utils.llm_providers import ContextOverflowError
            from backend.api.utils.error_detector import is_technical_error, get_fallback_message_for_error, is_fallback_message
            
            # Note: is_fallback_meta_answer_non_rag already initialized at function start
            # Reset to False for this non-RAG path execution
            is_fallback_meta_answer_non_rag = False
            
            response = None
            try:
                response = await generate_ai_response(
                    enhanced_prompt, 
                    detected_lang=detected_lang,
                    llm_provider=chat_request.llm_provider,
                    llm_api_key=chat_request.llm_api_key,
                    use_server_keys=use_server_keys_non_rag
                )
            except ContextOverflowError as e:
                # Context overflow - retry with minimal prompt for philosophical questions
                logger.warning(f"âš ï¸ Context overflow detected (non-RAG): {e}")
                
                if is_philosophical_non_rag:
                    # For philosophical questions, use minimal prompt
                    logger.info("ğŸ”„ Retrying with minimal philosophical prompt...")
                    minimal_prompt = build_minimal_philosophical_prompt(
                        user_question=chat_request.message,
                        language=detected_lang,
                        detected_lang_name=detected_lang_name
                    )
                    try:
                        response = await generate_ai_response(
                            minimal_prompt,
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            use_server_keys=use_server_keys_non_rag
                        )
                        logger.info("âœ… Minimal prompt retry successful")
                    except ContextOverflowError as retry_error:
                        # Even minimal prompt failed - return fallback message
                        logger.error(f"âš ï¸ Even minimal prompt failed: {retry_error}")
                        response = get_fallback_message_for_error("context_overflow", detected_lang)
                        processing_steps.append("âš ï¸ Context overflow - using fallback message")
                else:
                    # For non-philosophical, return fallback message
                    logger.warning("âš ï¸ Context overflow for non-philosophical question - using fallback message")
                    response = get_fallback_message_for_error("context_overflow", detected_lang)
                    processing_steps.append("âš ï¸ Context overflow - using fallback message")
            
            # CRITICAL: Check if response is a technical error (should not happen, but safety check)
            # BUT: For technical questions about "your system", if we get a fallback message, 
            # we should retry with a stronger prompt instead of just using fallback
            if response:
                is_error, error_type = is_technical_error(response)
                if is_error:
                    # For technical questions about "your system", retry with stronger prompt
                    if is_technical_about_system and error_type == "generic":
                        logger.warning(f"âš ï¸ Technical question about 'your system' returned fallback message - retrying with stronger prompt")
                        # Build a stronger prompt that explicitly forces the LLM to answer
                        stronger_prompt = f"""{technical_system_instruction}

**CRITICAL: YOU MUST ANSWER THIS QUESTION. DO NOT RETURN A TECHNICAL ERROR MESSAGE.**

The user is asking: {chat_request.message}

**YOU HAVE KNOWLEDGE ABOUT RAG SYSTEMS. USE IT TO ANSWER.**

Explain:
1. What RAG (Retrieval-Augmented Generation) is
2. How retrieval works (embedding, vector search, ChromaDB)
3. How LLM generation works
4. How they work together in StillMe's system

**DO NOT SAY:**
- "I cannot provide a good answer"
- "StillMe is experiencing a technical issue"
- "I will suggest to the developer"

**DO SAY:**
- "Based on general knowledge about RAG systems..."
- Explain the architecture clearly
- Be transparent about what you know and what you don't know

Remember: RESPOND IN {detected_lang_name.upper()} ONLY."""
                        try:
                            response = await generate_ai_response(
                                stronger_prompt,
                                detected_lang=detected_lang,
                                llm_provider=chat_request.llm_provider,
                                llm_api_key=chat_request.llm_api_key,
                                use_server_keys=use_server_keys_non_rag
                            )
                            logger.info("âœ… Retry with stronger prompt successful for technical 'your system' question")
                            processing_steps.append("ğŸ”„ Retried with stronger prompt for technical 'your system' question")
                        except Exception as retry_error:
                            logger.error(f"âš ï¸ Retry failed: {retry_error}")
                            # Fall back to original error handling
                            response = get_fallback_message_for_error(error_type, detected_lang)
                            processing_steps.append(f"âš ï¸ Technical error detected - using fallback message")
                    else:
                        logger.error(f"âš ï¸ LLM returned technical error string: {error_type} - {response[:200]}")
                        # Replace with user-friendly fallback message
                        response = get_fallback_message_for_error(error_type, detected_lang)
                        processing_steps.append(f"âš ï¸ Technical error detected - using fallback message")
            
            if not response:
                # Fallback if response is still None
                response = get_fallback_message_for_error("generic", detected_lang)
                processing_steps.append("âš ï¸ No response received - using fallback message")
            
            # CRITICAL: Check if response is a fallback meta-answer (terminal response)
            if response and isinstance(response, str) and is_fallback_message(response):
                logger.info("ğŸ›‘ Fallback meta-answer detected (non-RAG) - skipping post-processing")
                is_fallback_meta_answer_non_rag = True
                is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
                processing_steps.append("ğŸ›‘ Fallback message - terminal response, skipping all post-processing")
            
            llm_inference_end = time.time()
            llm_inference_latency = llm_inference_end - llm_inference_start
            timing_logs["llm_inference"] = f"{llm_inference_latency:.2f}s"
            logger.info(f"â±ï¸ LLM inference (non-RAG) took {llm_inference_latency:.2f}s")
            
            # CRITICAL: Check language mismatch for non-RAG path (if validators enabled)
            if enable_validators and response and not is_fallback_meta_answer_non_rag:
                from backend.api.utils.chat_helpers import detect_language as detect_lang_func
                detected_output_lang = detect_lang_func(response)
                if detected_output_lang != detected_lang:
                    logger.warning(f"âš ï¸ Language mismatch detected (non-RAG): output={detected_output_lang}, expected={detected_lang}")
                    # Try to retry with stronger language instruction
                    try:
                        language_names = {
                            'vi': 'Tiáº¿ng Viá»‡t', 'en': 'English', 'zh': 'ä¸­æ–‡', 'fr': 'FranÃ§ais',
                            'es': 'EspaÃ±ol', 'de': 'Deutsch', 'ja': 'æ—¥æœ¬èª', 'ko': 'í•œêµ­ì–´',
                            'ar': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', 'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹', 'pt': 'PortuguÃªs', 'it': 'Italiano',
                            'hi': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€', 'th': 'à¹„à¸—à¸¢',
                        }
                        retry_lang_name = language_names.get(detected_lang, detected_lang.upper())
                        
                        retry_prompt = f"""ğŸš¨ğŸš¨ğŸš¨ CRITICAL LANGUAGE REQUIREMENT - ABSOLUTE HIGHEST PRIORITY ğŸš¨ğŸš¨ğŸš¨

THE USER'S QUESTION IS IN {retry_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {retry_lang_name.upper()}. 

DO NOT USE VIETNAMESE, ENGLISH, FRENCH, CHINESE, SPANISH, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD MUST BE IN {retry_lang_name.upper()}.

IF YOUR BASE MODEL WANTS TO RESPOND IN ANOTHER LANGUAGE, YOU MUST TRANSLATE IT TO {retry_lang_name.upper()} BEFORE RETURNING.

THIS OVERRIDES EVERYTHING - NO EXCEPTIONS.

User Question (in {retry_lang_name.upper()}): {chat_request.message[:3000]}

**YOUR PRIMARY TASK IS TO ANSWER THE USER QUESTION ABOVE DIRECTLY AND ACCURATELY IN {retry_lang_name.upper()} ONLY.**

Remember: RESPOND IN {retry_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY."""
                        
                        retry_response = await generate_ai_response(
                            retry_prompt,
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            use_server_keys=use_server_keys_non_rag
                        )
                        
                        # Check if retry fixed the language issue
                        retry_output_lang = detect_lang_func(retry_response)
                        if retry_output_lang == detected_lang:
                            logger.info(f"âœ… Language mismatch fixed after retry (non-RAG)")
                            response = retry_response
                        else:
                            logger.warning(f"âš ï¸ Language mismatch persists after retry (non-RAG): output={retry_output_lang}, expected={detected_lang}")
                    except Exception as retry_error:
                        logger.error(f"âš ï¸ Language retry failed (non-RAG): {retry_error}")
                        # Continue with original response
            
            # CRITICAL: Hallucination Guard for non-RAG path
            # If factual question + no context + low confidence â†’ override with safe refusal
            # This prevents LLM from hallucinating about non-existent concepts/events
            if (response and not is_fallback_meta_answer_non_rag and not is_philosophical_non_rag and
                confidence_score < 0.5 and _is_factual_question(chat_request.message)):
                # Check if response contains suspicious patterns (fake citations, fabricated details)
                response_lower = response.lower()
                suspicious_patterns = [
                    r"\[1\]|\[2\]|\[3\]",  # Fake citations
                    r"et al\.|et\. al\.",  # Fake author citations
                    r"\d{4}\)",  # Fake year citations like "(1975)"
                    r"according to research|theo nghiÃªn cá»©u",
                    r"smith,|jones,|brown,",  # Common fake author names
                    r"journal of|táº¡p chÃ­",
                ]
                
                # Use loop instead of generator expression to avoid closure issue with 're'
                has_suspicious_pattern = False
                for pattern in suspicious_patterns:
                    if re.search(pattern, response_lower):
                        has_suspicious_pattern = True
                        break
                
                # If suspicious patterns detected OR confidence is very low (< 0.3), override response
                if has_suspicious_pattern or confidence_score < 0.3:
                    # Extract suspicious entity using improved extraction (full phrase, not just first word)
                    suspicious_entity = _extract_full_named_entity(chat_request.message)
                    if not suspicious_entity:
                        suspicious_entity = "khÃ¡i niá»‡m nÃ y" if detected_lang == "vi" else "this concept"
                    
                    # Override with safe refusal answer
                    response = _build_safe_refusal_answer(chat_request.message, detected_lang, suspicious_entity)
                    
                    # CRITICAL: If None, it's a well-known historical fact - continue with normal flow (use base knowledge)
                    if response is None:
                        logger.info("âœ… Well-known historical fact detected - continuing with normal flow to use base knowledge")
                        processing_steps.append("âœ… Well-known historical fact - using base knowledge with transparency")
                        # Continue with normal flow (will use base knowledge instruction)
                    else:
                        logger.warning(
                            f"ğŸ›¡ï¸ Hallucination Guard triggered (non-RAG): "
                            f"factual_question=True, confidence={confidence_score:.2f}, "
                            f"suspicious_patterns={has_suspicious_pattern}, "
                            f"entity={suspicious_entity or 'unknown'}"
                        )
                        processing_steps.append("ğŸ›¡ï¸ Hallucination Guard: Overrode response with safe refusal")
                        # Mark as fallback to skip post-processing
                        is_fallback_meta_answer_non_rag = True
                        is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
            
            # CRITICAL: Add transparency warning for low confidence responses without context (non-RAG path)
            if confidence_score < 0.5 and not is_fallback_meta_answer_non_rag and not is_philosophical_non_rag and response:
                response_lower = response.lower()
                has_transparency = any(
                    phrase in response_lower for phrase in [
                        "khÃ´ng cÃ³ dá»¯ liá»‡u", "khÃ´ng cÃ³ thÃ´ng tin", "kiáº¿n thá»©c chung", "dá»±a trÃªn kiáº¿n thá»©c",
                        "don't have data", "don't have information", "general knowledge", "based on knowledge",
                        "khÃ´ng tá»« stillme", "not from stillme", "khÃ´ng tá»« rag", "not from rag"
                    ]
                )
                if not has_transparency:
                    # Generate multilingual transparency disclaimer
                    disclaimer = _get_transparency_disclaimer(detected_lang)
                    response = disclaimer + response
                    logger.info("â„¹ï¸ Added transparency disclaimer for low confidence response without context (non-RAG path)")
        
        # Score the response
        accuracy_score = None
        if accuracy_scorer:
            accuracy_score = accuracy_scorer.score_response(
                question=chat_request.message,
                actual_answer=response,
                scoring_method="semantic_similarity"
            )
        
        # Record learning session
        learning_session_id = None
        if knowledge_retention:
            learning_session_id = knowledge_retention.record_learning_session(
                session_type="chat",
                content_learned=f"Q: {chat_request.message}\nA: {response}",
                accuracy_score=accuracy_score or 0.5,
                metadata={"user_id": chat_request.user_id}
            )
        
        # Align tone if enabled
        if enable_tone_align:
            try:
                tone_start = time.time()
                from backend.tone.aligner import align_tone
                response = align_tone(response)
                tone_time = time.time() - tone_start
                timing_logs["tone_alignment"] = f"{tone_time:.2f}s"
                logger.info(f"â±ï¸ Tone alignment took {tone_time:.2f}s")
            except Exception as tone_error:
                logger.error(f"Tone alignment error: {tone_error}, using original response")
                # Continue with original response on error
        
        # ==========================================
        # PHASE 3: POST-PROCESSING PIPELINE (Non-RAG path)
        # Unified Style & Quality Enforcement Layer (Optimized)
        # ==========================================
        # CRITICAL: If response is a fallback meta-answer, skip all post-processing
        if is_fallback_meta_answer_non_rag:
            logger.info("ğŸ›‘ Skipping post-processing for fallback meta-answer (non-RAG)")
            # response already set, skip post-processing entirely
        else:
            # Check if question is philosophical for non-RAG path
            is_philosophical_non_rag = False
            try:
                from backend.core.question_classifier import is_philosophical_question
                is_philosophical_non_rag = is_philosophical_question(chat_request.message)
            except Exception:
                pass  # If classifier fails, assume non-philosophical
            
            postprocessing_start = time.time()
            # Initialize quality_result to prevent UnboundLocalError when fallback is detected
            quality_result = None
            final_response = None
            
            try:
                from backend.postprocessing.style_sanitizer import get_style_sanitizer
                from backend.postprocessing.quality_evaluator import get_quality_evaluator, QualityLevel
                from backend.postprocessing.rewrite_llm import get_rewrite_llm
                from backend.postprocessing.optimizer import get_postprocessing_optimizer
                
                optimizer = get_postprocessing_optimizer()
                
                # OPTIMIZATION: Check if we should skip post-processing
                should_skip, skip_reason = optimizer.should_skip_postprocessing(
                    question=chat_request.message,
                    response=response,
                    is_philosophical=is_philosophical_non_rag
                )
                
                if should_skip:
                    logger.info(f"â­ï¸ Skipping post-processing (non-RAG): {skip_reason}")
                    timing_logs["postprocessing"] = "skipped"
                else:
                    # Stage 2: Hard Filter (0 token) - Style Sanitization
                    sanitizer = get_style_sanitizer()
                    sanitized_response = sanitizer.sanitize(response, is_philosophical=is_philosophical_non_rag)
                    
                    # CRITICAL: Check if sanitized response is a technical error or fallback message BEFORE quality evaluation
                    from backend.api.utils.error_detector import is_technical_error, is_fallback_message
                    is_error, error_type = is_technical_error(sanitized_response)
                    is_fallback = is_fallback_message(sanitized_response)
                    
                    if is_error or is_fallback:
                        # Technical error or fallback message detected - skip quality evaluation and rewrite
                        if is_error:
                            logger.warning(
                                f"âš ï¸ Technical error detected in sanitized response (non-RAG, type: {error_type}), "
                                f"skipping quality evaluation and rewrite"
                            )
                            processing_steps.append(f"âš ï¸ Technical error detected - skipping post-processing")
                        else:
                            logger.info(
                                f"ğŸ›‘ Fallback meta-answer detected in sanitized response (non-RAG), "
                                f"skipping quality evaluation and rewrite"
                            )
                            processing_steps.append(f"ğŸ›‘ Fallback message detected - skipping post-processing")
                        final_response = sanitized_response
                        # Skip all remaining post-processing (quality evaluation, rewrite)
                        # quality_result remains None, which is fine - we won't use it
                    else:
                        # Stage 3: Quality Evaluator (0 token) - Rule-based Quality Check
                        # OPTIMIZATION: Check cache first
                        evaluator = get_quality_evaluator()
                        cached_quality = optimizer.get_cached_quality_result(
                            question=chat_request.message,
                            response=sanitized_response
                        )
                        
                        if cached_quality:
                            quality_result = cached_quality
                            logger.debug("âœ… Using cached quality evaluation (non-RAG)")
                        else:
                            quality_result = evaluator.evaluate(
                                text=sanitized_response,
                                is_philosophical=is_philosophical_non_rag,
                                original_question=chat_request.message
                            )
                        # Cache the result
                        optimizer.cache_quality_result(
                            question=chat_request.message,
                            response=sanitized_response,
                            quality_result=quality_result
                        )
                        
                        # Phase 3: Only rewrite when CRITICAL issues are present
                        # Critical issues: missing citation, anthropomorphic language, language mismatch, topic drift, template-like
                        # Non-critical issues (depth, unpacking, style) do NOT trigger rewrite
                        should_rewrite, rewrite_reason = optimizer.should_rewrite(
                            quality_result=quality_result,
                            is_philosophical=is_philosophical_non_rag,
                            response_length=len(sanitized_response)
                        )
                        
                        # Stage 4: ALWAYS rewrite (100% policy) - Má»¥c tiÃªu: minh báº¡ch, trung thá»±c, giáº£m áº£o giÃ¡c
                        if should_rewrite:
                            logger.info(
                                f"âš ï¸ Quality evaluator flagged output for rewrite (non-RAG). "
                                f"Issues: {quality_result['reasons']}, "
                                f"score: {quality_result.get('overall_score', 'N/A')}, "
                                f"length: {len(sanitized_response)}"
                            )
                            processing_steps.append(f"ğŸ”„ Quality improvement needed - rewriting with DeepSeek")
                            
                            rewrite_llm = get_rewrite_llm()
                            # Non-RAG path: no ctx_docs available, pass empty list
                            # CRITICAL: Check if this is AI_SELF_MODEL domain
                            from backend.style.style_engine import detect_domain, DomainType
                            detected_domain = detect_domain(chat_request.message)
                            is_ai_self_model_domain = (detected_domain == DomainType.AI_SELF_MODEL)
                            
                            rewrite_result = await rewrite_llm.rewrite(
                                text=sanitized_response,
                                original_question=chat_request.message,
                                quality_issues=quality_result["reasons"],
                                is_philosophical=is_philosophical_non_rag,
                                detected_lang=detected_lang,
                                ctx_docs=[],  # Non-RAG path has no context documents
                                is_ai_self_model=is_ai_self_model_domain
                            )
                            
                            if rewrite_result.was_rewritten:
                                # Re-sanitize rewritten output (in case rewrite introduced issues)
                                final_response = sanitizer.sanitize(rewrite_result.text, is_philosophical=is_philosophical_non_rag)
                                logger.debug(f"âœ… Post-processing complete (non-RAG): sanitized â†’ evaluated â†’ rewritten â†’ re-sanitized")
                            else:
                                # Fallback to sanitized original - rewrite failed
                                final_response = sanitized_response
                                error_detail = rewrite_result.error or "Unknown error"
                                logger.info(
                                    f"â„¹ï¸ DeepSeek rewrite skipped (non-RAG, error: {error_detail[:100]}), "
                                    f"using original sanitized response (this is normal if API is unavailable or timeout)"
                                )
                                processing_steps.append(f"â„¹ï¸ Rewrite skipped, using original (sanitized)")
                        else:
                            final_response = sanitized_response
                            if should_rewrite and quality_result:
                                logger.debug(f"â­ï¸ Skipping rewrite (non-RAG): {rewrite_reason}")
                            if quality_result:
                                logger.debug(f"âœ… Post-processing complete (non-RAG): sanitized â†’ evaluated â†’ passed (quality: {quality_result['depth_score']})")
                            else:
                                logger.debug(f"âœ… Post-processing complete (non-RAG): sanitized â†’ passed (no quality evaluation)")
                
                response = final_response
                
                # CRITICAL: Final check - ensure response is not a technical error
                if response:
                    is_error, error_type = is_technical_error(response)
                    if is_error:
                        logger.error(f"âš ï¸ Final response (non-RAG) is still a technical error (type: {error_type}) - replacing with fallback")
                        response = get_fallback_message_for_error(error_type, detected_lang)
                
                postprocessing_time = time.time() - postprocessing_start
                timing_logs["postprocessing"] = f"{postprocessing_time:.3f}s"
                logger.info(f"â±ï¸ Post-processing (non-RAG) took {postprocessing_time:.3f}s")
            except Exception as postprocessing_error:
                logger.error(f"Post-processing error (non-RAG): {postprocessing_error}", exc_info=True)
                # Fallback to original response if post-processing fails
                # Don't break the pipeline - post-processing is enhancement, not critical
                logger.warning(f"âš ï¸ Post-processing failed (non-RAG), using original response")
                timing_logs["postprocessing"] = "failed"
                # CRITICAL: Ensure final_response is set to original response if post-processing fails
                if final_response is None:
                    final_response = response
        
        # CRITICAL: Ensure response is set from final_response, or keep original if final_response is None
        logger.info(f"ğŸ” [TRACE] Before final_response assignment: response={response[:200] if response else 'None'}, final_response={final_response[:200] if final_response else 'None'}, response_type={type(response)}, final_response_type={type(final_response)}")
        if final_response is not None:
            response = final_response
            logger.info(f"ğŸ” [TRACE] After final_response assignment: response={response[:200] if response else 'None'}, response_type={type(response)}")
        else:
            logger.warning(f"âš ï¸ [TRACE] final_response is None, keeping original response: response={response[:200] if response else 'None'}, response_type={type(response)}")
            # CRITICAL: If both response and final_response are None, we have a problem
            if response is None:
                logger.error(f"âŒ [TRACE] CRITICAL: Both response and final_response are None! This should never happen. Using fallback.")
                from backend.api.utils.error_detector import get_fallback_message_for_error
                response = get_fallback_message_for_error("generic", detected_lang or "vi")
                processing_steps.append("âš ï¸ CRITICAL: Both response and final_response were None - using fallback")
        
        # Calculate total response latency
        # Total_Response_Latency: Time from request received to response returned
        total_response_end = time.time()
        total_response_latency = total_response_end - start_time
        
        # Format latency metrics log as specified by user
        # Báº®T BUá»˜C HIá»‚N THá»Š LOG: In ra ngay láº­p tá»©c sau cÃ¢u tráº£ lá»i
        latency_metrics_text = f"""--- LATENCY METRICS ---
RAG_Retrieval_Latency: {rag_retrieval_latency:.2f} giÃ¢y
LLM_Inference_Latency: {llm_inference_latency:.2f} giÃ¢y
Total_Response_Latency: {total_response_latency:.2f} giÃ¢y
-----------------------"""
        
        logger.info(latency_metrics_text)
        
        # Add latency metrics to timing_logs for API response
        timing_logs["rag_retrieval_latency"] = f"{rag_retrieval_latency:.2f}s"
        timing_logs["llm_inference_latency"] = f"{llm_inference_latency:.2f}s"
        timing_logs["total_response_latency"] = f"{total_response_latency:.2f}s"
        timing_logs["total"] = f"{total_response_latency:.2f}s"
        # Add formatted latency metrics text for frontend display
        timing_logs["latency_metrics_formatted"] = latency_metrics_text
        logger.info(f"ğŸ“Š Timing breakdown: {timing_logs}")
        
        # Analyze conversation for learning opportunities
        learning_proposal = None
        permission_request = None
        proposal_id = None
        
        # CRITICAL: Detect if base knowledge was used - this indicates a knowledge gap
        used_base_knowledge = False
        if response:
            response_lower = response.lower()
            base_knowledge_indicators = [
                "based on general knowledge", "not from stillme", "not from rag",
                "kiáº¿n thá»©c chung", "khÃ´ng tá»« stillme", "khÃ´ng tá»« rag",
                "training data", "dá»¯ liá»‡u huáº¥n luyá»‡n", "base knowledge",
                "general knowledge", "kiáº¿n thá»©c ná»n táº£ng"
            ]
            used_base_knowledge = any(indicator in response_lower for indicator in base_knowledge_indicators)
        
        if rag_retrieval and chat_request.use_rag:
            try:
                from backend.services.conversation_learning_extractor import get_conversation_learning_extractor
                from backend.services.learning_proposal_storage import get_learning_proposal_storage
                
                extractor = get_conversation_learning_extractor()
                storage = get_learning_proposal_storage()
                
                # PRIORITY: If base knowledge was used, extract topic for learning proposal
                # Check if: (1) No context, OR (2) Context exists but not relevant (low overlap)
                # Fix: Ensure validation_result is always defined before use
                has_no_context = not context or context.get("total_context_docs", 0) == 0
                has_low_relevance = False
                
                # CRITICAL: Extract validation_result from validation_info if available
                validation_result = None
                if validation_info and isinstance(validation_info, dict):
                    validation_result = validation_info.get("validation_result")
                elif hasattr(validation_info, 'validation_result'):
                    validation_result = validation_info.validation_result
                
                if validation_result and hasattr(validation_result, 'reasons'):
                    has_low_relevance = any("citation_relevance_warning" in r for r in validation_result.reasons)
                
                # CRITICAL: Only analyze for learning if response is NOT a fallback meta-answer
                if not is_fallback_for_learning and used_base_knowledge and (has_no_context or has_low_relevance):
                    # No RAG context OR irrelevant context + base knowledge used = knowledge gap detected
                    # Extract topic from user's question for learning proposal
                    logger.info(f"ğŸ” Base knowledge used - detecting knowledge gap for learning proposal (no_context: {has_no_context}, low_relevance: {has_low_relevance})")
                    try:
                        # Extract main topic from user question
                        user_question = chat_request.message
                        # Simple topic extraction: use first 200 chars or key phrases
                        topic_snippet = user_question[:200] if len(user_question) > 200 else user_question
                        
                        # Create learning proposal for this knowledge gap
                        base_knowledge_proposal = {
                            "knowledge_snippet": f"Topic: {topic_snippet}\n\nStillMe used base LLM knowledge to answer this question, indicating a knowledge gap in RAG. This topic should be prioritized for learning from trusted sources (arXiv, Wikipedia, RSS feeds).",
                            "source": "knowledge_gap_detection",
                            "knowledge_score": 0.7,  # High priority - user asked about it
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            "reason": f"StillMe used base knowledge to answer about '{topic_snippet[:50]}...'. This indicates RAG knowledge gap - should learn this topic from trusted sources.",
                            "is_knowledge_gap": True,
                            "user_question": user_question,
                            "detected_from": "base_knowledge_usage"
                        }
                        
                        # Store learning proposal
                        proposal_id = storage.save_proposal(
                            proposal=base_knowledge_proposal,
                            user_id=chat_request.user_id
                        )
                        base_knowledge_proposal["proposal_id"] = proposal_id
                        learning_proposal = base_knowledge_proposal
                        logger.info(f"âœ… Knowledge gap learning proposal created (id: {proposal_id}, topic: {topic_snippet[:50]}...)")
                    except Exception as gap_error:
                        logger.warning(f"Failed to create knowledge gap proposal: {gap_error}")
                
                # Also check for valuable knowledge from user/assistant (existing logic)
                # CRITICAL: Only analyze if response is NOT a fallback meta-answer
                if not is_fallback_for_learning and not learning_proposal:  # Only if we didn't already create a gap proposal
                    learning_proposal = extractor.analyze_conversation_for_learning(
                        user_message=chat_request.message,
                        assistant_response=response,
                        context=context
                    )
                
                if learning_proposal:
                    # If we didn't create proposal above, store it now
                    if "proposal_id" not in learning_proposal:
                        proposal_id = storage.save_proposal(
                            proposal=learning_proposal,
                            user_id=chat_request.user_id
                        )
                        learning_proposal["proposal_id"] = proposal_id
                    
                    # Format permission request
                    permission_request = extractor.format_permission_request(
                        learning_proposal=learning_proposal,
                        language=detected_lang
                    )
                    logger.info(f"Learning proposal generated (id: {proposal_id}, score: {learning_proposal.get('knowledge_score', 0):.2f})")
            except Exception as extractor_error:
                logger.warning(f"Conversation learning extractor error: {extractor_error}")
        
        # Store conversation in vector DB
        if rag_retrieval:
            rag_retrieval.add_learning_content(
                content=f"Q: {chat_request.message}\nA: {response}",
                source="user_chat",
                content_type="conversation",
                metadata={
                    "user_id": chat_request.user_id,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "accuracy_score": accuracy_score
                }
            )
        
        # Knowledge Alert: Retrieve important knowledge related to query
        knowledge_alert = None
        if rag_retrieval:
            try:
                important_knowledge = rag_retrieval.retrieve_important_knowledge(
                    query=chat_request.message,
                    limit=1,
                    min_importance=0.7
                )
                
                if important_knowledge:
                    doc = important_knowledge[0]
                    metadata = doc.get("metadata", {})
                    knowledge_alert = {
                        "title": metadata.get("title", "Important Knowledge"),
                        "source": metadata.get("source", "Unknown"),
                        "importance_score": metadata.get("importance_score", 0.7),
                        "content_preview": doc.get("content", "")[:200] + "..." if len(doc.get("content", "")) > 200 else doc.get("content", "")
                    }
                    logger.info(f"Knowledge alert generated: {knowledge_alert.get('title', 'Unknown')}")
            except Exception as alert_error:
                logger.warning(f"Knowledge alert error: {alert_error}")
        
        # Generate learning suggestions from knowledge gaps if context is empty or low confidence
        learning_suggestions = None
        if (confidence_score is not None and confidence_score < 0.5) or (context and context.get("total_context_docs", 0) == 0):
            try:
                self_diagnosis = get_self_diagnosis()
                if self_diagnosis:
                    gap_result = self_diagnosis.identify_knowledge_gaps(chat_request.message, threshold=0.5)
                    if gap_result.get("has_gap"):
                        suggestion = gap_result.get("suggestion")
                        if suggestion:
                            learning_suggestions = [suggestion]
                        else:
                            # Extract key terms from query for learning suggestions
                            words = re.findall(r'\b\w+\b', chat_request.message.lower())
                            # Filter out common words
                            common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'what', 'how', 'why', 'when', 'where', 'who', 'which', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'to', 'of', 'in', 'on', 'at', 'for', 'with', 'by', 'from', 'as', 'about', 'into', 'through', 'during', 'including', 'against', 'among', 'throughout', 'despite', 'towards', 'upon', 'concerning', 'to', 'of', 'in', 'on', 'at', 'for', 'with', 'by', 'from', 'as', 'about', 'into', 'through', 'during', 'including', 'against', 'among', 'throughout', 'despite', 'towards', 'upon', 'concerning'}
                            key_terms = [w for w in words if len(w) > 3 and w not in common_words][:3]
                            if key_terms:
                                learning_suggestions = [f"Consider learning more about: {', '.join(key_terms)}"]
            except Exception as suggestion_error:
                logger.warning(f"Failed to generate learning suggestions: {suggestion_error}")
        
        # Generate unique message ID for feedback tracking
        import uuid
        message_id = f"msg_{uuid.uuid4().hex[:16]}"
        
        # If style learning response exists, prepend it to the response
        if style_learning_response:
            response = f"{style_learning_response}\n\n---\n\n{response}"
        
        # CRITICAL: Final safety check - ensure response is never None or empty before returning
        logger.info(f"ğŸ” [TRACE] Final check before return: response={response[:200] if response else 'None'}, response_type={type(response)}, response_length={len(response) if response else 0}")
        if not response or not isinstance(response, str) or not response.strip():
            logger.error(f"âš ï¸ Response is None, empty, or invalid before returning ChatResponse - using fallback. response={response}, type={type(response)}, detected_lang={detected_lang}")
            logger.error(f"âš ï¸ Debug info: raw_response={raw_response[:200] if raw_response else 'None'}, final_response={final_response[:200] if final_response else 'None'}")
            logger.error(f"âš ï¸ Processing steps: {processing_steps[-5:] if len(processing_steps) > 5 else processing_steps}")
            from backend.api.utils.error_detector import get_fallback_message_for_error
            response = get_fallback_message_for_error("generic", detected_lang or "vi")
            processing_steps.append("âš ï¸ Response validation failed - using fallback message")
        
        return ChatResponse(
            response=response,
            message_id=message_id,
            context_used=context,
            accuracy_score=accuracy_score,
            confidence_score=confidence_score,
            validation_info=validation_info,
            learning_suggestions=learning_suggestions,
            learning_session_id=learning_session_id,
            knowledge_alert=knowledge_alert,
            learning_proposal=learning_proposal,  # Learning proposal (if valuable knowledge detected)
            permission_request=permission_request,  # Permission request message
            timing=timing_logs,
            latency_metrics=latency_metrics_text,  # Báº®T BUá»˜C HIá»‚N THá»Š LOG trong response cho frontend
            processing_steps=processing_steps  # Real-time processing steps for status indicator
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (they have proper status codes)
        raise
    except RateLimitExceeded:
        # Re-raise rate limit exceptions so global handler can return proper 429 status
        raise
    except Exception as e:
        # Log detailed error with context (without sensitive message content)
        logger.error(f"Chat error: {e}", exc_info=True)
        # Security: Don't log full message content (may contain sensitive info)
        # Only log message length and metadata
        logger.error(
            f"Request details: message_length={len(chat_request.message)}, "
            f"user_id={chat_request.user_id}, use_rag={chat_request.use_rag}"
        )
        
        # Check if it's a specific error we can handle
        error_str = str(e).lower()
        if "rag" in error_str and "not available" in error_str:
            raise HTTPException(status_code=503, detail="RAG system is not available. Please check backend initialization.")
        elif "embedding" in error_str or "model" in error_str:
            raise HTTPException(status_code=503, detail="Embedding service is not available. Please check backend logs.")
        else:
            raise HTTPException(
                status_code=500,
                detail=f"Chat error: {str(e)}. Please check backend logs for details."
            )


@router.post("/smart_router", response_model=ChatResponse)
async def chat_smart_router(request: Request, chat_request: ChatRequest):
    """
    Smart router that automatically selects the best chat endpoint.
    This is the main endpoint used by the dashboard.
    """
    try:
        # Use the RAG-enhanced chat endpoint as default
        return await chat_with_rag(request, chat_request)
    except HTTPException:
        # Re-raise HTTP exceptions (they have proper status codes)
        raise
    except RateLimitExceeded:
        # Re-raise rate limit exceptions so global handler can return proper 429 status
        raise
    except Exception as e:
        # Log detailed error for debugging
        logger.error(f"Smart router error: {e}", exc_info=True)
        # Return a more helpful error message
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}. Please check backend logs for details."
        )


# Legacy endpoints for backward compatibility
@router.post("/openai")
async def chat_openai(request: ChatRequest):
    """Legacy OpenAI chat endpoint"""
    # Create a dummy Request object for chat_with_rag
    # Note: This is a workaround - in production, we should refactor to not require Request
    class DummyRequest:
        pass
    dummy_request = DummyRequest()
    return await chat_with_rag(dummy_request, request)


@router.post("/deepseek")
async def chat_deepseek(request: ChatRequest):
    """Legacy DeepSeek chat endpoint"""
    # Create a dummy Request object for chat_with_rag
    class DummyRequest:
        pass
    dummy_request = DummyRequest()
    return await chat_with_rag(dummy_request, request)


@router.post("/ask", response_model=ChatResponse)
@limiter.limit(get_chat_rate_limit(), key_func=get_rate_limit_key_func)  # Chat rate limit (configurable for local testing)
async def ask_question(request: Request, chat_request: ChatRequest):
    """
    Simplified question-answering endpoint.
    Alias for /rag endpoint with RAG enabled by default.
    
    This endpoint is designed for simple Q&A use cases where RAG context
    is always desired. It's a convenience wrapper around the full RAG chat endpoint.
    """
    # Ensure RAG is enabled for /ask endpoint
    chat_request.use_rag = True
    # Use default context limit if not specified
    if chat_request.context_limit is None or chat_request.context_limit < 1:
        chat_request.context_limit = 3
    
    # Delegate to the main RAG chat endpoint
    return await chat_with_rag(request, chat_request)


@router.post("/validate")
@limiter.limit("20/minute", key_func=get_rate_limit_key_func)
async def validate_content(request: Request, chat_request: ChatRequest):
    """
    Standalone content validation endpoint.
    
    Validates user input/question for:
    - Ethical compliance
    - Content safety
    - Format validation
    
    Returns validation result without generating a response.
    This is useful for pre-validation before processing expensive RAG/LLM calls.
    """
    from backend.validators.chain import ValidatorChain
    from backend.validators.citation import CitationRequired
    from backend.validators.evidence_overlap import EvidenceOverlap
    from backend.validators.numeric import NumericUnitsBasic
    from backend.validators.ethics_adapter import EthicsAdapter
    
    try:
        # Get RAG retrieval for context (if needed for validation)
        rag_retrieval = get_rag_retrieval()
        
        # Get context if RAG is enabled
        context_docs = []
        if rag_retrieval and chat_request.use_rag:
            try:
                context = rag_retrieval.retrieve_context(
                    query=chat_request.message,
                    knowledge_limit=min(chat_request.context_limit, 3),  # Limit for validation
                    conversation_limit=0  # Don't need conversation for validation
                )
                context_docs = [
                    doc["content"] for doc in context.get("knowledge_docs", [])
                ]
            except Exception as context_error:
                logger.warning(f"Could not retrieve context for validation: {context_error}")
                context_docs = []
        
        # Create validator chain
        enable_validators = os.getenv("ENABLE_VALIDATORS", "false").lower() == "true"
        
        if enable_validators:
            from backend.services.ethics_guard import check_content_ethics
            chain = ValidatorChain([
                CitationRequired(),  # Not applicable for input, but included for completeness
                EvidenceOverlap(threshold=0.01),
                NumericUnitsBasic(),
                EthicsAdapter(guard_callable=check_content_ethics)  # Real ethics guard implementation
            ])
            
            # Validate the message itself (treating it as "answer" to check)
            # Note: This validates the user input, not a response
            validation_result = chain.run(chat_request.message, context_docs)
            
            # Record metrics
            try:
                from backend.validators.metrics import get_metrics
                metrics = get_metrics()
                metrics.record_validation(
                    passed=validation_result.passed,
                    reasons=validation_result.reasons,
                    overlap_score=0.0  # Not applicable for input validation
                )
            except Exception as metrics_error:
                logger.warning(f"Could not record validation metrics: {metrics_error}")
            
            return {
                "is_valid": validation_result.passed,
                "message": chat_request.message,
                "validation_details": {
                    "passed": validation_result.passed,
                    "reasons": validation_result.reasons,
                    "patched_content": validation_result.patched if hasattr(validation_result, 'patched') else None
                },
                "context_used": {
                    "context_docs_count": len(context_docs),
                    "rag_enabled": chat_request.use_rag
                }
            }
        else:
            # If validators are disabled, do basic validation only
            return {
                "is_valid": True,
                "message": chat_request.message,
                "validation_details": {
                    "passed": True,
                    "reasons": ["Validators disabled - basic format check passed"],
                    "note": "Full validation disabled via ENABLE_VALIDATORS=false"
                },
                "context_used": {
                    "context_docs_count": len(context_docs),
                    "rag_enabled": chat_request.use_rag
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in /validate endpoint: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Validation error: {str(e)}. Please check backend logs for details."
        )

