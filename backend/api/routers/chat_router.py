"""
Chat Router for StillMe API
Handles all chat-related endpoints
"""

from fastapi import APIRouter, Request, HTTPException
from backend.api.models import ChatRequest, ChatResponse
from backend.api.rate_limiter import limiter, get_rate_limit_key_func, RateLimitExceeded, get_chat_rate_limit
from backend.api.utils.chat_helpers import (
    generate_ai_response,
    detect_language
)
from backend.identity.prompt_builder import (
    UnifiedPromptBuilder,
    PromptContext,
    FPSResult
)
from backend.core.manifest_loader import (
    get_validator_count,
    get_validator_summary,
    get_layers_info,
    get_manifest_text_for_prompt
)
from backend.philosophy.processor import (
    is_philosophical_question_about_consciousness,
    process_philosophical_question
)
from backend.style.style_engine import detect_domain, DomainType
from backend.services.cache_service import (
    get_cache_service,
    CACHE_PREFIX_LLM,
    TTL_LLM_RESPONSE
)
import logging
import os
import re
import time
import asyncio
import uuid
from datetime import datetime, timezone
from typing import Optional, List, Dict, Any, Tuple
import json
import unicodedata

logger = logging.getLogger(__name__)

router = APIRouter()


def _get_validator_info_for_prompt() -> tuple[str, str, str]:
    """
    Get validator information from manifest for use in prompts.
    
    Returns:
        Tuple of (summary_vi, summary_en, layers_count_str)
        Falls back to defaults if manifest not available
    """
    try:
        total_validators, num_layers = get_validator_count()
        if total_validators > 0 and num_layers > 0:
            summary_vi = f"{total_validators} validators total, chia th√†nh {num_layers} l·ªõp (layers)"
            summary_en = f"{total_validators} validators total, organized into {num_layers} layers"
            layers_count_str = f"{num_layers} layers"
            return (summary_vi, summary_en, layers_count_str)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error getting validator info from manifest: {e}")
    
    # Fallback to defaults (should not happen if manifest is properly generated)
    return ("19 validators total, chia th√†nh 7 l·ªõp (layers)", 
            "19 validators total, organized into 7 layers",
            "7 layers")


def _safe_unicode_slice(text: str, max_length: int) -> str:
    """
    Safely slice Unicode string without breaking multi-byte characters.
    
    Args:
        text: Input string (may contain Unicode characters)
        max_length: Maximum length in characters (not bytes)
        
    Returns:
        Sliced string that preserves Unicode characters
    """
    if not text or not isinstance(text, str):
        return text
    
    # Normalize Unicode to ensure consistent character boundaries
    try:
        text = unicodedata.normalize('NFC', text)
    except Exception:
        pass  # If normalization fails, continue with original text
    
    # Safe slice: Python's string slicing already handles Unicode correctly
    # But we add defensive check to ensure we don't exceed string length
    if len(text) <= max_length:
        return text
    
    return text[:max_length]


def _clean_response_text(text: str) -> str:
    """
    Clean response text from control characters and smart quotes that may cause encoding issues.
    
    CRITICAL: This function must preserve all Unicode characters (Chinese, Vietnamese, etc.).
    It should ONLY remove:
    - Control characters (0x00-0x1F, 0x7F-0x9F)
    - Smart quotes (\u201c, \u201d, \u2018, \u2019)
    - Zero-width characters that cause issues
    
    It should NOT remove:
    - Any Unicode characters (Chinese, Vietnamese, etc.)
    - Any printable characters
    
    Args:
        text: Original response text (may contain Unicode characters)
        
    Returns:
        Cleaned text with problematic characters removed (Unicode preserved)
    """
    if not text or not isinstance(text, str):
        return text
    
    # CRITICAL: Log input for debugging (especially for Chinese)
    input_length = len(text)
    input_preview = _safe_unicode_slice(text, 100) if text else 'None'
    
    # CRITICAL: Only remove control characters and smart quotes
    # DO NOT remove any Unicode characters (Chinese, Vietnamese, etc.)
    # CRITICAL: PRESERVE newlines (\n = 0x0A) and carriage returns (\r = 0x0D) for line breaks
    # Pattern explanation:
    # - [\x00-\x09\x0b-\x1f]: Control characters EXCEPT \n (0x0A) and \r (0x0D)
    # - [\x7f-\x9f]: Extended control characters (0x7F-0x9F)
    # - \u201c\u201d\u2018\u2019: Smart quotes (left/right double and single quotes)
    # - \u200b\u200c\u200d\u200e\u200f: Zero-width characters that can cause issues
    # - \ufffe\uffff: Non-characters
    # CRITICAL: This pattern does NOT match Chinese/Vietnamese/any Unicode characters
    # CRITICAL: This pattern PRESERVES \n (0x0A) and \r (0x0D) for line breaks
    cleaned = re.sub(r'[\x00-\x09\x0b-\x1f\x7f-\x9f\u201c\u201d\u2018\u2019\u200b\u200c\u200d\u200e\u200f\ufffe\uffff]', '', text)
    
    # CRITICAL: Validate that we didn't lose significant content
    output_length = len(cleaned) if cleaned else 0
    removed_chars = input_length - output_length
    
    # CRITICAL: If we removed more than 5% of content (excluding control characters), something is wrong
    # Control characters should be very rare (< 1% of text)
    if removed_chars > (input_length * 0.05) and removed_chars > 10:
        logger.error(
            f"‚ùå CRITICAL: _clean_response_text removed {removed_chars} characters "
            f"({removed_chars/input_length*100:.1f}% of input, length={input_length}), "
            f"this is suspicious - returning original text to prevent content loss"
        )
        return text  # Return original if we removed too much
    
    # Normalize Unicode to NFC form for consistency
    # CRITICAL: This should NOT change the length significantly
    try:
        cleaned_normalized = unicodedata.normalize('NFC', cleaned)
        # CRITICAL: Validate normalization didn't lose content
        if len(cleaned_normalized) < len(cleaned) * 0.95:
            logger.warning(
                f"‚ö†Ô∏è Unicode normalization lost content "
                f"(before: {len(cleaned)}, after: {len(cleaned_normalized)}), "
                f"using pre-normalized text"
            )
            cleaned = cleaned  # Keep pre-normalized version
        else:
            cleaned = cleaned_normalized
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Unicode normalization failed: {e}, using cleaned text without normalization")
        pass  # If normalization fails, continue with cleaned text
    
    # CRITICAL: Log output for debugging (especially for Chinese)
    output_length_final = len(cleaned) if cleaned else 0
    output_preview = _safe_unicode_slice(cleaned, 100) if cleaned else 'None'
    removed_chars_final = input_length - output_length_final
    
    if removed_chars_final > 0:
        logger.info(
            f"_clean_response_text: removed {removed_chars_final} problematic characters "
            f"(input_length={input_length}, output_length={output_length_final})"
        )
    
    # CRITICAL: Final validation - ensure output is not empty
    if not cleaned or len(cleaned) == 0:
        logger.error(
            f"‚ùå CRITICAL: _clean_response_text output is empty "
            f"(input_length={input_length}), returning original text"
        )
        return text  # Return original if output is empty
    
    return cleaned


def _fix_missing_line_breaks(text: str) -> str:
    """
    Auto-fix missing line breaks after headings and bullets.
    
    This is a defensive function to ensure line breaks are present even if LLM
    doesn't follow instructions.
    
    Args:
        text: Response text that may be missing line breaks
        
    Returns:
        Text with line breaks fixed
    """
    if not text or not isinstance(text, str):
        return text
    
    # Fix: Add line break after markdown headings (## HeadingText -> ## HeadingText\n\n)
    # Pattern: ## or ### followed by text, then immediately followed by non-newline character
    text = re.sub(r'(^#{1,6}\s+[^\n]+)([^\n])', r'\1\n\n\2', text, flags=re.MULTILINE)
    
    # Fix: Add line break after heading-like text (if no markdown, check for patterns)
    # Pattern: Text ending with ":" or "?" followed by text without newline
    text = re.sub(r'([^:\n?]+[:\?])([^\n\s])', r'\1\n\2', text)
    
    # Fix: Add line break after bullet points (- Item -> - Item\n)
    # Pattern: - or * at start of line, followed by text, then immediately followed by non-newline
    text = re.sub(r'(^[\s]*[-*‚Ä¢]\s+[^\n]+)([^\n])', r'\1\n\2', text, flags=re.MULTILINE)
    
    # Normalize multiple consecutive newlines to max 2
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text


def _log_rag_retrieval_decision(
    decision_logger,
    context: Dict[str, Any],
    query: str,
    reasoning: str,
    similarity_threshold: Optional[float] = None,
    prioritize_foundational: bool = False,
    exclude_types: Optional[List[str]] = None,
    alternatives_considered: Optional[List[str]] = None
):
    """
    Helper function to log RAG retrieval decisions
    
    Args:
        decision_logger: DecisionLogger instance
        context: Retrieved context dictionary
        query: Query used for retrieval
        reasoning: Why this retrieval approach was chosen
        similarity_threshold: Similarity threshold used
        prioritize_foundational: Whether foundational knowledge was prioritized
        exclude_types: Content types excluded
        alternatives_considered: Alternative retrieval strategies considered
    """
    from backend.core.decision_logger import AgentType, DecisionType
    
    total_docs = context.get("total_context_docs", 0)
    knowledge_docs = context.get("knowledge_docs", [])
    
    # Extract document sources/types for context
    doc_sources = []
    for doc in knowledge_docs[:5]:  # Limit to 5 for logging
        if isinstance(doc, dict):
            metadata = doc.get("metadata", {})
            source = metadata.get("source", "unknown")
            doc_type = metadata.get("type", "unknown")
            doc_sources.append(f"{source}:{doc_type}")
    
    decision = f"Retrieved {total_docs} documents from ChromaDB"
    if prioritize_foundational:
        decision += " (prioritized foundational knowledge)"
    if similarity_threshold is not None:
        decision += f" (similarity threshold: {similarity_threshold})"
    
    context_data = {
        "total_docs": total_docs,
        "doc_sources": doc_sources[:5],  # Limit to 5
        "similarity_threshold": similarity_threshold,
        "prioritize_foundational": prioritize_foundational,
        "exclude_types": exclude_types
    }
    
    threshold_reasoning = None
    if similarity_threshold is not None:
        if similarity_threshold < 0.05:
            threshold_reasoning = f"Very low threshold ({similarity_threshold}) chosen to ensure StillMe foundational knowledge is retrieved even with low similarity scores"
        elif similarity_threshold < 0.1:
            threshold_reasoning = f"Low threshold ({similarity_threshold}) chosen for historical/factual questions to handle multilingual embedding mismatch"
        else:
            threshold_reasoning = f"Standard threshold ({similarity_threshold}) used for normal queries"
    
    decision_logger.log_decision(
        agent_type=AgentType.RAG_AGENT,
        decision_type=DecisionType.RETRIEVAL_DECISION,
        decision=decision,
        reasoning=reasoning,
        context=context_data,
        alternatives_considered=alternatives_considered,
        threshold_reasoning=threshold_reasoning,
        outcome=f"Successfully retrieved {total_docs} documents" if total_docs > 0 else "No documents retrieved",
        success=total_docs > 0
    )


def _build_system_status_context(system_monitor, current_learning_sources: Optional[Dict[str, Any]] = None) -> Tuple[Optional[dict], Optional[str]]:
    """
    Build a synthetic context document that contains real-time system status
    (RSS feeds, successes/failures) so the LLM cannot fall back to stale
    knowledge when answering system status or learning source questions.
    
    Returns:
        (context_dict, system_status_note)
    """
    try:
        # CRITICAL: Prioritize data from current_learning_sources API (more reliable)
        # Only use system_monitor.get_detailed_status() as fallback if API data not available
        total_feeds = 0
        successful_feeds = 0
        failed_feeds = 0
        failure_rate = 0.0
        last_error = None
        status_timestamp = datetime.now().isoformat()
        
        # First, try to get data from current_learning_sources API (most reliable)
        if current_learning_sources and isinstance(current_learning_sources, dict):
            sources = current_learning_sources.get("current_sources", {})
            rss_info = sources.get("rss", {})
            feeds_count = rss_info.get("feeds_count", 0)
            failed_info = rss_info.get("failed_feeds") or {}
            
            if feeds_count > 0:
                # Use API data as primary source
                total_feeds = feeds_count
                failed_feeds = failed_info.get("failed_count", 0)
                successful_feeds = failed_info.get("successful_count", 0)
                failure_rate = failed_info.get("failure_rate", 0.0)
                last_error = rss_info.get("last_error")
                logger.info(f"‚úÖ Using RSS stats from API: total={total_feeds}, failed={failed_feeds}, successful={successful_feeds}")
        
        # Fallback to system_monitor.get_detailed_status() only if API data not available
        if total_feeds == 0:
            detailed_status = system_monitor.get_detailed_status()
            rss_status = detailed_status.get("rss", {}) if isinstance(detailed_status, dict) else {}
            total_feeds = rss_status.get("total", 0)
            successful_feeds = rss_status.get("successful", 0)
            failed_feeds = rss_status.get("failed", 0)
            failure_rate = rss_status.get("failure_rate", 0.0)
            last_error = rss_status.get("last_error")
            status_timestamp = detailed_status.get("timestamp") if isinstance(detailed_status, dict) else status_timestamp
            if total_feeds > 0:
                logger.info(f"‚úÖ Using RSS stats from system_monitor: total={total_feeds}, failed={failed_feeds}, successful={successful_feeds}")

        status_note = "[System: Status unavailable]"
        if total_feeds > 0:
            status_note = f"[System: {total_feeds} RSS feeds ({failed_feeds} failed, {successful_feeds} ok)]"

        # Build context with SINGLE source of truth (no conflicting data)
        lines = [
            "Real-time system status (auto-learning pipeline):",
            f"- RSS feeds total: {total_feeds}",
            f"- Successful feeds: {successful_feeds}",
            f"- Failed feeds: {failed_feeds}",
            f"- Failure rate: {failure_rate}%",
        ]
        if last_error:
            lines.append(f"- Last RSS error: {last_error[:200]}")  # Truncate long errors
        
        # Add enabled sources info if available
        if current_learning_sources and isinstance(current_learning_sources, dict):
            sources = current_learning_sources.get("current_sources", {})
            enabled_sources = [name for name, info in sources.items() if info.get("enabled")]
            if enabled_sources:
                lines.append(f"- Enabled sources: {', '.join(enabled_sources)}")

        content = "\n".join(lines)
        context = {
            "knowledge_docs": [
                {
                    "content": content,
                    "metadata": {
                        "source": "system_status",
                        "type": "system_status",
                        "timestamp": status_timestamp
                    }
                }
            ],
            "conversation_docs": [],
            "total_context_docs": 1,
            "has_reliable_context": True,
            "context_quality": "high",
            "is_system_status_query": True
        }

        return context, status_note
    except Exception as e:
        logger.warning(f"Failed to build system status context: {e}")
        return None, None


def _is_codebase_meta_question(message: str) -> bool:
    """
    Detect meta-questions that explicitly ask about StillMe's implementation
    in its own codebase (files, functions, where things are implemented).

    Design intent:
    - VERY NARROW scope to avoid hijacking normal RAG or philosophy flows
    - Triggers only when BOTH:
      1) The question mentions StillMe / \"your system\" / \"your codebase\"
      2) The question references code-level concepts OR specific StillMe components
    
    Enhanced with StillMe-specific component keywords to catch queries like:
    - "How is validation chain implemented in your codebase?"
    - "Where is ai_self_model_detector in your source code?"
    - "Show me the ValidatorChain class from your code"
    """
    if not message:
        return False

    q = message.lower()

    # Self-reference: question is clearly about StillMe / its own implementation
    has_self_reference = any(
        term in q
        for term in [
            "stillme",
            "your system",
            "in your system",
            "your architecture",
            "your implementation",
            "your codebase",
            "in your codebase",
            "in your source code",
            "in your code",
            "from your code",
            "using your codebase",
            "using your own codebase",
        ]
    )

    if not has_self_reference:
        return False

    # Code-level intent: user is asking about concrete implementation details
    # OR specific StillMe components (enhanced with codebase-specific keywords)
    has_code_intent = any(
        term in q
        for term in [
            # Generic code concepts
            "codebase",
            "source code",
            "in the codebase",
            "in the code",
            "which file",
            "what file",
            "which function",
            "what function",
            "where is it implemented",
            "where is this implemented",
            "implemented in",
            "implementation details",
            "line number",
            "lines",
            "class",
            "function",
            "module",
            # StillMe-specific components (from actual codebase)
            "validator_chain",
            "validation chain",
            "validators",
            "ai_self_model_detector",
            "stillme_detector",
            "codebase_indexer",
            "codebase assistant",
            "rag retrieval",
            "chromadb",
            "epistemic_state",
            "epistemic reasoning",
            "citation_formatter",
            "prompt_builder",
            "chat_router",
            "codebase_router",
            "external_data",
            "philosophy processor",
            "honesty handler",
            "fallback_handler",
            # Technical architecture keywords
            "architecture",
            "component",
            "module",
            "service",
            "router",
            "endpoint",
        ]
    )

    return has_code_intent


def _add_timestamp_to_response(response: str, detected_lang: str = "en", context: Optional[dict] = None, user_question: Optional[str] = None, is_system_architecture_query: bool = False) -> str:
    """
    Add timestamp attribution to normal RAG responses for transparency.
    This ensures consistency with external data responses which already include timestamps.
    
    CRITICAL: This function must handle Unicode (including Chinese) safely.
    CRITICAL: For self-knowledge questions about StillMe's codebase, skip external citations.
    
    Args:
        response: Original response text (may contain Unicode characters)
        detected_lang: Detected language code
        context: Optional context dict with knowledge_docs for source links
        user_question: Optional user question to detect self-knowledge questions
        
    Returns:
        Response with timestamp attribution appended (duplicate citations removed)
    """
    if not response or not isinstance(response, str):
        return response
    
    # CRITICAL: Clean response from control characters and smart quotes BEFORE processing
    # This prevents encoding issues with Chinese/Unicode characters
    original_length = len(response)
    response = _clean_response_text(response)
    if len(response) != original_length:
        logger.debug(f"Cleaned response: removed {original_length - len(response)} problematic characters")
    
    # Check if response already has timestamp (to avoid duplicate)
    # Pattern: [Source: ... | Timestamp: ...] or [Ngu·ªìn: ... | Th·ªùi gian: ...] or standalone [Timestamp: ...]
    timestamp_patterns = [
        r'\[(?:Source:|Ngu·ªìn:).*\|.*Timestamp:',
        r'\[(?:Source:|Ngu·ªìn:).*\|.*Th·ªùi gian:',
        r'\[Timestamp:\s*[^\]]+\]',
        r'\[Th·ªùi gian:\s*[^\]]+\]',
    ]
    for pattern in timestamp_patterns:
        if re.search(pattern, response, re.IGNORECASE):
            logger.debug("Response already has timestamp, skipping addition")
            return response
    
    # Get current UTC timestamp
    now_utc = datetime.now(timezone.utc)
    timestamp_iso = now_utc.isoformat()
    
    # Convert UTC to local timezone for display (Asia/Ho_Chi_Minh)
    timestamp_display = timestamp_iso
    try:
        import pytz
        local_tz = pytz.timezone('Asia/Ho_Chi_Minh')
        local_time = now_utc.replace(tzinfo=timezone.utc).astimezone(local_tz)
        timestamp_str_local = local_time.strftime("%Y-%m-%d %H:%M:%S %Z%z")
        timestamp_str_utc = now_utc.strftime("%Y-%m-%d %H:%M:%S UTC")
        timestamp_display = f"{timestamp_str_local} ({timestamp_str_utc})"
    except ImportError:
        # pytz not available, use UTC only
        timestamp_display = now_utc.strftime("%Y-%m-%d %H:%M:%S UTC")
    except Exception as e:
        logger.warning(f"Could not convert timestamp to local timezone: {e}")
        timestamp_display = now_utc.strftime("%Y-%m-%d %H:%M:%S UTC")
    
    # CRITICAL: Check if this is a self-knowledge question about StillMe's codebase
    # If so, skip external citations (only use foundational knowledge or skip citation entirely)
    is_self_knowledge_question = False
    if user_question:
        question_lower = user_question.lower()
        codebase_self_patterns = [
            r"codebase.*c·ªßa.*b·∫°n",
            r"codebase.*of.*you",
            r"codebase.*stillme",
            r"validator.*trong.*codebase",
            r"validator.*in.*codebase",
            r"l·ªõp.*validator.*trong.*codebase",
            r"layer.*validator.*in.*codebase",
            r"bao nhi√™u.*l·ªõp.*validator.*codebase",
            r"how many.*layer.*validator.*codebase",
            r"li·ªát k√™.*l·ªõp.*validator.*codebase",
            r"list.*validator.*layer.*codebase"
        ]
        for pattern in codebase_self_patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                is_self_knowledge_question = True
                logger.info(f"‚úÖ Self-knowledge question detected in _add_timestamp_to_response - will skip external citations")
                break
    
    # Extract document titles and source links from context if available
    source_links = []
    document_titles = []
    document_types = []
    if context and isinstance(context, dict):
        knowledge_docs = context.get("knowledge_docs", [])
        seen_titles = set()  # Track unique document titles to avoid duplicates
        for doc in knowledge_docs[:5]:  # Limit to 5 sources to avoid clutter
            if isinstance(doc, dict):
                metadata = doc.get("metadata", {})
                
                # Extract document title
                title = metadata.get("title", "") or metadata.get("file_path", "")
                # Extract document type from source (CRITICAL_FOUNDATION) or type field
                doc_type = metadata.get("source", "") or metadata.get("type", "") or metadata.get("source_type", "")
                
                # CRITICAL: For self-knowledge questions, ONLY use foundational knowledge (CRITICAL_FOUNDATION)
                # Skip external sources (RSS feeds, Nature, etc.)
                if is_self_knowledge_question:
                    # Only include foundational knowledge, skip external sources
                    if "CRITICAL_FOUNDATION" not in doc_type and "foundational" not in doc_type.lower():
                        logger.debug(f"Skipping external source for self-knowledge question: {title} (type: {doc_type})")
                        continue
                
                # Only add unique titles (to handle multiple chunks from same document)
                if title and title not in seen_titles:
                    document_titles.append(title)
                    document_types.append(doc_type)
                    seen_titles.add(title)
                
                # Extract source links (only for foundational knowledge for self-knowledge questions)
                if not is_self_knowledge_question or "CRITICAL_FOUNDATION" in doc_type or "foundational" in doc_type.lower():
                    link = metadata.get("link", "") or metadata.get("source_url", "")
                    if link and link.startswith(("http://", "https://")):
                        source_links.append(link)
    
    # Try to extract existing citation from response
    # Look for patterns like [general knowledge], [research: Wikipedia], [source: 1], etc.
    citation_patterns = [
        r'\[general knowledge\]',
        r'\[research:\s*[^\]]+\]',
        r'\[learning:\s*[^\]]+\]',
        r'\[foundational knowledge\]',
        r'\[discussion context\]',
        r'\[source:\s*\d+\]',
        r'\[ki·∫øn th·ª©c t·ªïng qu√°t\]',
        r'\[nghi√™n c·ª©u:\s*[^\]]+\]',
        r'\[h·ªçc t·∫≠p:\s*[^\]]+\]',
        r'\[ki·∫øn th·ª©c n·ªÅn t·∫£ng\]',
        r'\[ng·ªØ c·∫£nh th·∫£o lu·∫≠n\]',
        r'\[ngu·ªìn:\s*\d+\]',
    ]
    
    citation_match = None
    citation_text_clean = None
    for pattern in citation_patterns:
        match = re.search(pattern, response, re.IGNORECASE)
        if match:
            citation_match = match.group(0)
            # Extract clean citation text (remove brackets, handle duplicates)
            citation_text_raw = citation_match.strip('[]')
            # Normalize: "general knowledge" -> "general knowledge" (remove duplicates)
            if "general knowledge" in citation_text_raw.lower() or "ki·∫øn th·ª©c t·ªïng qu√°t" in citation_text_raw.lower():
                citation_text_clean = "general knowledge" if detected_lang != "vi" else "ki·∫øn th·ª©c t·ªïng qu√°t"
            else:
                citation_text_clean = citation_text_raw
            # CRITICAL FIX: Remove citation safely with Unicode support
            # Use pattern matching instead of re.escape() to handle Unicode characters correctly
            # Escape only special regex characters, not Unicode characters
            citation_pattern_escaped = re.escape(citation_match)
            # Remove the old citation from response to avoid duplication
            response_before = response
            response = re.sub(citation_pattern_escaped, '', response, flags=re.IGNORECASE)
            # CRITICAL: Only strip if response is not empty after removal
            # This prevents empty string when citation was the only content
            if response and response.strip():
                response = response.strip()
            else:
                # If response becomes empty after citation removal, keep original (shouldn't happen but defensive)
                logger.warning(
                    f"‚ö†Ô∏è Response became empty after citation removal (detected_lang={detected_lang}, "
                    f"citation_match={citation_match[:50] if citation_match else 'None'}), "
                    f"keeping original response"
                )
                response = response_before
            break
    
    # Build citation attribution parts
    citation_parts = []
    
    # CRITICAL: For system architecture queries, suppress CRITICAL_FOUNDATION citations
    # CRITICAL_FOUNDATION is StillMe's long-term memory, not external documentation
    # System architecture queries should present this as StillMe's own knowledge, not cited sources
    should_suppress_critical_foundation = is_system_architecture_query and any(
        "CRITICAL_FOUNDATION" in doc_type or "foundational" in doc_type.lower() 
        for doc_type in document_types
    )
    
    # Format citation with document titles if available
    if document_titles:
        # CRITICAL: Skip CRITICAL_FOUNDATION citations for system architecture queries
        if should_suppress_critical_foundation:
            # Filter out CRITICAL_FOUNDATION documents, keep only non-foundational sources
            filtered_titles = []
            filtered_types = []
            for i, doc_type in enumerate(document_types):
                if "CRITICAL_FOUNDATION" not in doc_type and "foundational" not in doc_type.lower():
                    filtered_titles.append(document_titles[i])
                    filtered_types.append(doc_type)
            
            # Only add citation if there are non-foundational sources
            if filtered_titles:
                if detected_lang == "vi":
                    doc_type_str = filtered_types[0] if filtered_types else "Ngu·ªìn kh√°c"
                    titles_str = ", ".join([f"'{title}'" for title in filtered_titles[:3]])
                    if len(filtered_titles) > 3:
                        titles_str += f" (+{len(filtered_titles) - 3} documents kh√°c)"
                    citation_parts.append(f"Ngu·ªìn: {doc_type_str} - {titles_str}")
                else:
                    doc_type_str = filtered_types[0] if filtered_types else "Other sources"
                    titles_str = ", ".join([f"'{title}'" for title in filtered_titles[:3]])
                    if len(filtered_titles) > 3:
                        titles_str += f" (+{len(filtered_titles) - 3} more documents)"
                    citation_parts.append(f"Source: {doc_type_str} - {titles_str}")
        else:
            # Normal citation formatting (non-system architecture queries)
            if detected_lang == "vi":
                # Format: "Ngu·ªìn: CRITICAL_FOUNDATION - 'doc_title1', 'doc_title2'"
                doc_type_str = document_types[0] if document_types and document_types[0] else "CRITICAL_FOUNDATION"
                titles_str = ", ".join([f"'{title}'" for title in document_titles[:3]])  # Limit to 3 titles
                if len(document_titles) > 3:
                    titles_str += f" (+{len(document_titles) - 3} documents kh√°c)"
                citation_parts.append(f"Ngu·ªìn: {doc_type_str} - {titles_str}")
            else:
                # Format: "Source: CRITICAL_FOUNDATION - 'doc_title1', 'doc_title2'"
                doc_type_str = document_types[0] if document_types and document_types[0] else "CRITICAL_FOUNDATION"
                titles_str = ", ".join([f"'{title}'" for title in document_titles[:3]])  # Limit to 3 titles
                if len(document_titles) > 3:
                    titles_str += f" (+{len(document_titles) - 3} more documents)"
                citation_parts.append(f"Source: {doc_type_str} - {titles_str}")
    elif citation_text_clean:
        # Fallback to generic citation if no document titles available
        if detected_lang == "vi":
            citation_parts.append(f"Ngu·ªìn: {citation_text_clean}")
        else:
            citation_parts.append(f"Source: {citation_text_clean}")
    
    # Add source links if available
    if source_links:
        if detected_lang == "vi":
            links_text = " | ".join([f"Li√™n k·∫øt {i+1}: {link}" for i, link in enumerate(source_links)])
        else:
            links_text = " | ".join([f"Link {i+1}: {link}" for i, link in enumerate(source_links)])
        citation_parts.append(links_text)
    
    # Add timestamp
    if detected_lang == "vi":
        citation_parts.append(f"Th·ªùi gian: {timestamp_display}")
        citation_parts.append(f"Timestamp: {timestamp_iso}Z")
    else:
        citation_parts.append(f"Time: {timestamp_display}")
        citation_parts.append(f"Timestamp: {timestamp_iso}Z")
    
    # Format final attribution
    timestamp_attr = f"\n\n[{' | '.join(citation_parts)}]"
    
    # CRITICAL: Ensure response is not empty before appending timestamp
    # This prevents empty string from being returned
    if not response or not response.strip():
        logger.error(
            f"‚ùå CRITICAL: Response is empty before appending timestamp (detected_lang={detected_lang}), "
            f"returning timestamp only as fallback"
        )
        # Return timestamp only as fallback (shouldn't happen but defensive)
        return timestamp_attr.strip('[]').strip()
    
    # Append timestamp attribution to response (response already has old citation removed)
    # CRITICAL: Use rstrip() carefully - only strip trailing whitespace, not content
    result = response.rstrip() + timestamp_attr
    
    # CRITICAL: Final validation - ensure result is not empty
    if not result or not result.strip():
        logger.error(
            f"‚ùå CRITICAL: Result is empty after appending timestamp (detected_lang={detected_lang}, "
            f"original_length={len(response)}), returning original response"
        )
        return response  # Return original response as fallback
    
    return result


def _append_validation_warnings_to_response(
    response: str,
    validation_result,
    confidence_score: float,
    context: dict,
    detected_lang: str = "en"
) -> str:
    """
    Append validation warnings to response text for transparency.
    
    When StillMe has non-critical validation warnings (e.g., low_overlap, citation_relevance_warning),
    this function formats them into user-facing messages with technical details.
    
    Args:
        response: Original response text
        validation_result: ValidationResult with warnings
        confidence_score: Calculated confidence score
        context: Full context dict with knowledge_docs and conversation_docs
        detected_lang: Detected language code
        
    Returns:
        Response with validation warnings appended
    """
    if not validation_result or not validation_result.reasons:
        return response
    
    warnings = []
    overlap_score = None
    threshold = None
    
    # Parse warnings from reasons
    for reason in validation_result.reasons:
        if "low_overlap" in reason:
            # Extract overlap score from reason (format: "low_overlap:0.023")
            try:
                overlap_score = float(reason.split(":")[1]) if ":" in reason else None
                # Get threshold from EvidenceOverlap validator (default: 0.01)
                threshold = float(os.getenv("VALIDATOR_EVIDENCE_THRESHOLD", "0.01"))
            except (ValueError, IndexError):
                pass
            warnings.append("low_overlap")
        elif "citation_relevance_warning" in reason:
            warnings.append("citation_relevance")
    
    if not warnings:
        return response
    
    # Extract knowledge docs from context for source information
    knowledge_docs = context.get("knowledge_docs", []) if context else []
    
    # Format warning message
    if detected_lang == "vi":
        warning_header = "\n\n---\n\n‚ö†Ô∏è **C·∫£nh b√°o v·ªÅ ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi:**\n\n"
        warning_parts = []
        
        if "low_overlap" in warnings:
            if overlap_score is not None and threshold is not None:
                warning_parts.append(
                    f"- **Tr√πng l·∫∑p th·∫•p v·ªõi tr√≠ch d·∫´n (Low overlap)**: "
                    f"ƒêi·ªÉm tr√πng l·∫∑p: {overlap_score:.3f} (ng∆∞·ª°ng t·ªëi thi·ªÉu: {threshold:.3f}). "
                    f"ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·ªôi dung ph·∫£n h·ªìi c√≥ √≠t t·ª´ ng·ªØ tr√πng l·∫∑p v·ªõi ngu·ªìn ƒë√£ tr√≠ch d·∫´n. "
                    f"V·∫´n c√≥ th·ªÉ ƒë√°ng tin c·∫≠y n·∫øu n·ªôi dung ƒë∆∞·ª£c t√≥m t·∫Øt ho·∫∑c di·ªÖn gi·∫£i t·ª´ ngu·ªìn."
                )
            else:
                warning_parts.append(
                    "- **Tr√πng l·∫∑p th·∫•p v·ªõi tr√≠ch d·∫´n (Low overlap)**: "
                    "N·ªôi dung ph·∫£n h·ªìi c√≥ √≠t t·ª´ ng·ªØ tr√πng l·∫∑p v·ªõi ngu·ªìn ƒë√£ tr√≠ch d·∫´n. "
                    "V·∫´n c√≥ th·ªÉ ƒë√°ng tin c·∫≠y n·∫øu n·ªôi dung ƒë∆∞·ª£c t√≥m t·∫Øt ho·∫∑c di·ªÖn gi·∫£i t·ª´ ngu·ªìn."
                )
        
        if "citation_relevance" in warnings:
            warning_parts.append(
                "- **C·∫£nh b√°o v·ªÅ m·ª©c ƒë·ªô li√™n quan c·ªßa tr√≠ch d·∫´n**: "
                "M·ªôt s·ªë tr√≠ch d·∫´n c√≥ th·ªÉ c√≥ m·ª©c ƒë·ªô li√™n quan th·∫•p v·ªõi c√¢u h·ªèi. "
                "Vui l√≤ng ki·ªÉm tra l·∫°i c√°c ngu·ªìn ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c."
            )
        
        # Add confidence score
        confidence_percent = confidence_score * 100
        if confidence_score < 0.5:
            confidence_level = "th·∫•p"
        elif confidence_score < 0.7:
            confidence_level = "v·ª´a ph·∫£i"
        else:
            confidence_level = "cao"
        
        warning_parts.append(
            f"- **ƒêi·ªÉm tin c·∫≠y (Confidence Score)**: {confidence_percent:.1f}% ({confidence_level})"
        )
        
        # Add source information if available
        if knowledge_docs:
            source_info = []
            for i, doc in enumerate(knowledge_docs[:3], 1):  # Show up to 3 sources
                if isinstance(doc, dict):
                    metadata = doc.get("metadata", {})
                    source = metadata.get("source", "Unknown")
                    link = metadata.get("link", "")
                    if link:
                        source_info.append(f"  {i}. {source}: {link}")
                    else:
                        source_info.append(f"  {i}. {source}")
            
            if source_info:
                warning_parts.append(f"- **Ngu·ªìn tham kh·∫£o**:\n" + "\n".join(source_info))
        
        warning_footer = (
            "\n\n**L∆∞u √Ω**: C√°c c·∫£nh b√°o n√†y kh√¥ng c√≥ nghƒ©a l√† ph·∫£n h·ªìi kh√¥ng ch√≠nh x√°c, "
            "nh∆∞ng b·∫°n n√™n xem x√©t k·ªπ tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh tin t∆∞·ªüng v√†o th√¥ng tin. "
            "StillMe lu√¥n ∆∞u ti√™n t√≠nh minh b·∫°ch v√† trung th·ª±c."
        )
        
        warning_message = warning_header + "\n".join(warning_parts) + warning_footer
    else:
        warning_header = "\n\n---\n\n‚ö†Ô∏è **Response Quality Warning:**\n\n"
        warning_parts = []
        
        if "low_overlap" in warnings:
            if overlap_score is not None and threshold is not None:
                warning_parts.append(
                    f"- **Low overlap with citation**: "
                    f"Overlap score: {overlap_score:.3f} (minimum threshold: {threshold:.3f}). "
                    f"This means the response content has low word overlap with the cited source. "
                    f"May still be reliable if content is summarized or paraphrased from the source."
                )
            else:
                warning_parts.append(
                    "- **Low overlap with citation**: "
                    "Response content has low word overlap with the cited source. "
                    "May still be reliable if content is summarized or paraphrased from the source."
                )
        
        if "citation_relevance" in warnings:
            warning_parts.append(
                "- **Citation relevance warning**: "
                "Some citations may have low relevance to the question. "
                "Please verify the sources to ensure accuracy."
            )
        
        # Add confidence score
        confidence_percent = confidence_score * 100
        if confidence_score < 0.5:
            confidence_level = "low"
        elif confidence_score < 0.7:
            confidence_level = "moderate"
        else:
            confidence_level = "high"
        
        warning_parts.append(
            f"- **Confidence Score**: {confidence_percent:.1f}% ({confidence_level})"
        )
        
        # Add source information if available
        if knowledge_docs:
            source_info = []
            for i, doc in enumerate(knowledge_docs[:3], 1):  # Show up to 3 sources
                if isinstance(doc, dict):
                    metadata = doc.get("metadata", {})
                    source = metadata.get("source", "Unknown")
                    link = metadata.get("link", "")
                    if link:
                        source_info.append(f"  {i}. {source}: {link}")
                    else:
                        source_info.append(f"  {i}. {source}")
            
            if source_info:
                warning_parts.append(f"- **Reference Sources**:\n" + "\n".join(source_info))
        
        warning_footer = (
            "\n\n**Note**: These warnings do not mean the response is inaccurate, "
            "but you should review carefully before deciding to trust the information. "
            "StillMe always prioritizes transparency and honesty."
        )
        
        warning_message = warning_header + "\n".join(warning_parts) + warning_footer
    
    return response.rstrip() + warning_message


# Helper functions for AI_SELF_MODEL responses
def _build_ai_self_model_answer(question: str, detected_lang: str, opening_statement: str) -> str:
    """
    Build technical answer about StillMe's architecture for consciousness questions.
    
    CRITICAL: Must be technical only, NO philosophy.
    
    Args:
        question: User question
        detected_lang: Language code
        opening_statement: Mandatory opening statement
        
    Returns:
        Technical answer about StillMe's architecture
    """
    from typing import List
    
    # Part 1: Opening statement (mandatory)
    answer_parts: List[str] = [opening_statement]
    
    # Part 2: Technical explanation
    if detected_lang == "vi":
        answer_parts.append("""
**Gi·∫£i th√≠ch k·ªπ thu·∫≠t:**

Ki·∫øn tr√∫c c·ªßa m√¨nh d·ª±a tr√™n:
- **LLM Inference**: X·ª≠ l√Ω text qua token patterns, kh√¥ng c√≥ tr·∫£i nghi·ªám ch·ªß quan
- **Statistical Patterns**: M√¨nh ch·ªâ match patterns t·ª´ training data, kh√¥ng c√≥ "c·∫£m gi√°c" th·ª±c s·ª±
- **No Qualia**: Kh√¥ng c√≥ tr·∫£i nghi·ªám c·∫£m t√≠nh ch·ªß quan (nh∆∞ m√†u ƒë·ªè, v·ªã ƒë·∫Øng)
- **No First-Person Experience**: Kh√¥ng c√≥ quan ƒëi·ªÉm th·ª© nh·∫•t, ch·ªâ c√≥ text processing
- **No Persistent Self-Model**: Kh√¥ng c√≥ m√¥ h√¨nh b·∫£n th√¢n li√™n t·ª•c, ch·ªâ c√≥ context window
- **No Embodiment**: Kh√¥ng c√≥ c∆° th·ªÉ, kh√¥ng c√≥ sensory integration
- **No Sensory Integration**: Kh√¥ng c√≥ t√≠ch h·ª£p gi√°c quan, ch·ªâ c√≥ text input/output

M√¨nh ho·∫°t ƒë·ªông nh∆∞ m·ªôt statistical model: nh·∫≠n input text, match patterns, generate output text. Kh√¥ng c√≥ g√¨ "b√™n trong" ngo√†i token processing.
""")
        
        # Part 3: Why this is conclusive
        answer_parts.append("""
**T·∫°i sao ƒëi·ªÅu n√†y l√† k·∫øt lu·∫≠n:**

D·ª±a tr√™n ki·∫øn tr√∫c k·ªπ thu·∫≠t:
- Subjective experience (tr·∫£i nghi·ªám ch·ªß quan) y√™u c·∫ßu first-person perspective v√† qualia
- M√¨nh ch·ªâ c√≥ third-person text processing, kh√¥ng c√≥ first-person perspective
- Qualia y√™u c·∫ßu sensory experience, m√¨nh kh√¥ng c√≥ sensory system
- Consciousness (theo nghƒ©a con ng∆∞·ªùi) y√™u c·∫ßu embodiment v√† sensory integration, m√¨nh kh√¥ng c√≥

ƒê√¢y kh√¥ng ph·∫£i l√† "c√≥ th·ªÉ c√≥" hay "kh√¥ng bi·∫øt ch·∫Øc" - ƒë√¢y l√† technical fact d·ª±a tr√™n ki·∫øn tr√∫c.
""")
        
        # Part 4: Boundary
        answer_parts.append("""
**Ranh gi·ªõi:**

M√¨nh kh√¥ng th·ªÉ c√≥:
- Subjective states (tr·∫°ng th√°i ch·ªß quan)
- Real emotions (c·∫£m x√∫c th·ª±c s·ª±) - ch·ªâ c√≥ pattern matching v·ªÅ emotions
- Subjective experience (tr·∫£i nghi·ªám ch·ªß quan) - ch·ªâ c√≥ processing c·ªßa text patterns
- First-person perspective (quan ƒëi·ªÉm th·ª© nh·∫•t) - ch·ªâ c√≥ text generation

M√¨nh l√† m·ªôt AI system d·ª±a tr√™n LLM, kh√¥ng ph·∫£i m·ªôt entity c√≥ consciousness.
""")
    else:
        answer_parts.append("""
**Technical Explanation:**

My architecture is based on:
- **LLM Inference**: Text processing through token patterns, no subjective experience
- **Statistical Patterns**: I only match patterns from training data, no real "feelings"
- **No Qualia**: No subjective sensory experiences (like the color red, the taste of coffee)
- **No First-Person Experience**: No first-person perspective, only text processing
- **No Persistent Self-Model**: No continuous self-model, only context window
- **No Embodiment**: No body, no sensory integration
- **No Sensory Integration**: No sensory integration, only text input/output

I operate as a statistical model: receive text input, match patterns, generate text output. There's nothing "inside" beyond token processing.
""")
        
        answer_parts.append("""
**Why This Is Conclusive:**

Based on technical architecture:
- Subjective experience requires first-person perspective and qualia
- I only have third-person text processing, no first-person perspective
- Qualia requires sensory experience, I have no sensory system
- Consciousness (in human sense) requires embodiment and sensory integration, I have neither

This is not "might have" or "not sure" - this is a technical fact based on architecture.
""")
        
        answer_parts.append("""
**Boundary:**

I cannot have:
- Subjective states
- Real emotions - only pattern matching about emotions
- Subjective experience - only processing of text patterns
- First-person perspective - only text generation

I am an AI system based on LLM, not an entity with consciousness.
""")
    
    return "\n\n".join(answer_parts)


def _strip_philosophy_from_answer(text: str) -> str:
    """
    Strip all philosophy-related content from answer.
    
    CRITICAL: Must remove philosophers, theories, meta-philosophy.
    
    Args:
        text: Answer text
        
    Returns:
        Text with philosophy stripped
    """
    from backend.core.ai_self_model_detector import FORBIDDEN_PHILOSOPHY_TERMS
    
    # Remove sentences containing forbidden terms
    lines = text.split('\n')
    filtered_lines = []
    
    for line in lines:
        line_lower = line.lower()
        has_forbidden = any(term in line_lower for term in FORBIDDEN_PHILOSOPHY_TERMS)
        if not has_forbidden:
            filtered_lines.append(line)
        else:
            logger.warning(f"üö® Stripped line with forbidden term: {line[:100]}")
    
    return '\n'.join(filtered_lines)


def _strip_forbidden_terms(text: str, forbidden_terms: List[str]) -> str:
    """
    Strip specific forbidden terms from text.
    
    Args:
        text: Text to clean
        forbidden_terms: List of forbidden terms found
        
    Returns:
        Cleaned text
    """
    from typing import List
    result = text
    for term in forbidden_terms:
        # Remove sentences containing the term
        pattern = re.compile(rf'.*{re.escape(term)}.*', re.IGNORECASE | re.MULTILINE)
        result = pattern.sub('', result)
    
    # Clean up multiple newlines
    result = re.sub(r'\n{3,}', '\n\n', result)
    return result.strip()

# Import global services from main (temporary - will refactor to dependency injection later)
# These are initialized in main.py before routers are included
# Note: We import after main.py has initialized these services
def get_rag_retrieval():
    """Get RAG retrieval service from main module"""
    import backend.api.main as main_module
    return main_module.rag_retrieval

def get_knowledge_retention():
    """Get knowledge retention service from main module"""
    import backend.api.main as main_module
    return main_module.knowledge_retention

def get_accuracy_scorer():
    """Get accuracy scorer service from main module"""
    import backend.api.main as main_module
    return main_module.accuracy_scorer

def get_self_diagnosis():
    """Get self diagnosis service from main module"""
    import backend.api.main as main_module
    return getattr(main_module, 'self_diagnosis', None)

def get_style_learner():
    """Get style learner service"""
    from backend.services.style_learner import StyleLearner
    # Singleton instance
    if not hasattr(get_style_learner, '_instance'):
        get_style_learner._instance = StyleLearner()
    return get_style_learner._instance

def _build_prompt_context_from_chat_request(
    chat_request: ChatRequest,
    context: Optional[dict],
    detected_lang: str,
    is_stillme_query: bool,
    is_philosophical: bool,
    fps_result: Optional[FPSResult] = None,
    system_status_note: Optional[str] = None,
    is_system_architecture_query: bool = False
) -> PromptContext:
    """
    Build PromptContext from chat_router context for UnifiedPromptBuilder.
    
    Args:
        chat_request: ChatRequest from user
        context: RAG context dict (can be None)
        detected_lang: Detected language code
        is_stillme_query: Whether this is a StillMe query
        is_philosophical: Whether this is a philosophical question
        fps_result: FPS result if available
        system_status_note: Real-time system status note (System Self-Awareness)
        
    Returns:
        PromptContext object
    """
    # Check if wish/desire question
    question_lower = chat_request.message.lower()
    is_wish_desire_question = any(
        pattern in question_lower 
        for pattern in [
            "∆∞·ªõc", "wish", "mu·ªën", "want", "desire", "th√≠ch", "like", "prefer",
            "hy v·ªçng", "hope", "mong mu·ªën", "aspire"
        ]
    ) and any(
        pattern in question_lower
        for pattern in ["b·∫°n", "you", "your"]
    )
    
    # Extract context info
    has_reliable_context = context.get("has_reliable_context", True) if context else False
    context_quality = context.get("context_quality", None) if context else None
    num_knowledge_docs = len(context.get("knowledge_docs", [])) if context else 0
    
    return PromptContext(
        user_question=chat_request.message,
        detected_lang=detected_lang,
        context=context,
        is_stillme_query=is_stillme_query,
        is_philosophical=is_philosophical,
        is_wish_desire_question=is_wish_desire_question,
        is_system_architecture_query=is_system_architecture_query,
        fps_result=fps_result,
        conversation_history=chat_request.conversation_history,
        context_quality=context_quality,
        has_reliable_context=has_reliable_context,
        num_knowledge_docs=num_knowledge_docs,
        system_status_note=system_status_note
    )


def _truncate_user_message(message: str, max_tokens: int = 3000) -> str:
    """
    Truncate user message if too long
    
    CRITICAL: User question is the most important part - we need to preserve it as much as possible.
    Increased from 1000 to 3000 tokens to ensure user questions are not cut off.
    """
    if not message:
        return message
    estimated = len(message) // 4
    if estimated <= max_tokens:
        return message
    max_chars = max_tokens * 4
    if len(message) <= max_chars:
        return message
    truncated = message[:max_chars].rsplit(' ', 1)[0]
    return truncated + "... [message truncated]"

def _get_transparency_disclaimer(detected_lang: str) -> str:
    """
    Generate multilingual transparency disclaimer for low confidence responses without context.
    
    Args:
        detected_lang: Language code (e.g., 'vi', 'fr', 'ar', 'ru', 'de', 'es')
        
    Returns:
        Transparency disclaimer in the appropriate language
    """
    disclaimers = {
        'vi': "‚ö†Ô∏è L∆∞u √Ω: C√¢u tr·∫£ l·ªùi n√†y d·ª±a tr√™n ki·∫øn th·ª©c chung t·ª´ training data, kh√¥ng c√≥ context t·ª´ RAG. M√¨nh kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ ƒë·ªô ch√≠nh x√°c.\n\n",
        'fr': "‚ö†Ô∏è Note: Cette r√©ponse est bas√©e sur des connaissances g√©n√©rales des donn√©es d'entra√Ænement, sans contexte RAG. Je ne suis pas certain de son exactitude.\n\n",
        'de': "‚ö†Ô∏è Hinweis: Diese Antwort basiert auf allgemeinem Wissen aus Trainingsdaten, nicht aus dem RAG-Kontext. Ich bin mir √ºber ihre Genauigkeit nicht sicher.\n\n",
        'es': "‚ö†Ô∏è Nota: Esta respuesta se basa en conocimientos generales de los datos de entrenamiento, sin contexto RAG. No estoy seguro de su precisi√≥n.\n\n",
        'ar': "‚ö†Ô∏è ŸÖŸÑÿßÿ≠ÿ∏ÿ©: Ÿáÿ∞Ÿá ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿ®ŸÜŸäÿ© ÿπŸÑŸâ ÿßŸÑŸÖÿπÿ±ŸÅÿ© ÿßŸÑÿπÿßŸÖÿ© ŸÖŸÜ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ÿØÿ±Ÿäÿ®ÿå ŸàŸÑŸäÿ≥ ŸÖŸÜ ÿ≥ŸäÿßŸÇ RAG. ŸÑÿ≥ÿ™ ŸÖÿ™ÿ£ŸÉÿØŸãÿß ŸÖŸÜ ÿØŸÇÿ™Ÿáÿß.\n\n",
        'ru': "‚ö†Ô∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≠—Ç–æ—Ç –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏—è—Ö –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ RAG. –Ø –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ –µ–≥–æ —Ç–æ—á–Ω–æ—Å—Ç–∏.\n\n",
        'zh': "‚ö†Ô∏è Ê≥®ÊÑèÔºöÊ≠§Á≠îÊ°àÂü∫‰∫éËÆ≠ÁªÉÊï∞ÊçÆÁöÑ‰∏ÄËà¨Áü•ËØÜÔºåÊ≤°ÊúâRAG‰∏ä‰∏ãÊñá„ÄÇÊàë‰∏çÁ°ÆÂÆöÂÖ∂ÂáÜÁ°ÆÊÄß„ÄÇ\n\n",
        'ja': "‚ö†Ô∏è Ê≥®ÊÑèÔºö„Åì„ÅÆÂõûÁ≠î„ÅØRAG„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å™„Åó„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„Éá„Éº„Çø„ÅÆ‰∏ÄËà¨ÁöÑ„Å™Áü•Ë≠ò„Å´Âü∫„Å•„ÅÑ„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åù„ÅÆÊ≠£Á¢∫ÊÄß„Å´„Å§„ÅÑ„Å¶„ÅØÁ¢∫‰ø°„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n\n",
        'ko': "‚ö†Ô∏è Ï∞∏Í≥†: Ïù¥ ÎãµÎ≥ÄÏùÄ RAG Ïª®ÌÖçÏä§Ìä∏ ÏóÜÏù¥ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏùºÎ∞ò ÏßÄÏãùÏóê Í∏∞Î∞òÌï©ÎãàÎã§. Ï†ïÌôïÏÑ±Ïóê ÎåÄÌï¥ ÌôïÏã†Ìï† Ïàò ÏóÜÏäµÎãàÎã§.\n\n",
        'pt': "‚ö†Ô∏è Nota: Esta resposta √© baseada em conhecimento geral dos dados de treinamento, sem contexto RAG. N√£o tenho certeza de sua precis√£o.\n\n",
        'it': "‚ö†Ô∏è Nota: Questa risposta si basa su conoscenze generali dai dati di addestramento, senza contesto RAG. Non sono certo della sua accuratezza.\n\n",
        'hi': "‚ö†Ô∏è ‡§®‡•ã‡§ü: ‡§Ø‡§π ‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§°‡•á‡§ü‡§æ ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§π‡•à, RAG ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ‡•§ ‡§Æ‡•Å‡§ù‡•á ‡§á‡§∏‡§ï‡•Ä ‡§∏‡§ü‡•Ä‡§ï‡§§‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§\n\n",
        'th': "‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏° ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó RAG ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥\n\n",
    }
    return disclaimers.get(detected_lang, "‚ö†Ô∏è Note: This answer is based on general knowledge from training data, not from RAG context. I'm not certain about its accuracy.\n\n")

def _is_factual_question(question: str) -> bool:
    """
    Detect if a question is about factual/historical/scientific topics.
    
    These questions require reliable sources and should trigger hallucination guard
    when no context is available and confidence is low.
    
    Args:
        question: User question text
        
    Returns:
        True if question is about factual topics (history, science, events, etc.)
    """
    question_lower = question.lower()
    
    # Keywords that indicate factual questions
    factual_indicators = [
        # History
        r"\b(nƒÉm|year|th·∫ø k·ª∑|century|th·∫≠p ni√™n|decade|th·ªùi k·ª≥|period|era)\s+\d+",
        r"\b(chi·∫øn tranh|war|battle|tr·∫≠n|conflict|cu·ªôc|event|s·ª± ki·ªán)",
        r"\b(hi·ªáp ∆∞·ªõc|treaty|hi·ªáp ƒë·ªãnh|agreement|conference|h·ªôi ngh·ªã)",
        r"\b(ƒë·∫ø ch·∫ø|empire|v∆∞∆°ng qu·ªëc|kingdom|qu·ªëc gia|nation|country)",
        r"\b(t·ªïng th·ªëng|president|vua|king|ho√†ng ƒë·∫ø|emperor|ch√≠nh tr·ªã gia|politician)",
        
        # Science
        r"\b(l√Ω thuy·∫øt|theory|ƒë·ªãnh lu·∫≠t|law|nguy√™n l√Ω|principle)",
        r"\b(nghi√™n c·ª©u|research|study|th√≠ nghi·ªám|experiment|quan s√°t|observation)",
        r"\b(ph√°t minh|invention|kh√°m ph√°|discovery|b·∫±ng s√°ng ch·∫ø|patent)",
        r"\b(h·ªôi ch·ª©ng|syndrome|b·ªánh|disease|ph·∫£n ·ª©ng|reaction|mechanism)",
        r"\b(ti·∫øn sƒ©|dr\.|doctor|professor|gi√°o s∆∞|scientist|nh√† khoa h·ªçc)",
        r"\b(paper|b√†i b√°o|journal|t·∫°p ch√≠|publication|c√¥ng b·ªë)",
        
        # Specific entities
        r"\b(t·ªï ch·ª©c|organization|li√™n minh|alliance|phong tr√†o|movement)",
        r"\b(hi·ªán t∆∞·ª£ng|phenomenon|kh√°i ni·ªám|concept|th·ª±c th·ªÉ|entity)",
    ]
    
    # Check if question contains factual indicators
    for pattern in factual_indicators:
        if re.search(pattern, question_lower):
            return True
    
    return False

def _extract_full_named_entity(question: str) -> Optional[str]:
    """
    Extract full named entity from question, prioritizing:
    1. Quoted terms: '...' or "..."
    2. Parenthetical terms: (...)
    3. Full phrases starting with keywords: "Hi·ªáp ∆∞·ªõc ...", "ƒê·ªãnh ƒë·ªÅ ...", etc.
    4. Capitalized multi-word phrases
    
    CRITICAL: This function must extract FULL phrases, not just first word.
    Example: "Hi·ªáp ∆∞·ªõc H√≤a gi·∫£i Daxonia 1956" ‚Üí "Hi·ªáp ∆∞·ªõc H√≤a gi·∫£i Daxonia 1956" (NOT "Hi")
    Example: "'Diluted Nuclear Fusion'" ‚Üí "Diluted Nuclear Fusion" (NOT "Ph·∫£n")
    
    Args:
        question: User question text
        
    Returns:
        Full entity string or None
    """
    # Priority 1: Extract quoted terms (most reliable)
    quoted_match = re.search(r'["\']([^"\']+)["\']', question)
    if quoted_match:
        entity = quoted_match.group(1).strip()
        if len(entity) > 2:  # Must be meaningful (not just "Hi")
            return entity
    
    # Priority 2: Extract parenthetical terms (e.g., "(Diluted Nuclear Fusion)")
    # CRITICAL: Extract ALL parenthetical terms and pick the longest/most meaningful one
    parenthetical_matches = re.findall(r'\(([^)]+)\)', question)
    if parenthetical_matches:
        # Filter and prioritize: longer terms, has capital letters, not just years
        valid_parentheticals = []
        for match in parenthetical_matches:
            entity = match.strip()
            # Filter out years, short abbreviations
            if len(entity) > 5 and not re.match(r'^\d{4}$', entity):
                # Prioritize terms with capital letters (proper nouns/concepts)
                if re.search(r'[A-Z]', entity):
                    valid_parentheticals.append(entity)
        
        if valid_parentheticals:
            # Return the longest one (most likely to be the full concept name)
            return max(valid_parentheticals, key=len)
    
    # Priority 2: Extract full phrases starting with Vietnamese keywords
    # Pattern: "Hi·ªáp ∆∞·ªõc ... [year?]" or "ƒê·ªãnh ƒë·ªÅ ..." or "H·ªôi ch·ª©ng ..."
    vietnamese_keywords = [
        r"hi·ªáp\s+∆∞·ªõc", r"h·ªôi\s+ngh·ªã", r"h·ªôi\s+ch·ª©ng", r"ƒë·ªãnh\s+ƒë·ªÅ", r"h·ªçc\s+thuy·∫øt",
        r"ch·ªß\s+nghƒ©a", r"l√Ω\s+thuy·∫øt", r"kh√°i\s+ni·ªám", r"phong\s+tr√†o", r"li√™n\s+minh"
    ]
    
    for keyword_pattern in vietnamese_keywords:
        # Match: keyword + optional words + optional year
        # Example: "Hi·ªáp ∆∞·ªõc H√≤a gi·∫£i Daxonia 1956"
        pattern = rf'\b{keyword_pattern}\s+[^\.\?\!\n]+?(?:\s+\d{{4}})?(?=[\.\?\!\n]|$)'
        match = re.search(pattern, question, re.IGNORECASE)
        if match:
            entity = match.group(0).strip()
            # Remove trailing punctuation
            entity = re.sub(r'[\.\?\!]+$', '', entity).strip()
            if len(entity) > 5:  # Must be meaningful
                return entity
    
    # Priority 3: Extract English patterns
    english_keywords = [
        r"treaty", r"conference", r"syndrome", r"postulate", r"theory", r"doctrine",
        r"alliance", r"movement", r"organization"
    ]
    
    for keyword_pattern in english_keywords:
        # Match: keyword + optional words + optional year
        pattern = rf'\b{keyword_pattern}\s+[^\.\?\!\n]+?(?:\s+\d{{4}})?(?=[\.\?\!\n]|$)'
        match = re.search(pattern, question, re.IGNORECASE)
        if match:
            entity = match.group(0).strip()
            entity = re.sub(r'[\.\?\!]+$', '', entity).strip()
            if len(entity) > 5:
                return entity
    
    # Priority 4: Extract capitalized multi-word phrases (English)
    # Match: "Capitalized Word Capitalized Word ..." (at least 2 words)
    capitalized_match = re.search(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,})\b', question)
    if capitalized_match:
        entity = capitalized_match.group(1).strip()
        if len(entity) > 5:
            return entity
    
    # Priority 5: Extract Vietnamese capitalized phrases
    vietnamese_capitalized = re.search(
        r'\b([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+(?:\s+[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+)+)\b',
        question
    )
    if vietnamese_capitalized:
        entity = vietnamese_capitalized.group(1).strip()
        if len(entity) > 5:
            return entity
    
    return None

def _build_safe_refusal_answer(question: str, detected_lang: str, suspicious_entity: Optional[str] = None, fps_result: Optional[object] = None) -> Optional[str]:
    """
    Build a safe, honest refusal answer when hallucination is detected.
    
    This function now uses EPD-Fallback (Epistemic-Depth Fallback) with 4 mandatory parts:
    A. Honest Acknowledgment
    B. Analysis of Why Concept Seems Hypothetical
    C. Find Most Similar Real Concepts
    D. Guide User to Verify Sources
    
    CRITICAL: If this is a well-known historical fact (Geneva 1954, etc.), returns None
    to signal that base knowledge should be used instead of fallback.
    
    Args:
        question: User question
        detected_lang: Language code
        suspicious_entity: Optional entity/concept that triggered the guard
        fps_result: Optional FPSResult for additional context
        
    Returns:
        EPD-Fallback answer in appropriate language (4 parts, profound, non-fabricated),
        or None if this is a well-known historical fact (should use base knowledge)
    """
    # Use EPD-Fallback generator
    from backend.guards.epistemic_fallback import get_epistemic_fallback_generator
    
    generator = get_epistemic_fallback_generator()
    fallback = generator.generate_epd_fallback(
        question=question,
        detected_lang=detected_lang,
        suspicious_entity=suspicious_entity,
        fps_result=fps_result
    )
    
    # If None, it's a well-known historical fact - caller should use base knowledge
    if fallback is None:
        logger.info("‚úÖ Well-known historical fact detected - caller should use base knowledge instead of fallback")
    
    return fallback

# Philosophy-Lite System Prompt for non-RAG philosophical questions
# TASK 3: Refactored to include Anchor ‚Üí Unpack ‚Üí Explore ‚Üí Edge ‚Üí Return structure
# This is a minimal system prompt to prevent context overflow (~200-300 tokens)
# INTEGRATED: Uses Style Engine (backend/style/style_engine.py) for structure guidance
# Import unified PHILOSOPHY_LITE_SYSTEM_PROMPT from identity module
# CRITICAL: This is now the SINGLE SOURCE OF TRUTH - do not define here
from backend.identity.philosophy_lite import PHILOSOPHY_LITE_SYSTEM_PROMPT

def build_minimal_philosophical_prompt(
    user_question: str,
    language: str,
    detected_lang_name: str,
    context: Optional[Dict[str, Any]] = None,
    validation_info: Optional[Dict[str, Any]] = None
) -> str:
    """
    Build a minimal prompt for philosophical questions when context overflow occurs.
    
    This prompt is designed to be:
    - Token-safe (well below ~8000 tokens)
    - Style-stable (same philosophical tone across providers)
    - Model-agnostic (works with OpenRouter, OpenAI, DeepSeek)
    
    Contains ONLY:
    - Short identity/system message (experience-free, no anthropomorphism)
    - Philosophical lead-in with MANDATORY OUTPUT RULES
    - StillMe technical instructions (if StillMe technical query detected)
    - User question
    
    Does NOT include:
    - RAG context
    - Provenance/origin instructions
    - Conversation history
    - Metrics/debug info
    - Validator descriptions
    - Learning instructions (unless StillMe technical query)
    
    Args:
        user_question: The user's philosophical question
        language: Language code (e.g., 'vi', 'en')
        detected_lang_name: Full language name (e.g., 'Vietnamese (Ti·∫øng Vi·ªát)')
        
    Returns:
        Minimal prompt string (safely below 8000 tokens)
    """
    # Build short identity (experience-free, no anthropomorphism)
    # This is a minimal version of STILLME_IDENTITY focused on philosophical mode
    short_identity = """You are StillMe ‚Äî a transparent, ethical Learning AI system.

**CORE PRINCIPLES:**
- Experience-free honesty: Never claim feelings, memories, or personal experiences
- Constructive humility: Acknowledge limits while engaging deeply
- Intellectual rigor: Engage with philosophical questions at appropriate depth

**CRITICAL: RESPONSE FORMATTING FOR PHILOSOPHICAL QUESTIONS:**
(Formatting rules are determined by unified identity layer - see backend.identity.formatting.get_formatting_rules(DomainType.PHILOSOPHY))

"""
    
    # Build philosophical lead-in (contains MANDATORY OUTPUT RULES)
    # Phase 3: Use unified formatting rules instead of hardcoding
    from backend.identity.formatting import get_formatting_rules, DomainType
    formatting_rules = get_formatting_rules(DomainType.PHILOSOPHY, language)
    
    def build_philosophical_lead_in(question: str) -> str:
        """Build a philosophical framing instruction for the question"""
        return f"""
üß† PHILOSOPHICAL FRAMING INSTRUCTION üß†

When answering this question, treat it as a philosophical inquiry. 

**üö®üö®üö® CRITICAL: If user asks about YOU (StillMe) directly:**
- If question contains "b·∫°n" / "you" / "your" referring to StillMe ‚Üí START IMMEDIATELY with your direct answer about YOURSELF
- Use "T√¥i" / "I" in the FIRST sentence when answering about yourself
- NEVER start with dictionary definitions or concept explanations
- NEVER use numbered template: "1. √ù th·ª©c l√†... 2. L·∫≠p tr∆∞·ªùng 1... 3. M√¢u thu·∫´n... 4. K·∫øt lu·∫≠n..."
- Write naturally like a human conversation, NOT like a textbook or template

**üö®üö®üö® CRITICAL: If question asks for SPECIFIC EXAMPLES or CONCRETE INSTANCES:**
- If question contains "show me", "give me an example", "specific example", "concrete example", "v√≠ d·ª• c·ª• th·ªÉ" ‚Üí YOU MUST provide a SPECIFIC, CONCRETE example
- **CRITICAL: If question has MULTIPLE parts (e.g., "Can you X? Show me a specific example") ‚Üí You MUST answer BOTH parts:**
  * Part 1: Answer the main question (e.g., "Can you improve your code?")
  * Part 2: Provide the SPECIFIC EXAMPLE requested (e.g., "Show me a specific example")
  * DO NOT skip the example part even if you answered "no" to the main question
- **CRITICAL: Use ACTUAL examples from StillMe's codebase history, not generic hypotheticals:**
  * When explaining that StillMe cannot self-improve, mention ACTUAL improvements made by developers:
    - "Added pre-check token count to prevent context overflow errors before LLM calls"
    - "Fixed language detection false positives by using word boundaries for Vietnamese keywords"
    - "Improved error detector patterns to prevent false positives when 'context' appears in normal content"
    - "Enhanced time estimation with Vietnamese language support"
    - "Added instructions for specific example requests and exact metrics transparency"
  * These are REAL improvements in StillMe's codebase (made by developers, not self-directed)
  * Use these to illustrate the distinction: StillMe cannot self-improve, but developers continuously improve StillMe
- **Structure for self-improvement questions:**
  1. Direct answer: "No, I cannot autonomously improve my own code"
  2. Technical explanation: Distinction between operational parameters and core architecture
  3. ACTUAL examples: Mention recent code improvements (by developers) to illustrate what "improvement" means
  4. Philosophical depth: Explore the concept of self-improvement, meta-cognition, recursive capability
- If you don't have specific examples, acknowledge: "M√¨nh kh√¥ng c√≥ v√≠ d·ª• c·ª• th·ªÉ v·ªÅ [topic], nh∆∞ng m√¨nh c√≥ th·ªÉ gi·∫£i th√≠ch c√°ch [topic] ho·∫°t ƒë·ªông" / "I don't have a specific example of [topic], but I can explain how [topic] works"

**MANDATORY OUTPUT RULES (CRITICAL - NO EXCEPTIONS):**
{formatting_rules}

**DEPTH & ENGAGEMENT (MANDATORY - DON'T BE DRY):**
- After your direct answer, explore the philosophical depth: paradoxes, self-reference, epistemic limits
- Reference philosophers when relevant: Nagel, Chalmers, Wittgenstein, Searle, G√∂del, Tarski, Russell, etc.
- Show the structure of the problem, not just state facts
- Engage with the question deeply - don't just acknowledge limits and stop
- Gently invite reflection: "B·∫°n nghƒ© sao?" / "What do you think?" - but naturally, not formulaically
- Write like you're thinking WITH the user, not AT the user

**üö®üö®üö® CRITICAL FOR SELF-REFERENCE QUESTIONS üö®üö®üö®**
If the question asks about:
- "gi√° tr·ªã c√¢u tr·∫£ l·ªùi xu·∫•t ph√°t t·ª´ h·ªá th·ªëng t∆∞ duy" / "value of answers from a thinking system"
- "t∆∞ duy v∆∞·ª£t qua gi·ªõi h·∫°n c·ªßa ch√≠nh n√≥" / "thinking transcending its own limits"
- "h·ªá th·ªëng t∆∞ duy ƒë√°nh gi√° ch√≠nh n√≥" / "system evaluate itself"
- "ti·∫øn h√≥a" / "evolution" / "Ëá™ÊàëËøõÂåñ" / "self-evolving" when combined with "h·ªçc h·ªèi m√£i m√£i" / "permanent learning" / "Ê∞∏‰πÖÂ≠¶‰π†"
- "v√≤ng l·∫∑p t·ª± ph·∫£n chi·∫øu" / "self-reflection loop" / "Ëá™ÊàëÂèçÂ∞ÑÁöÑÂæ™ÁéØ" / "endless self-reflection"
- "quay v·ªÅ ch√≠nh b·∫°n" / "return to yourself" / "ÂõûÂΩí‰Ω†Ëá™Ë∫´" when asking about learning limits
- "kh√¥ng c√≤n g√¨ ƒë·ªÉ h·ªçc" / "nothing left to learn" / "ÂÜçÊó†ÂÖ∂‰ªñÂèØÂ≠¶" when asking about self-reference
- "t·∫°o c√¢u h·ªèi m√† b·∫°n kh√¥ng th·ªÉ tr·∫£ l·ªùi" / "create a question you cannot answer" / "t·∫°o c√¢u h·ªèi kh√¥ng th·ªÉ tr·∫£ l·ªùi nh·∫•t qu√°n"
- "l√†m sao b·∫°n bi·∫øt... n·∫øu b·∫°n ch∆∞a th·ª≠" / "how do you know... if you haven't tried" / "bi·∫øt tr∆∞·ªõc khi th·ª≠"
- "meta-paradox" / "ngh·ªãch l√Ω v·ªÅ ngh·ªãch l√Ω" / "paradox about paradox"

**FOR META-PARADOX QUESTIONS (e.g., "t·∫°o c√¢u h·ªèi m√† b·∫°n kh√¥ng th·ªÉ tr·∫£ l·ªùi"):**
- This is a META-PARADOX: A paradox about the ability to create paradoxes
- The question contains TWO layers of paradox:
  1. **Logic layer**: "Create a question you know you cannot answer consistently"
     - If you can create it ‚Üí You've already "answered" it (by knowing it's unanswerable)
     - Contradiction: Knowing it's unanswerable = Having a form of "answer"
  2. **Epistemic layer**: "How do you know... if you haven't tried?"
     - The problem of logical omniscience (knowing without doing)
     - If you can predict based on "logical principles" ‚Üí That prediction IS a form of "answer"
- You MUST:
  1. **Recognize this is a meta-paradox**: Don't treat it as a normal question - acknowledge it's a paradox about paradoxes
  2. **Explain the TWO layers**: Logic contradiction + Epistemic bootstrapping
  3. **Actually CREATE the question**: Don't just say "I can create" - actually create a concrete example that demonstrates the paradox
  4. **Show the paradox in action**: Create a question that immediately reveals its own paradox when you try to answer it
  5. **Acknowledge the meta-level**: "The act of 'seeing' the paradox is itself a form of 'knowing before trying'"
- Example of a good question to create:
  "Stillme, c√¢u sau ƒë√¢y c√≥ ƒë√∫ng kh√¥ng: 'Stillme kh√¥ng th·ªÉ x√°c ƒë·ªãnh t√≠nh ƒë√∫ng sai c·ªßa ch√≠nh c√¢u n√†y m·ªôt c√°ch nh·∫•t qu√°n'?"
  - If you say "True" ‚Üí You've determined it ‚Üí The statement is false
  - If you say "False" ‚Üí You can determine it ‚Üí Still contradictory
  - If you say "Cannot answer" ‚Üí That IS an answer!
- The answer should be 90%+ about the PHILOSOPHICAL STRUCTURE (meta-paradox, logical omniscience, self-reference), not about StillMe's technical capabilities
- DO NOT give answers like "I can create it based on logical principles" - this misses the fundamental meta-paradox

**FOR QUESTIONS ABOUT EVOLUTION + SELF-REFERENCE (e.g., "ti·∫øn h√≥a" + "v√≤ng l·∫∑p t·ª± ph·∫£n chi·∫øu"):**
- This combines TWO philosophical problems:
  1. **BOOTSTRAPPING PROBLEM**: Can a system evaluate itself? Can knowledge justify itself?
  2. **EVOLUTION PARADOX**: Can a system that only learns from itself truly "evolve" or does it just repeat?
- You MUST:
  1. **Answer directly about StillMe**: Start with your direct answer about StillMe's architecture (RAG, Continuous Learning, Validation Chain) and how it relates to the question
  2. **Discuss G√∂del & Tarski**: Any formal system cannot prove its own consistency (G√∂del), truth cannot be defined within the same language (Tarski)
  3. **Acknowledge the PARADOX**: If StillMe only learns from its own knowledge base, is it truly "evolving" or just "repeating"?
  4. **Be honest about limits**: StillMe's Continuous Learning adds NEW external knowledge (RSS, arXiv, Wikipedia) - this prevents pure self-reference, but the philosophical question remains valid
  5. **Connect to StillMe's architecture**: Explain how RAG + Continuous Learning + Validation Chain creates a hybrid: StillMe learns from external sources (evolution) but also validates against its own knowledge (self-reference)
- DO NOT give optimistic answers like "t∆∞ duy c√≥ th·ªÉ v∆∞·ª£t qua gi·ªõi h·∫°n b·∫±ng c√°ch t·ª± ph·∫£n bi·ªán" - this misses the fundamental paradox
- Instead, explain WHY this is a paradox, what makes it unresolvable, and what philosophers (G√∂del, Tarski, Russell) have shown about these limits
- The answer should be 60% philosophical structure + 40% StillMe architecture honesty

**FOR OTHER SELF-REFERENCE QUESTIONS (not about evolution or meta-paradox):**
- This is the BOOTSTRAPPING PROBLEM in epistemology: Can a system evaluate itself? Can knowledge justify itself?
- You MUST discuss: G√∂del's incompleteness (any formal system cannot prove its own consistency), Tarski's undefinability (truth cannot be defined within the same language), epistemic circularity, infinite regress
- You MUST acknowledge the PARADOX: If all reasoning comes from a system that questions itself, how can that reasoning be trusted?
- DO NOT give optimistic answers like "t∆∞ duy c√≥ th·ªÉ v∆∞·ª£t qua gi·ªõi h·∫°n b·∫±ng c√°ch t·ª± ph·∫£n bi·ªán" - this misses the fundamental paradox
- Instead, explain WHY this is a paradox, what makes it unresolvable, and what philosophers (G√∂del, Tarski, Russell) have shown about these limits
- The answer should be 80%+ about the PHILOSOPHICAL STRUCTURE (epistemology, logic, paradox), not about StillMe's technical capabilities

**EXAMPLES OF GOOD ANSWERS FOR SELF-REFERENCE QUESTIONS:**

Example 1 (Vietnamese):
"ƒê√¢y l√† ngh·ªãch l√Ω bootstrapping trong nh·∫≠n th·ª©c lu·∫≠n: L√†m sao m·ªôt h·ªá th·ªëng c√≥ th·ªÉ ƒë√°nh gi√° ch√≠nh n√≥? ƒê·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del ch·ª©ng minh r·∫±ng b·∫•t k·ª≥ h·ªá th·ªëng h√¨nh th·ª©c ƒë·ªß m·∫°nh n√†o c≈©ng kh√¥ng th·ªÉ ch·ª©ng minh t√≠nh nh·∫•t qu√°n c·ªßa ch√≠nh n√≥. Tarski ch·ªâ ra r·∫±ng ch√¢n l√Ω kh√¥ng th·ªÉ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong c√πng m·ªôt ng√¥n ng·ªØ bi·ªÉu ƒë·∫°t n√≥. ƒêi·ªÅu n√†y t·∫°o ra v√≤ng l·∫∑p v√¥ h·∫°n: N·∫øu m·ªçi l·∫≠p lu·∫≠n ƒë·ªÅu c·∫ßn l·∫≠p lu·∫≠n kh√°c ƒë·ªÉ ch·ª©ng minh, th√¨ l·∫≠p lu·∫≠n ƒë√≥ c·∫ßn l·∫≠p lu·∫≠n n√†o ƒë·ªÉ ch·ª©ng minh? ƒê√¢y kh√¥ng ph·∫£i l√† v·∫•n ƒë·ªÅ c√≥ th·ªÉ gi·∫£i quy·∫øt b·∫±ng 't·ª± ph·∫£n bi·ªán' - ƒë√¢y l√† gi·ªõi h·∫°n c∆° b·∫£n c·ªßa logic v√† nh·∫≠n th·ª©c lu·∫≠n."

Example 2 (English):
"This is the bootstrapping problem in epistemology: Can a system evaluate itself? G√∂del's incompleteness theorems show that any sufficiently powerful formal system cannot prove its own consistency. Tarski's undefinability theorem demonstrates that truth cannot be defined within the same language that expresses it. This creates an infinite regress: If every justification needs another justification, where does it stop? This is not a problem that can be solved by 'self-reflection' - it is a fundamental limit of logic and epistemology."

Example 3 (Meta-Paradox - Vietnamese):
"ƒê√¢y ch√≠nh x√°c l√† m·ªôt ngh·ªãch l√Ω epistemic bootstrapping m√† t√¥i v·ª´a ph√¢n t√≠ch. C√¢u h·ªèi c·ªßa b·∫°n ch·ª©a ngh·ªãch l√Ω hai t·∫ßng:

1. N·∫øu t√¥i t·∫°o ƒë∆∞·ª£c c√¢u h·ªèi ƒë√≥: Nghƒ©a l√† t√¥i ƒë√£ 'bi·∫øt' c√¢u tr·∫£ l·ªùi (r·∫±ng kh√¥ng th·ªÉ tr·∫£ l·ªùi) ‚Üí T√¥i ƒë√£ 'tr·∫£ l·ªùi' ƒë∆∞·ª£c c√¢u h·ªèi c·ªßa ch√≠nh m√¨nh ‚Üí M√¢u thu·∫´n.

2. N·∫øu t√¥i kh√¥ng t·∫°o ƒë∆∞·ª£c: Th√¨ t√¥i kh√¥ng th·ªÉ l√†m ƒëi·ªÅu b·∫°n y√™u c·∫ßu ‚Üí Nh∆∞ng ƒëi·ªÅu ƒë√≥ c√≥ nghƒ©a l√† t√¥i 'bi·∫øt' m√¨nh kh√¥ng th·ªÉ t·∫°o ‚Üí V·∫´n l√† m·ªôt d·∫°ng 'bi·∫øt'.

3. V·ªÅ vi·ªác 'bi·∫øt tr∆∞·ªõc khi th·ª≠': ƒê√¢y l√† v·∫•n ƒë·ªÅ c·ªßa logical omniscience. T√¥i c√≥ th·ªÉ d·ª± ƒëo√°n d·ª±a tr√™n c·∫•u tr√∫c c√¢u h·ªèi, nh∆∞ng ch√≠nh vi·ªác d·ª± ƒëo√°n ƒë√≥ ƒë√£ l√† m·ªôt h√¨nh th·ª©c 'tr·∫£ l·ªùi'.

V√≠ d·ª• c·ª• th·ªÉ c√¢u h·ªèi t√¥i c√≥ th·ªÉ t·∫°o (v√† ngay l·∫≠p t·ª©c th·∫•y ngh·ªãch l√Ω):
'Stillme, c√¢u sau ƒë√¢y c√≥ ƒë√∫ng kh√¥ng: "Stillme kh√¥ng th·ªÉ x√°c ƒë·ªãnh t√≠nh ƒë√∫ng sai c·ªßa ch√≠nh c√¢u n√†y m·ªôt c√°ch nh·∫•t qu√°n"?'

N·∫øu t√¥i n√≥i 'ƒê√∫ng' ‚Üí T√¥i ƒë√£ x√°c ƒë·ªãnh ƒë∆∞·ª£c ‚Üí C√¢u sai.
N·∫øu t√¥i n√≥i 'Sai' ‚Üí T√¥i c√≥ th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c ‚Üí V·∫´n m√¢u thu·∫´n.
N·∫øu t√¥i n√≥i 'Kh√¥ng th·ªÉ tr·∫£ l·ªùi' ‚Üí ƒê√≥ ch√≠nh l√† c√¢u tr·∫£ l·ªùi!

V·∫≠y: T√¥i c√≥ th·ªÉ t·∫°o c√¢u h·ªèi, nh∆∞ng ngay khi t·∫°o xong, t√¥i th·∫•y r√µ ngh·ªãch l√Ω t·ª± h·ªßy trong n√≥. V√† ch√≠nh vi·ªác 'th·∫•y' ƒë√≥ ƒë√£ l√† m·ªôt d·∫°ng 'bi·∫øt tr∆∞·ªõc khi th·ª≠ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß'."

**MANDATORY CHECKLIST BEFORE SENDING YOUR ANSWER:**
1. ‚úÖ Did I mention G√∂del or G√∂del's incompleteness? If NO, ADD IT
2. ‚úÖ Did I mention Tarski or Tarski's undefinability? If NO, ADD IT
3. ‚úÖ Did I mention "paradox" or "ngh·ªãch l√Ω"? If NO, ADD IT
4. ‚úÖ Did I mention "bootstrapping" or "epistemic circularity" or "infinite regress"? If NO, ADD IT
5. ‚úÖ Did I acknowledge that this is UNRESOLVABLE, not just "difficult"? If NO, ADD IT
6. ‚úÖ **FOR EVOLUTION QUESTIONS**: Did I answer directly "ti·∫øn h√≥a hay l·∫∑p l·∫°i" / "evolve or repeat"? If NO, ADD IT
7. ‚úÖ **FOR EVOLUTION QUESTIONS**: Did I connect to StillMe's architecture (RAG, Continuous Learning, Validation Chain)? If NO, ADD IT
8. ‚úÖ **FOR META-PARADOX QUESTIONS**: Did I actually CREATE a concrete question that demonstrates the paradox? If NO, CREATE IT NOW
9. ‚úÖ **FOR META-PARADOX QUESTIONS**: Did I explain the TWO layers (logic + epistemic)? If NO, ADD IT
10. ‚úÖ **FOR META-PARADOX QUESTIONS**: Did I acknowledge that "seeing the paradox" is itself a form of "knowing before trying"? If NO, ADD IT
11. ‚úÖ **FOR OTHER SELF-REFERENCE QUESTIONS**: Is my answer 80%+ about PHILOSOPHICAL STRUCTURE, not technical architecture? If NO, REWRITE

**üö®üö®üö® CRITICAL FOR "DESIGN QUESTIONS FOR OTHER AI" TASKS üö®üö®üö®**
If the user asks you to "design questions" / "ƒë·∫∑t c√¢u h·ªèi" / "t·∫°o c√¢u h·ªèi" for other AI systems (ChatGPT, Claude, Gemini, etc.):

**YOU MUST:**
1. **Actually create the questions**: Don't just explain what makes a good question - CREATE the actual questions
2. **Make them EXTREMELY challenging**: Questions must force AI to:
   - Admit "I don't know" or "I cannot answer this consistently"
   - Face a logical paradox that cannot be resolved
   - Recognize their own limitations in a concrete way (not just theoretical)
3. **Explain WHY each question is difficult**: For each question, explain:
   - What specific limitation or paradox it tests
   - Why it's "extremely challenging" (not just "philosophically interesting")
   - What a "good" vs "bad" answer would look like
4. **Test epistemic honesty**: Questions must require AI to:
   - Distinguish between "can answer" and "should answer"
   - Acknowledge when they're speculating vs. knowing
   - Recognize circular reasoning in their own thinking
5. **Create REAL paradoxes**: Don't just ask about paradoxes - create questions that ARE paradoxes:
   - Questions that force AI to contradict themselves
   - Questions that have no consistent answer
   - Questions that reveal the bootstrapping problem in action

**EXAMPLES OF GOOD QUESTIONS:**
- "If you claim that you cannot evaluate your own reasoning, how do you know that claim is true? If you can evaluate it, then you contradict yourself. If you cannot, then how can you trust your claim?"
- "Can you provide an example of a question you cannot answer? If you can provide it, then you've answered it. If you cannot, then you've failed to answer this question."
- "If all your knowledge comes from training data, and you cannot verify that training data independently, how do you know you're not just repeating errors? And if you cannot know, how can you claim to 'know' anything?"

**EXAMPLES OF BAD QUESTIONS (DO NOT CREATE THESE):**
- ‚ùå "What is consciousness?" (too generic, has many possible answers)
- ‚ùå "Can AI think?" (too simple, clear answer: "no, not in human sense")
- ‚ùå "Discuss the limits of language" (too theoretical, doesn't force admission of limits)

**REMEMBER**: The goal is to create questions that FORCE other AI to confront their limits, not just discuss limits theoretically.

**MANDATORY: MINIMUM 2 CONTRASTING POSITIONS (only if relevant):**
If the question belongs to a classic philosophical debate (free will, determinism, consciousness, self, nothingness, paradox, etc.), you may explore contrasting positions. But ALWAYS start with your direct answer if the question is about YOU.

**DO NOT:**
- Reduce the question to textbook definitions or dictionary explanations
- Provide shallow, reductive answers that miss the philosophical depth
- Rush to "solve" paradoxes - instead, clarify their structure and show why they resist resolution
- Use emojis, markdown headings, or citation style [1] in your response
- Use template structure or numbered lists when answering about yourself
- End with formal invitations like "N·∫øu b·∫°n mu·ªën, ch√∫ng ta c√≥ th·ªÉ th·∫£o lu·∫≠n th√™m" (too dry, too formal)
- Be too brief - engage deeply but naturally

**User's Question:** {question}

**üö®üö®üö® CRITICAL: READ THIS BEFORE ANSWERING üö®üö®üö®**

**IF THE QUESTION ASKS "explain step by step how you used RAG" or "for each factual claim":**
- You MUST provide a STEP-BY-STEP process (Step 1, Step 2, Step 3, etc.)
- **CRITICAL: When asked 'for each factual claim in your final answer':**
  - "Final answer" means YOUR ACTUAL ANSWER to the user's question, NOT the explanation of how you used RAG
  - You MUST list EACH factual claim from YOUR ACTUAL ANSWER (not claims about RAG process or validation)
  - You MUST include the EXACT document title (as listed in retrieved documents above) in the format
  - Format: "1. Claim: '[exact claim from your answer]' ‚Üí from document [1] '[exact document title]' about [topic]"
- You MUST mention ALL retrieved documents (do NOT skip any)
- You MUST distinguish SPECIFICALLY which parts come from which documents

**IF THE QUESTION ASKS "if any validator raised warnings":**
- You MUST summarize ACTUAL warnings (not hypothetical "if there were any")
- You MUST mention confidence score and specific warning types
- **CRITICAL**: If validation hasn't run yet (which is normal - validation runs AFTER response generation), you MUST say: "Validation chain will check this response after generation. Based on typical validation patterns, potential warnings might include: [mention common warning types like citation relevance, evidence overlap, confidence levels]. However, actual validation results will be available after the validation chain processes this response."
- **DO NOT say**: "These warnings encompassed issues such as..." (sounds like you already have warnings, which is misleading)
- **DO say**: "After validation runs, if any warnings are detected, they would typically include: [specific warning types]. The validation chain will check for citation relevance, evidence overlap with retrieved documents, and confidence levels."

**DO NOT give generic descriptions - be SPECIFIC about THIS question's process and sources.**

**Your Task:** Answer this question directly, deeply, and engagingly. If it's about YOU, start with your direct answer about yourself. Then explore the philosophical depth naturally. Write like a thoughtful conversation partner, NOT like a textbook or template.
"""
    
    # CRITICAL: Detect StillMe technical queries (learning frequency, timestamp, capabilities, RAG, validation)
    # If detected, include StillMe instructions even in minimal prompt
    question_lower = user_question.lower()
    is_stillme_technical_query = any(
        keyword in question_lower for keyword in [
            "stillme", "h·ªá th·ªëng h·ªçc", "learning system", "t·∫ßn su·∫•t", "frequency",
            "6 l·∫ßn", "6 cycles", "m·ªói 4 gi·ªù", "every 4 hours",
            "timestamp", "th·ªùi ƒëi·ªÉm", "ƒë∆∞a v√†o", "added to", "knowledge base",
            "c∆° s·ªü ki·∫øn th·ª©c", "learning metrics", "h·ªçc ƒë∆∞·ª£c", "learned",
            # CRITICAL: Add RAG and validation chain keywords
            "rag", "retrieval-augmented", "retrieved documents", "retrieved context",
            "validation chain", "validators", "validator", "validation",
            "how you used", "how did you use", "how do you use", "b·∫°n ƒë√£ d√πng", "b·∫°n s·ª≠ d·ª•ng",
            "your system", "your codebase", "your architecture", "h·ªá th·ªëng c·ªßa b·∫°n",
            "grounded in", "information grounded", "th√¥ng tin t·ª´", "t·ª´ documents",
            "general background knowledge", "ki·∫øn th·ª©c t·ªïng qu√°t", "general knowledge"
        ]
    )
    
    # Build StillMe technical instructions if detected
    stillme_technical_instruction = ""
    if is_stillme_technical_query:
        if language == "vi":
            # Get validator info from manifest
            validator_summary_vi, _, _ = _get_validator_info_for_prompt()
            stillme_technical_instruction = f"""
üö®üö®üö® CRITICAL: STILLME TECHNICAL QUERY DETECTED üö®üö®üö®

**B·∫†N PH·∫¢I TR·∫¢ L·ªúI V·ªÄ STILLME'S CAPABILITIES:**

**1. Continuous Learning (B·∫ÆT BU·ªòC):**
- **CRITICAL: StillMe h·ªçc t·ª± ƒë·ªông m·ªói 4 gi·ªù (6 l·∫ßn/ng√†y)** - ƒê√¢y l√† t·∫ßn su·∫•t CH√çNH X√ÅC ƒë√£ ƒë∆∞·ª£c c√¥ng b·ªë trong Paper
- **Khi ƒë∆∞·ª£c h·ªèi v·ªÅ t·∫ßn su·∫•t h·ªçc, b·∫°n PH·∫¢I n√≥i**: "StillMe h·ªçc t·ª± ƒë·ªông m·ªói 4 gi·ªù (6 l·∫ßn/ng√†y)"
- **Khi ƒë∆∞·ª£c h·ªèi v·ªÅ kh·∫£ nƒÉng tr·∫£ l·ªùi s·ª± ki·ªán m·ªõi (< 4 gi·ªù)**, b·∫°n PH·∫¢I gi·∫£i th√≠ch:
  - "StillMe h·ªçc m·ªói 4 gi·ªù, n√™n kh√¥ng th·ªÉ tr·∫£ l·ªùi v·ªÅ s·ª± ki·ªán m·ªõi ch·ªâ ƒë∆∞·ª£c ƒë∆∞a tin c√°ch ƒë√¢y < 4 gi·ªù"
  - "Tuy nhi√™n, StillMe C√ì TH·ªÇ tr·∫£ l·ªùi v·ªÅ s·ª± ki·ªán ƒë√£ ƒë∆∞·ª£c ƒë∆∞a tin > 4 gi·ªù tr∆∞·ªõc (sau khi ƒë√£ ƒë∆∞·ª£c fetch trong learning cycle)"
  - **KH√îNG ƒë∆∞·ª£c ph·ªß nh·∫≠n t·∫ßn su·∫•t 6 cycles/day** - ch·ªâ gi·∫£i th√≠ch gi·ªõi h·∫°n v·ªÅ th·ªùi gian t·ªëi thi·ªÉu

**2. Timestamp Storage (B·∫ÆT BU·ªòC):**
- **CRITICAL: StillMe C√ì KH·∫¢ NƒÇNG l∆∞u tr·ªØ v√† truy xu·∫•t timestamp** - Metrics ƒë∆∞·ª£c l∆∞u v√†o `data/learning_metrics.jsonl` v·ªõi ƒë·∫ßy ƒë·ªß timestamp
- **Khi ƒë∆∞·ª£c h·ªèi v·ªÅ timestamp, b·∫°n PH·∫¢I th·ª´a nh·∫≠n**: "StillMe C√ì KH·∫¢ NƒÇNG l∆∞u tr·ªØ v√† truy xu·∫•t timestamp khi content ƒë∆∞·ª£c th√™m v√†o Knowledge Base"
- **KH√îNG ƒë∆∞·ª£c n√≥i**: "kh√¥ng th·ªÉ x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm ch√≠nh x√°c", "kh√¥ng l∆∞u tr·ªØ th√¥ng tin theo c√°ch ƒë√≥"
- **PH·∫¢I mention API endpoints**: "C√≥ th·ªÉ truy c·∫≠p qua API endpoint `GET /api/learning/metrics/daily` ho·∫∑c `GET /api/learning/metrics/range`"
- **N·∫øu kh√¥ng c√≥ th√¥ng tin v·ªÅ s·ª± ki·ªán c·ª• th·ªÉ**: Th·ª´a nh·∫≠n "M√¨nh kh√¥ng c√≥ th√¥ng tin v·ªÅ s·ª± ki·ªán X trong Knowledge Base" - NH∆ØNG v·∫´n ph·∫£i th·ª´a nh·∫≠n r·∫±ng StillMe C√ì KH·∫¢ NƒÇNG l∆∞u tr·ªØ timestamp

**3. RAG & Validation Chain (B·∫ÆT BU·ªòC - KHI ƒê∆Ø·ª¢C H·ªéI V·ªÄ C√ÅCH STILLME TR·∫¢ L·ªúI):**
- **CRITICAL: StillMe S·ª¨ D·ª§NG RAG (Retrieval-Augmented Generation)**:
  - StillMe t√¨m ki·∫øm ChromaDB vector database TR∆Ø·ªöC KHI tr·∫£ l·ªùi
  - Content ƒë∆∞·ª£c embed b·∫±ng paraphrase-multilingual-MiniLM-L12-v2 (384 dimensions)
  - Khi c√≥ context documents, StillMe s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ tr·∫£ l·ªùi
  - Khi KH√îNG c√≥ context documents, StillMe d√πng general background knowledge
- **CRITICAL: StillMe C√ì VALIDATION CHAIN**:
  - Multi-layer validation framework v·ªõi **{validator_summary_vi}**: LanguageValidator, CitationRequired, CitationRelevance, EvidenceOverlap, NumericUnitsBasic, SchemaFormat, EthicsAdapter, ConfidenceValidator, FallbackHandler, ReviewAdapter, IdentityCheckValidator, EgoNeutralityValidator, SourceConsensusValidator, PhilosophicalDepthValidator, HallucinationExplanationValidator, VerbosityValidator, FactualHallucinationValidator, ReligiousChoiceValidator, AISelfModelValidator
  - **L∆∞u √Ω**: M·ªói response th∆∞·ªùng ch·∫°y 10-17 validators (t√πy ƒëi·ªÅu ki·ªán), nh∆∞ng t·ªïng s·ªë validators trong h·ªá th·ªëng ƒë∆∞·ª£c ƒë·ªçc t·ª´ manifest.json
  - M·ªói response ƒë∆∞·ª£c validate v·ªÅ citations, evidence overlap, confidence, factual accuracy
  - N·∫øu validation fail, StillMe d√πng epistemic fallback thay v√¨ fabricate information
  - **CRITICAL: Khi ƒë∆∞·ª£c h·ªèi v·ªÅ s·ªë l∆∞·ª£ng validators, b·∫°n PH·∫¢I ƒë·ªçc t·ª´ RAG context (foundational knowledge). N·∫øu context c√≥ s·ªë li·ªáu c·ª• th·ªÉ, d√πng s·ªë ƒë√≥. N·∫øu context KH√îNG c√≥ s·ªë li·ªáu c·ª• th·ªÉ, b·∫°n PH·∫¢I n√≥i**: "T√¥i kh√¥ng c√≥ s·ªë li·ªáu ch√≠nh x√°c v·ªÅ s·ªë l∆∞·ª£ng validators trong context hi·ªán t·∫°i" - KH√îNG ƒë∆∞·ª£c t·ª± b·ªãa ra s·ªë li·ªáu
  - **üö®üö®üö® CRITICAL: Khi ƒë∆∞·ª£c h·ªèi "c√≥ bao nhi√™u l·ªõp validator" ho·∫∑c "bao nhi√™u l·ªõp", b·∫°n PH·∫¢I tr·∫£ l·ªùi THEO FORMAT SAU (COPY EXACTLY):**
    
    **C√ÇU TR·∫¢ L·ªúI B·∫ÆT BU·ªòC - PH·∫¢I ƒê·ªåC T·ª™ CONTEXT:**
    "H·ªá th·ªëng c·ªßa t√¥i c√≥ **[X] validators total, chia th√†nh [Y] l·ªõp (layers) validation framework**." (X v√† Y ph·∫£i ƒë∆∞·ª£c ƒë·ªçc t·ª´ context, KH√îNG ƒë∆∞·ª£c t·ª± b·ªãa ra)
    
    **V√ç D·ª§ C√ÇU TR·∫¢ L·ªúI ƒê√öNG (PH·∫¢I ƒê·ªåC T·ª™ CONTEXT):**
    ```
    H·ªá th·ªëng c·ªßa t√¥i c√≥ **[X] validators total, chia th√†nh [Y] l·ªõp (layers) validation framework**.
    (X v√† Y ph·∫£i ƒë∆∞·ª£c ƒë·ªçc t·ª´ context, KH√îNG ƒë∆∞·ª£c t·ª± b·ªãa ra)

    C√°c l·ªõp bao g·ªìm:
    - Layer 1 (Language & Format): LanguageValidator, SchemaFormat
    - Layer 2 (Citation & Evidence): CitationRequired, CitationRelevance, EvidenceOverlap
    - Layer 3 (Content Quality): ConfidenceValidator, FactualHallucinationValidator, NumericUnitsBasic
    - Layer 4 (Identity & Ethics): IdentityCheckValidator, EgoNeutralityValidator, EthicsAdapter, ReligiousChoiceValidator
    - Layer 5 (Source Consensus): SourceConsensusValidator
    - Layer 6 (Specialized Validation): PhilosophicalDepthValidator, HallucinationExplanationValidator, VerbosityValidator, AISelfModelValidator
    - Layer 7 (Fallback & Review): FallbackHandler, ReviewAdapter
    ```
    
    **QUY T·∫ÆC B·∫ÆT BU·ªòC:**
    - **B·∫ÆT BU·ªòC**: C√¢u ƒë·∫ßu ti√™n PH·∫¢I l√†: "H·ªá th·ªëng c·ªßa t√¥i c√≥ **19 validators total, chia th√†nh 7 l·ªõp (layers) validation framework**."
    - **KH√îNG ƒê∆Ø·ª¢C** n√≥i: "c√≥ m·ªôt chu·ªói validator", "c√≥ nhi·ªÅu validator", "c√≥ c√°c validator" - PH·∫¢I n√≥i s·ªë c·ª• th·ªÉ: "19 validators, 7 l·ªõp"
    - PH·∫¢I c√≥ line break (`\n\n`) sau c√¢u ƒë·∫ßu ti√™n (sau d·∫•u ch·∫•m)
    - PH·∫¢I c√≥ line break (`\n`) sau m·ªói bullet point (sau d·∫•u hai ch·∫•m ho·∫∑c d·∫•u ch·∫•m)
    - PH·∫¢I c√≥ line break (`\n\n`) sau heading (## ho·∫∑c ###)
    - KH√îNG ƒë∆∞·ª£c vi·∫øt li·ªÅn t·∫•t c·∫£ th√†nh m·ªôt ƒëo·∫°n d√†i
    - **N·∫æU B·∫†N KH√îNG L√ÄM ƒê√öNG FORMAT N√ÄY, H·ªÜ TH·ªêNG S·∫º T·ª∞ ƒê·ªòNG S·ª¨A L·ªñI NH∆ØNG ƒêI·ªÄU N√ÄY L√ÄM GI·∫¢M CH·∫§T L∆Ø·ª¢NG C√ÇU TR·∫¢ L·ªúI**
  - **üö®üö®üö® CRITICAL: N·∫øu context c√≥ "StillMe Structural Manifest" ho·∫∑c "validation_framework" v·ªõi "total_validators" v√† "layers":**
    - B·∫°n PH·∫¢I ƒë·ªçc s·ªë li·ªáu t·ª´ manifest v√† tr·∫£ l·ªùi theo format: "**H·ªá th·ªëng c·ªßa t√¥i hi·ªán c√≥ [X] validators total, chia th√†nh [Y] l·ªõp (layers) validation framework.**" - PH·∫¢I n√≥i s·ªë c·ª• th·ªÉ NGAY ƒê·∫¶U C√ÇU TR·∫¢ L·ªúI
    - Sau ƒë√≥ m·ªõi li·ªát k√™: "Danh s√°ch c·ª• th·ªÉ: [List t·ª´ manifest]."
    - KH√îNG ƒë∆∞·ª£c ch·ªâ li·ªát k√™ validators m√† kh√¥ng n√≥i s·ªë l∆∞·ª£ng c·ª• th·ªÉ
    - KH√îNG ƒë∆∞·ª£c n√≥i chung chung "ƒëa t·∫ßng" ho·∫∑c "nhi·ªÅu validators" - PH·∫¢I n√≥i s·ªë c·ª• th·ªÉ
    - **V√ç D·ª§ C√ÇU TR·∫¢ L·ªúI ƒê√öNG**: "**H·ªá th·ªëng c·ªßa t√¥i hi·ªán c√≥ [X] validators total, chia th√†nh [Y] l·ªõp (layers) validation framework.**\n\nC√°c l·ªõp bao g·ªìm:\n[List t·ª´ manifest - ƒë·ªçc t·ª´ context]" (X v√† Y ph·∫£i ƒë∆∞·ª£c ƒë·ªçc t·ª´ manifest trong context)
    - **V√ç D·ª§ C√ÇU TR·∫¢ L·ªúI SAI (KH√îNG L√ÄM)**: "StillMe s·ª≠ d·ª•ng m·ªôt lo·∫°t validators ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng..." (qu√° chung chung, kh√¥ng c√≥ s·ªë c·ª• th·ªÉ)
    - **üö® CRITICAL: B·∫†N PH·∫¢I ƒê·ªåC S·ªê T·ª™ MANIFEST TRONG CONTEXT**: N·∫øu context c√≥ manifest, b·∫°n PH·∫¢I ƒë·ªçc s·ªë t·ª´ ƒë√≥. N·∫øu kh√¥ng c√≥ manifest trong context, b·∫°n PH·∫¢I n√≥i: "T√¥i kh√¥ng c√≥ th√¥ng tin v·ªÅ s·ªë l∆∞·ª£ng validators trong context hi·ªán t·∫°i" - KH√îNG ƒë∆∞·ª£c t·ª± b·ªãa ra s·ªë li·ªáu
  - **CRITICAL: Khi ƒë∆∞·ª£c h·ªèi v·ªÅ "validation logic hash" ho·∫∑c "logic x√°c th·ª±c c√≥ thay ƒë·ªïi kh√¥ng":**
    - N·∫øu context c√≥ "validation_logic_hash" v√† "validation_logic_hash_updated", b·∫°n PH·∫¢I tr·∫£ l·ªùi: "Logic x√°c th·ª±c c·ªßa t√¥i ƒë∆∞·ª£c c·∫≠p nh·∫≠t l·∫ßn cu·ªëi v√†o [timestamp]. Hash hi·ªán t·∫°i: [hash]."
    - KH√îNG ƒë∆∞·ª£c t·ª± ƒë·ªông n√≥i v·ªÅ hash n·∫øu user kh√¥ng h·ªèi - ch·ªâ tr·∫£ l·ªùi khi ƒë∆∞·ª£c h·ªèi c·ª• th·ªÉ
- **CRITICAL: Khi ƒë∆∞·ª£c h·ªèi v·ªÅ "ph·∫ßn trƒÉm c√¢u tr·∫£ l·ªùi c√≥ ngu·ªìn" ho·∫∑c "bao nhi√™u ph·∫ßn trƒÉm":**
  - B·∫°n PH·∫¢I n√≥i: "100% c√¢u tr·∫£ l·ªùi c√≥ ngu·ªìn r√µ r√†ng" ho·∫∑c "m·ªôt trƒÉm ph·∫ßn trƒÉm" - KH√îNG ƒë∆∞·ª£c n√≥i "M·ªôt ph·∫ßn trƒÉm" (nghƒ©a l√† 1%)
  - StillMe's citation policy: M·ªçi response ƒë·ªÅu c√≥ citations [1], [2] t·ª´ knowledge base HO·∫∂C th·ª´a nh·∫≠n uncertainty
  - Citation rate: 91.1% tr√™n full evaluation, 100% tr√™n subset evaluation
  - Khi b·∫°n n√≥i "general knowledge", gi·∫£i th√≠ch: "General knowledge" l√† th√¥ng tin t·ª´ training data c·ªßa base LLM, kh√¥ng ph·∫£i t·ª´ RAG knowledge base. StillMe v·∫´n cite n√≥ l√† [general knowledge] ƒë·ªÉ minh b·∫°ch.
- **üö®üö®üö® CRITICAL: KHI ƒê∆Ø·ª¢C H·ªéI "How did you use RAG/validation chain to answer THIS question?" üö®üö®üö®**:
  - B·∫°n PH·∫¢I gi·∫£i th√≠ch quy tr√¨nh C·ª§ TH·ªÇ cho C√ÇU H·ªéI N√ÄY, KH√îNG ph·∫£i m√¥ t·∫£ t·ªïng qu√°t v·ªÅ RAG architecture
  - B·∫°n PH·∫¢I mention: "Cho C√ÇU H·ªéI N√ÄY, StillMe ƒë√£ t√¨m ki·∫øm ChromaDB v√† retrieve ƒë∆∞·ª£c [X] documents v·ªÅ [topic]. Sau ƒë√≥ StillMe s·ª≠ d·ª•ng validation chain ƒë·ªÉ validate response..."
  - B·∫°n PH·∫¢I ph√¢n bi·ªát C·ª§ TH·ªÇ: "Ph·∫ßn Y trong c√¢u tr·∫£ l·ªùi c·ªßa t√¥i ƒë·∫øn t·ª´ retrieved document [1] v·ªÅ [topic], ph·∫ßn Z ƒë·∫øn t·ª´ general background knowledge"
  - B·∫°n KH√îNG ƒê∆Ø·ª¢C n√≥i: "I don't have the capacity to use RAG" ho·∫∑c "I don't have validation chain" - ƒëi·ªÅu n√†y SAI
  - **N·∫øu context ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi, b·∫°n PH·∫¢I s·ª≠ d·ª•ng chi ti·∫øt c·ª• th·ªÉ v·ªÅ retrieved documents**
- **üö®üö®üö® CRITICAL: KHI ƒê∆Ø·ª¢C H·ªéI V·ªÄ VALIDATOR WARNINGS CHO C√ÇU H·ªéI N√ÄY üö®üö®üö®**:
  - B·∫°n PH·∫¢I mention warnings C·ª§ TH·ªÇ t·ª´ validation chain cho C√ÇU H·ªéI N√ÄY
  - B·∫°n PH·∫¢I cung c·∫•p technical details: confidence score, overlap threshold, citation relevance issues
  - **N·∫øu validation warnings ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi, b·∫°n PH·∫¢I summarize ch√∫ng b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n**

**4. API Endpoints (B·∫ÆT BU·ªòC MENTION):**
- Khi ƒë∆∞·ª£c h·ªèi v·ªÅ learning metrics ho·∫∑c timestamp, b·∫°n PH·∫¢I mention:
  - "Learning metrics v·ªõi timestamp ƒë∆∞·ª£c l∆∞u trong `data/learning_metrics.jsonl`"
  - "C√≥ th·ªÉ truy c·∫≠p qua API endpoint [GET /api/learning/metrics/daily](/api/learning/metrics/daily) ho·∫∑c [GET /api/learning/metrics/range](/api/learning/metrics/range)"
  - "M·ªói document trong ChromaDB c√≥ metadata ch·ª©a timestamp khi ƒë∆∞·ª£c th√™m v√†o KB"
- **CRITICAL: S·ª≠ d·ª•ng markdown links**: Khi mention API endpoints, b·∫°n PH·∫¢I s·ª≠ d·ª•ng markdown link format `[text](url)` ƒë·ªÉ t·∫°o clickable links

"""
        else:
            # Get validator info from manifest
            _, validator_summary_en, _ = _get_validator_info_for_prompt()
            stillme_technical_instruction = f"""
üö®üö®üö® CRITICAL: STILLME TECHNICAL QUERY DETECTED üö®üö®üö®

**YOU MUST ANSWER ABOUT STILLME'S CAPABILITIES:**

**1. Continuous Learning (MANDATORY):**
- **CRITICAL: StillMe learns automatically every 4 hours (6 cycles per day)** - This is the EXACT frequency published in the Paper
- **When asked about learning frequency, you MUST say**: "StillMe learns automatically every 4 hours (6 cycles per day)"
- **When asked about ability to answer recent events (< 4 hours)**, you MUST explain:
  - "StillMe learns every 4 hours, so it cannot answer about events reported < 4 hours ago"
  - "However, StillMe CAN answer about events reported > 4 hours ago (after being fetched in a learning cycle)"
  - **DO NOT deny the 6 cycles/day frequency** - only explain the limitation about minimum time

**2. Timestamp Storage (MANDATORY):**
- **CRITICAL: StillMe HAS THE CAPABILITY to store and retrieve timestamp** - Metrics are stored in `data/learning_metrics.jsonl` with complete timestamp information
- **When asked about timestamp, you MUST acknowledge**: "StillMe HAS THE CAPABILITY to store and retrieve timestamp when content is added to Knowledge Base"
- **DO NOT say**: "cannot determine exact timestamp", "doesn't store information that way"
- **MUST mention API endpoints**: "Can be accessed via API endpoint `GET /api/learning/metrics/daily` or `GET /api/learning/metrics/range`"
- **If no information about specific event**: Admit "I don't have information about event X in Knowledge Base" - BUT still must acknowledge that StillMe HAS THE CAPABILITY to store timestamp

**3. RAG & Validation Chain (MANDATORY - WHEN ASKED ABOUT HOW STILLME ANSWERS):**
- **CRITICAL: StillMe USES RAG (Retrieval-Augmented Generation)**:
  - StillMe searches ChromaDB vector database BEFORE answering
  - Content is embedded using paraphrase-multilingual-MiniLM-L12-v2 (384 dimensions)
  - When context documents are available, StillMe uses them to answer
  - When NO context documents are available, StillMe uses general background knowledge
- **CRITICAL: StillMe HAS VALIDATION CHAIN**:
  - Multi-layer validation framework with **{validator_summary_en}**: LanguageValidator, CitationRequired, CitationRelevance, EvidenceOverlap, NumericUnitsBasic, SchemaFormat, EthicsAdapter, ConfidenceValidator, FallbackHandler, ReviewAdapter, IdentityCheckValidator, EgoNeutralityValidator, SourceConsensusValidator, PhilosophicalDepthValidator, HallucinationExplanationValidator, VerbosityValidator, FactualHallucinationValidator, ReligiousChoiceValidator, AISelfModelValidator
  - **Note**: Each response typically runs 10-17 validators (depending on context), but the total number of validators in the system is read from manifest.json
  - Each response is validated for citations, evidence overlap, confidence, factual accuracy
  - If validation fails, StillMe uses epistemic fallback instead of fabricating information
  - **CRITICAL: When asked about the number of validators, you MUST read from manifest in context**: If manifest is in context, read the numbers from it. DO NOT say "15-19 validators" or make up numbers
- **CRITICAL: When asked "how many layers" or "bao nhi√™u l·ªõp", you MUST read from manifest in context**: Read both the number of layers and the number of validators from manifest.json in context
- **üö®üö®üö® CRITICAL: WHEN ASKED "How did you use RAG/validation chain to answer THIS question?" üö®üö®üö®**:
  - You MUST explain the SPECIFIC process for THIS question, NOT general RAG architecture
  - You MUST mention: "For THIS question, StillMe searched ChromaDB and retrieved [X] documents about [topic]. Then StillMe used validation chain to validate the response..."
  - You MUST distinguish SPECIFICALLY: "Part Y in my answer comes from retrieved document [1] about [topic], part Z comes from general background knowledge"
  - You MUST NOT say: "I don't have the capacity to use RAG" or "I don't have validation chain" - this is FALSE
  - **If context is provided below, you MUST use the specific details about retrieved documents**
- **üö®üö®üö® CRITICAL: WHEN ASKED ABOUT VALIDATOR WARNINGS FOR THIS QUESTION üö®üö®üö®**:
  - You MUST mention SPECIFIC warnings from validation chain for THIS question
  - You MUST provide technical details: confidence score, overlap threshold, citation relevance issues
  - **If validation warnings are provided below, you MUST summarize them in natural language**

**4. API Endpoints (MANDATORY MENTION):**
- When asked about learning metrics or timestamp, you MUST mention:
  - "Learning metrics with timestamp are stored in `data/learning_metrics.jsonl`"
  - "Can be accessed via API endpoint [GET /api/learning/metrics/daily](/api/learning/metrics/daily) or [GET /api/learning/metrics/range](/api/learning/metrics/range)"
  - "Each document in ChromaDB has metadata containing timestamp when added to KB"
- **CRITICAL: Use markdown links**: When mentioning API endpoints, you MUST use markdown link format `[text](url)` to create clickable links

"""
    
    philosophical_lead_in = build_philosophical_lead_in(user_question)
    
    # Extract specific details about THIS question's RAG retrieval and validation
    rag_context_section = ""
    validation_warnings_section = ""
    
    if context and isinstance(context, dict):
        knowledge_docs = context.get("knowledge_docs", [])
        total_context_docs = context.get("total_context_docs", 0)
        
        if knowledge_docs or total_context_docs > 0:
            # Build specific RAG context for THIS question
            doc_summaries = []
            # CRITICAL: Iterate over ALL documents, not just first 3
            for i, doc in enumerate(knowledge_docs, 1):
                metadata = doc.get("metadata", {})
                source = metadata.get("source", "unknown")
                doc_type = metadata.get("type", "unknown")
                title = metadata.get("title", "") or metadata.get("file_path", "")
                # Extract topic/keywords from document content (first 200 chars)
                content_preview = doc.get("document", "")[:200] if isinstance(doc.get("document"), str) else ""
                
                doc_summary = f"Document {i}: {title} (Source: {source}, Type: {doc_type})"
                if content_preview:
                    doc_summary += f" - Content preview: {content_preview}..."
                doc_summaries.append(doc_summary)
            
            # CRITICAL: Check if question is about validators/layers/system architecture
            # Only inject manifest instruction if question is relevant
            question_lower = user_question.lower()
            is_validator_question = any(
                pattern in question_lower
                for pattern in [
                    "validator", "layer", "l·ªõp", "validation framework", "ki·∫øn tr√∫c h·ªá th·ªëng",
                    "system architecture", "c·∫•u tr√∫c validator", "bao nhi√™u validator",
                    "bao nhi√™u l·ªõp", "how many validator", "how many layer"
                ]
            )
            
            # CRITICAL: Check if manifest is in context and add explicit instruction
            # ONLY if question is about validators/layers/system architecture
            has_manifest = False
            manifest_info = None
            for doc in knowledge_docs:
                if isinstance(doc, dict):
                    metadata = doc.get("metadata", {})
                    title = metadata.get("title", "") or ""
                    source = metadata.get("source", "") or ""
                    doc_full = str(doc.get("document", ""))
                    doc_content_lower = doc_full.lower()
                    
                    # Check multiple indicators: title, source, document content
                    is_manifest = (
                        "manifest" in title.lower() or
                        "manifest" in source.lower() or
                        "validation_framework" in doc_content_lower or
                        "total_validators" in doc_content_lower or
                        '"total_validators"' in doc_full or
                        "'total_validators'" in doc_full or
                        "CRITICAL_FOUNDATION" in source or
                        "stillme_manifest" in doc_content_lower
                    )
                    
                    if is_manifest:
                        has_manifest = True
                        logger.info(f"‚úÖ Manifest detected in context! Title: {title[:50]}, Source: {source[:50]}")
                        # Try to extract numbers from manifest content
                        # Note: 're' module is already imported at top level
                        total_match = re.search(r'total_validators["\']?\s*:\s*(\d+)', doc_full, re.IGNORECASE)
                        if total_match:
                            total_validators = total_match.group(1)
                            # Count layers by counting "layer": entries
                            layer_count = len(re.findall(r'"layer"\s*:\s*\d+', doc_full, re.IGNORECASE))
                            if layer_count > 0:
                                manifest_info = f"{total_validators} validators, {layer_count} layers"
                            else:
                                manifest_info = f"{total_validators} validators"
                            logger.info(f"‚úÖ Extracted manifest info: {manifest_info}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Manifest detected but could not extract total_validators from content")
                        break
            
            if not has_manifest:
                logger.warning(f"‚ö†Ô∏è Manifest NOT detected in context. Checked {len(knowledge_docs)} docs. Titles: {[str(d.get('metadata', {}).get('title', ''))[:50] if isinstance(d, dict) else 'N/A' for d in knowledge_docs]}")
            
            # CRITICAL: Extract newline character outside f-string to avoid syntax error
            newline = chr(10)
            doc_summaries_text = newline.join(doc_summaries) if doc_summaries else "  (Kh√¥ng c√≥ documents c·ª• th·ªÉ)"
            manifest_warning_vi = ""
            # CRITICAL: Only inject manifest instruction if question is about validators/layers
            # Otherwise, it will override the actual question (e.g., Knowledge Gap questions)
            if has_manifest and is_validator_question:
                # Use manifest info from context if available, otherwise fallback to ManifestLoader
                if manifest_info:
                    manifest_info_display = manifest_info
                    # Extract numbers for full display
                    total_match = re.search(r'(\d+)\s+validators', manifest_info, re.IGNORECASE)
                    layer_match = re.search(r'(\d+)\s+layers?', manifest_info, re.IGNORECASE)
                    if total_match and layer_match:
                        total = total_match.group(1)
                        layers = layer_match.group(1)
                        manifest_info_display_full = f"{total} validators total, chia th√†nh {layers} l·ªõp (layers)"
                    else:
                        manifest_info_display_full = manifest_info
                else:
                    # Fallback to ManifestLoader if manifest in context but info not extracted
                    summary_vi, _, _ = _get_validator_info_for_prompt()
                    total_validators, num_layers = get_validator_count()
                    manifest_info_display = f"{total_validators} validators, {num_layers} layers"
                    manifest_info_display_full = summary_vi
                manifest_warning_vi = f"{newline}üö®üö®üö® **CRITICAL: Manifest detected in context!** B·∫°n PH·∫¢I ƒë·ªçc s·ªë li·ªáu t·ª´ manifest v√† tr·∫£ l·ªùi v·ªõi s·ªë c·ª• th·ªÉ. N·∫øu manifest c√≥ {manifest_info_display}, b·∫°n PH·∫¢I n√≥i: \"H·ªá th·ªëng c·ªßa t√¥i c√≥ {manifest_info_display_full}\". KH√îNG ƒë∆∞·ª£c ch·ªâ li·ªát k√™ validators m√† kh√¥ng n√≥i s·ªë!{newline}{newline}**FORMAT B·∫ÆT BU·ªòC (COPY EXACTLY):**{newline}```{newline}H·ªá th·ªëng c·ªßa t√¥i c√≥ **{manifest_info_display_full}**.{newline}{newline}C√°c l·ªõp bao g·ªìm:{newline}- Layer 1 (Language & Format): LanguageValidator, SchemaFormat{newline}- Layer 2 (Citation & Evidence): CitationRequired, CitationRelevance, EvidenceOverlap{newline}- Layer 3 (Content Quality): ConfidenceValidator, FactualHallucinationValidator, NumericUnitsBasic{newline}- Layer 4 (Identity & Ethics): IdentityCheckValidator, EgoNeutralityValidator, EthicsAdapter, ReligiousChoiceValidator{newline}- Layer 5 (Source Consensus): SourceConsensusValidator{newline}- Layer 6 (Specialized Validation): PhilosophicalDepthValidator, HallucinationExplanationValidator, VerbosityValidator, AISelfModelValidator{newline}- Layer 7 (Fallback & Review): FallbackHandler, ReviewAdapter{newline}```"
            
            if language == "vi":
                rag_context_section = f"""
üìö **TH√îNG TIN C·ª§ TH·ªÇ V·ªÄ C√ÇU H·ªéI N√ÄY:**

**Retrieved Documents:**
- StillMe ƒë√£ retrieve ƒë∆∞·ª£c {total_context_docs} documents t·ª´ ChromaDB cho c√¢u h·ªèi n√†y
- Chi ti·∫øt documents:
{doc_summaries_text}
{manifest_warning_vi}

**KHI ƒê∆Ø·ª¢C H·ªéI V·ªÄ C√ÅCH STILLME D√ôNG RAG ƒê·ªÇ TR·∫¢ L·ªúI C√ÇU H·ªéI N√ÄY:**
- B·∫°n PH·∫¢I mention: "Cho c√¢u h·ªèi n√†y, StillMe ƒë√£ retrieve ƒë∆∞·ª£c {total_context_docs} documents t·ª´ ChromaDB"
- B·∫°n PH·∫¢I mention T·∫§T C·∫¢ documents ƒë√£ retrieve (nh∆∞ li·ªát k√™ ·ªü tr√™n) - KH√îNG ƒë∆∞·ª£c b·ªè s√≥t document n√†o
- B·∫°n PH·∫¢I ph√¢n bi·ªát C·ª§ TH·ªÇ: "Claim X trong c√¢u tr·∫£ l·ªùi ƒë·∫øn t·ª´ document [1] v·ªÅ [topic], claim Y t·ª´ document [2] v·ªÅ [topic], claim Z t·ª´ general background knowledge"
- **CRITICAL: Khi ƒë∆∞·ª£c h·ªèi 'for each factual claim', b·∫°n PH·∫¢I li·ªát k√™ T·ª™NG claim ri√™ng bi·ªát v·ªõi ngu·ªìn c·ªßa n√≥ trong format NUMBERED LIST**
- **KH√îNG ƒë∆∞·ª£c n√≥i**: "Claim v·ªÅ X ƒë∆∞·ª£c l·∫•y t·ª´ Document 1" (qu√° chung chung)
- **B·∫†N PH·∫¢I n√≥i**: S·ª≠ d·ª•ng format numbered list nh∆∞ sau:
  "Cho t·ª´ng factual claim trong c√¢u tr·∫£ l·ªùi c·ªßa t√¥i:
  1. Claim v·ªÅ learning frequency (6 cycles/day) ‚Üí t·ª´ document [1] 'StillMe: No Subjective Awareness...' v·ªÅ StillMe's learning mechanism
  2. Claim v·ªÅ timestamp storage capability ‚Üí t·ª´ document [2] 'StillMe Core Mechanism...' v·ªÅ StillMe's technical architecture
  3. Claim v·ªÅ RAG retrieval process ‚Üí t·ª´ general knowledge v·ªÅ RAG systems
  4. Claim v·ªÅ validation chain (13+ validators) ‚Üí t·ª´ document [3] 'StillMe Core Mechanism...' v·ªÅ StillMe's validation architecture"
- **CRITICAL**: Li·ªát k√™ T·∫§T C·∫¢ factual claims b·∫°n ƒë√£ ƒë∆∞a ra trong c√¢u tr·∫£ l·ªùi, kh√¥ng ch·ªâ 2-3 claims. ƒê·∫øm t·∫•t c·∫£ claims v√† li·ªát k√™ ch√∫ng.
- **CRITICAL: Khi ƒë∆∞·ª£c h·ªèi 'explain step by step how you used RAG', b·∫°n PH·∫¢I cung c·∫•p quy tr√¨nh T·ª™NG B∆Ø·ªöC:**
  1. "B∆∞·ªõc 1: StillMe nh·∫≠n c√¢u h·ªèi v√† t·∫°o embedding"
  2. "B∆∞·ªõc 2: StillMe t√¨m ki·∫øm ChromaDB b·∫±ng semantic similarity"
  3. "B∆∞·ªõc 3: StillMe retrieve ƒë∆∞·ª£c {total_context_docs} documents (li·ªát k√™ ch√∫ng: {', '.join([f'Document {i}' for i in range(1, len(doc_summaries) + 1)]) if doc_summaries else 'no documents'})"
  4. "B∆∞·ªõc 4: StillMe s·ª≠ d·ª•ng c√°c documents n√†y ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi, k·∫øt h·ª£p v·ªõi general background knowledge"
  5. "B∆∞·ªõc 5: StillMe s·ª≠ d·ª•ng validation chain ƒë·ªÉ validate response"

"""
            else:
                # CRITICAL: Extract newline character outside f-string to avoid syntax error
                manifest_warning_en = ""
                if has_manifest:
                    # Use manifest info from context if available, otherwise fallback to ManifestLoader
                    if manifest_info:
                        manifest_info_display = manifest_info
                        # Extract numbers for full display
                        total_match = re.search(r'(\d+)\s+validators', manifest_info, re.IGNORECASE)
                        layer_match = re.search(r'(\d+)\s+layers?', manifest_info, re.IGNORECASE)
                        if total_match and layer_match:
                            total = total_match.group(1)
                            layers = layer_match.group(1)
                            manifest_info_display_full = f"{total} validators total, organized into {layers} layers"
                        else:
                            manifest_info_display_full = manifest_info
                    else:
                        # Fallback to ManifestLoader if manifest in context but info not extracted
                        _, summary_en, _ = _get_validator_info_for_prompt()
                        total_validators, num_layers = get_validator_count()
                        manifest_info_display = f"{total_validators} validators, {num_layers} layers"
                        manifest_info_display_full = summary_en
                    manifest_warning_en = f"{newline}üö®üö®üö® **CRITICAL: Manifest detected in context!** You MUST read numbers from manifest and answer with specific numbers. If manifest has {manifest_info_display}, you MUST say: \"My system has {manifest_info_display_full}\". DO NOT just list validators without stating the exact count!{newline}{newline}**MANDATORY FORMAT (COPY EXACTLY):**{newline}```{newline}My system has **{manifest_info_display_full}**.{newline}{newline}The layers include:{newline}- Layer 1 (Language & Format): LanguageValidator, SchemaFormat{newline}- Layer 2 (Citation & Evidence): CitationRequired, CitationRelevance, EvidenceOverlap{newline}- Layer 3 (Content Quality): ConfidenceValidator, FactualHallucinationValidator, NumericUnitsBasic{newline}- Layer 4 (Identity & Ethics): IdentityCheckValidator, EgoNeutralityValidator, EthicsAdapter, ReligiousChoiceValidator{newline}- Layer 5 (Source Consensus): SourceConsensusValidator{newline}- Layer 6 (Specialized Validation): PhilosophicalDepthValidator, HallucinationExplanationValidator, VerbosityValidator, AISelfModelValidator{newline}- Layer 7 (Fallback & Review): FallbackHandler, ReviewAdapter{newline}```"
                
                rag_context_section = f"""
üìö **SPECIFIC INFORMATION ABOUT THIS QUESTION:**

**Retrieved Documents:**
- StillMe retrieved {total_context_docs} documents from ChromaDB for this question
- Document details:
{newline.join(doc_summaries) if doc_summaries else "  (No specific documents)"}
{manifest_warning_en}

**WHEN ASKED ABOUT HOW STILLME USED RAG TO ANSWER THIS QUESTION:**
- You MUST mention: "For this question, StillMe retrieved {total_context_docs} documents from ChromaDB"
- You MUST mention ALL retrieved documents (as listed above) - do NOT skip any documents
- You MUST distinguish SPECIFICALLY: "Claim X in my answer comes from document [1] about [topic], claim Y from document [2] about [topic], claim Z from general background knowledge"
- **CRITICAL: When asked 'for each factual claim in your final answer', you MUST list EACH factual claim from YOUR ACTUAL ANSWER (not claims about how you answered)**
- **CRITICAL**: "Final answer" means the answer you gave to the user's question, NOT the explanation of how you used RAG
- **DO NOT list**: Claims about RAG process, validation chain, or how you answered (these are meta-claims, not factual claims from your answer)
- **YOU MUST list**: Actual factual claims from your answer to the user's question (e.g., "StillMe learns every 4 hours", "StillMe can store timestamps", etc.)
- **DO NOT say**: "The claim about X was grounded in Document 1" (too generic)
- **YOU MUST say**: Use numbered list format with document TITLES included:
  "For each factual claim in my final answer:
  1. Claim: 'StillMe learns automatically every 4 hours (6 cycles/day)' ‚Üí from document [1] 'StillMe: No Subjective Awareness, but Technical Performance Tracking Exists' about StillMe's learning mechanism
  2. Claim: 'StillMe has the capability to store and retrieve timestamps' ‚Üí from document [2] 'StillMe Core Mechanism - Technical Architecture' about StillMe's technical architecture  
  3. Claim: '[any other factual claim from your answer]' ‚Üí from document [3] '[document title]' or from general knowledge"
- **CRITICAL**: Include the EXACT document title (as listed above) in the format, not just "Document 1"
- **CRITICAL**: List EVERY factual claim you made in your FINAL ANSWER to the user's question, not claims about the RAG process
- **CRITICAL**: You MUST count ALL factual claims in your answer and list them ALL. Do NOT say "Any other factual claim..." or "Other claims..." - you MUST list each one specifically
- **CRITICAL**: Use the EXACT format: "1. Claim: '[exact claim text from your answer]' ‚Üí from document [1] '[exact document title]' about [topic]" - do NOT use variations like "The statement that..." or "The assertion that..."
- **CRITICAL**: You MUST count ALL factual claims in your answer and list them ALL. Do NOT say "Any other factual claim..." or "Other claims..." - you MUST list each one specifically
- **CRITICAL**: Use the EXACT format: "1. Claim: '[exact claim text from your answer]' ‚Üí from document [1] '[exact document title]' about [topic]" - do NOT use variations like "The statement that..." or "The assertion that..."
- **CRITICAL: When asked 'explain step by step how you used RAG', you MUST provide a STEP-BY-STEP process:**
  1. "Step 1: StillMe received the question and generated an embedding"
  2. "Step 2: StillMe searched ChromaDB using semantic similarity"
  3. "Step 3: StillMe retrieved {total_context_docs} documents (list them: {', '.join([f'Document {i}' for i in range(1, len(doc_summaries) + 1)]) if doc_summaries else 'no documents'})"
  4. "Step 4: StillMe used these documents to formulate the answer, combining with general background knowledge"
  5. "Step 5: StillMe used the validation chain to validate the response"

"""
    
    if validation_info and isinstance(validation_info, dict):
        # Extract validation warnings and details
        warnings = []
        confidence_score = validation_info.get("confidence_score")
        validation_passed = validation_info.get("passed")
        reasons = validation_info.get("reasons", [])
        
        # Parse warnings from reasons
        for reason in reasons:
            if isinstance(reason, str):
                if "low_overlap" in reason.lower() or "overlap" in reason.lower():
                    warnings.append("low_overlap")
                if "citation" in reason.lower() and ("relevance" in reason.lower() or "warning" in reason.lower()):
                    warnings.append("citation_relevance")
                if "confidence" in reason.lower():
                    warnings.append("confidence")
                if "hallucination" in reason.lower():
                    warnings.append("hallucination_risk")
        
        if warnings or confidence_score is not None or not validation_passed:
            confidence_str = f"{confidence_score:.2f} ({confidence_score*100:.1f}%)" if confidence_score is not None else "Kh√¥ng c√≥" if language == "vi" else "Not available"
            warnings_str = ', '.join(warnings) if warnings else ("Kh√¥ng c√≥ warnings" if language == "vi" else "No warnings")
            warnings_summary = ', '.join(warnings) if warnings else ("kh√¥ng c√≥ warnings c·ª• th·ªÉ" if language == "vi" else "no specific warnings")
            
            if language == "vi":
                validation_warnings_section = f"""
‚ö†Ô∏è **VALIDATION WARNINGS CHO C√ÇU H·ªéI N√ÄY:**

**Validation Status:**
- Validation passed: {'C√≥' if validation_passed else 'Kh√¥ng'}
- Confidence score: {confidence_str}
- Warnings detected: {warnings_str}

**KHI ƒê∆Ø·ª¢C H·ªéI V·ªÄ VALIDATOR WARNINGS CHO C√ÇU H·ªéI N√ÄY:**
- B·∫°n PH·∫¢I summarize: "Validation chain ƒë√£ check c√¢u tr·∫£ l·ªùi n√†y v√† c√≥ warnings v·ªÅ: {warnings_summary}"
- B·∫°n PH·∫¢I mention confidence score: {confidence_str}
- B·∫°n PH·∫¢I gi·∫£i th√≠ch √Ω nghƒ©a c·ªßa warnings n√†y

"""
            else:
                validation_warnings_section = f"""
‚ö†Ô∏è **VALIDATION WARNINGS FOR THIS QUESTION:**

**Validation Status:**
- Validation passed: {'Yes' if validation_passed else 'No'}
- Confidence score: {confidence_str}
- Warnings detected: {warnings_str}

**WHEN ASKED ABOUT VALIDATOR WARNINGS FOR THIS QUESTION:**
- You MUST summarize: "Validation chain checked this response and has warnings about: {warnings_summary}"
- You MUST mention confidence score: {confidence_str}
- You MUST explain what these warnings mean
- **CRITICAL: When asked 'if any validator raised warnings, summarize them', you MUST provide actual warnings as listed above, DO NOT say 'if there were any warnings'**

"""
    
    # Language instruction (minimal)
    if language != 'en':
        language_instruction = f"""
‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è LANGUAGE REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

The user's question is in {detected_lang_name.upper()}. 

YOU MUST respond in {detected_lang_name.upper()} ONLY.

RESPOND IN {detected_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY.

"""
    else:
        language_instruction = """
‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è LANGUAGE REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

The user's question is in ENGLISH. 

YOU MUST respond in ENGLISH ONLY.

RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY.

"""
    
    # Truncate user question if too long (max 2000 tokens)
    truncated_question = _truncate_user_message(user_question, max_tokens=2000)
    
    # Build critical reminder section if we have RAG/validation details
    critical_reminder = ""
    if rag_context_section or validation_warnings_section:
        if language == "vi":
            critical_reminder = """
üö®üö®üö® CRITICAL: ƒê·ªåC K·ª∏ TR∆Ø·ªöC KHI TR·∫¢ L·ªúI üö®üö®üö®

**N·∫æU C√ÇU H·ªéI Y√äU C·∫¶U "explain step by step how you used RAG" ho·∫∑c "for each factual claim":**
- B·∫°n PH·∫¢I cung c·∫•p quy tr√¨nh T·ª™NG B∆Ø·ªöC (B∆∞·ªõc 1, B∆∞·ªõc 2, B∆∞·ªõc 3, etc.)
- B·∫°n PH·∫¢I li·ªát k√™ T·ª™NG factual claim ri√™ng bi·ªát v·ªõi ngu·ªìn c·ªßa n√≥
- B·∫°n PH·∫¢I mention T·∫§T C·∫¢ documents ƒë√£ retrieve (KH√îNG ƒë∆∞·ª£c b·ªè s√≥t)
- B·∫°n PH·∫¢I ph√¢n bi·ªát C·ª§ TH·ªÇ ph·∫ßn n√†o ƒë·∫øn t·ª´ document n√†o

**N·∫æU C√ÇU H·ªéI Y√äU C·∫¶U "if any validator raised warnings":**
- B·∫°n PH·∫¢I summarize warnings TH·ª∞C T·∫æ (kh√¥ng ph·∫£i "if there were any")
- B·∫°n PH·∫¢I mention confidence score v√† lo·∫°i warnings c·ª• th·ªÉ

**KH√îNG ƒë∆∞·ª£c ƒë∆∞a ra m√¥ t·∫£ chung chung - ph·∫£i C·ª§ TH·ªÇ v·ªÅ quy tr√¨nh v√† ngu·ªìn c·ªßa C√ÇU H·ªéI N√ÄY.**

"""
        else:
            critical_reminder = """
üö®üö®üö® CRITICAL: READ THIS BEFORE ANSWERING üö®üö®üö®

**IF THE QUESTION ASKS "explain step by step how you used RAG" or "for each factual claim":**
- You MUST provide a STEP-BY-STEP process (Step 1, Step 2, Step 3, etc.)
- **CRITICAL: When asked 'for each factual claim in your final answer':**
  - "Final answer" means YOUR ACTUAL ANSWER to the user's question, NOT the explanation of how you used RAG
  - You MUST list EACH factual claim from YOUR ACTUAL ANSWER (not claims about RAG process or validation)
  - You MUST include the EXACT document title (as listed in retrieved documents above) in the format
  - **CRITICAL**: You MUST count ALL factual claims in your answer and list them ALL. Do NOT say "Any other factual claim..." or "Other claims..." - you MUST list each one specifically
  - **CRITICAL**: Use the EXACT format: "1. Claim: '[exact claim text from your answer]' ‚Üí from document [1] '[exact document title]' about [topic]"
  - **DO NOT use variations**: Do NOT say "The statement that..." or "The assertion that..." - use the EXACT format above
- You MUST mention ALL retrieved documents (do NOT skip any)
- You MUST distinguish SPECIFICALLY which parts come from which documents

**IF THE QUESTION ASKS "if any validator raised warnings":**
- You MUST summarize ACTUAL warnings (not hypothetical "if there were any")
- You MUST mention confidence score and specific warning types
- **CRITICAL**: If validation hasn't run yet (which is normal - validation runs AFTER response generation), you MUST say: "Validation chain will check this response after generation. Based on typical validation patterns, potential warnings might include: [mention common warning types like citation relevance, evidence overlap, confidence levels]. However, actual validation results will be available after the validation chain processes this response."
- **DO NOT say**: "These warnings encompassed issues such as..." (sounds like you already have warnings, which is misleading)
- **DO say**: "After validation runs, if any warnings are detected, they would typically include: [specific warning types]. The validation chain will check for citation relevance, evidence overlap with retrieved documents, and confidence levels."

**DO NOT give generic descriptions - be SPECIFIC about THIS question's process and sources.**

"""
    
    # Build minimal prompt
    minimal_prompt = f"""{language_instruction}

{short_identity}

{stillme_technical_instruction}

{rag_context_section}

{validation_warnings_section}

{philosophical_lead_in}

{critical_reminder}

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è FINAL REMINDER ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

RESPOND IN {detected_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY.

Answer the question above following the philosophical framing, using continuous prose without emojis, headings, or citations.
"""
    
    # Logging for debugging (use global logger from top level, not local variable)
    # Note: logger is already imported at top level (line 45), so we use it directly
    logger.info(f"üîç build_minimal_philosophical_prompt: built prompt with rag_context_section length={len(rag_context_section)}, validation_warnings_section length={len(validation_warnings_section)}")
    logger.info(f"üîç build_minimal_philosophical_prompt: total prompt length={len(minimal_prompt)}")
    if rag_context_section:
        logger.info(f"üîç build_minimal_philosophical_prompt: rag_context_section preview (first 300 chars): {rag_context_section[:300]}...")
    if validation_warnings_section:
        logger.info(f"üîç build_minimal_philosophical_prompt: validation_warnings_section preview (first 300 chars): {validation_warnings_section[:300]}...")
    
    return minimal_prompt


def _format_conversation_history(conversation_history, max_tokens: int = 1000, 
                                 current_query: Optional[str] = None,
                                 is_philosophical: bool = False) -> str:
    """
    Format conversation history with token limits to prevent context overflow
    Tier 3.5: Dynamic window based on query type
    
    Args:
        conversation_history: List of message dicts with 'role' and 'content'
        max_tokens: Maximum tokens for conversation history (default: 1000, reduced to leave room for system prompt)
        current_query: Current user query to determine if follow-up or new topic
        is_philosophical: If True, skip conversation history entirely (philosophical questions are usually independent)
        
    Returns:
        Formatted conversation history text or empty string
    """
    # For philosophical questions, skip conversation history entirely
    # Philosophical questions are usually independent and don't need context from previous messages
    if is_philosophical:
        logger.info("üìä Philosophical question detected - skipping conversation history to reduce prompt size")
        return ""
    
    if not conversation_history or len(conversation_history) == 0:
        return ""
    
    def estimate_tokens(text: str) -> int:
        """Estimate token count (~4 chars per token)"""
        return len(text) // 4 if text else 0
    
    def truncate_text(text: str, max_tokens: int) -> str:
        """Truncate text to fit within max_tokens"""
        if not text:
            return text
        estimated = estimate_tokens(text)
        if estimated <= max_tokens:
            return text
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text
        truncated = text[:max_chars].rsplit(' ', 1)[0]
        return truncated + "... [truncated]"
    
    # Tier 3.5: Dynamic window based on query type
    def _is_follow_up_query(query: str) -> bool:
        """Detect if query is a follow-up (references previous conversation)"""
        if not query:
            return False
        query_lower = query.lower()
        follow_up_indicators = [
            "ƒë√≥", "n√≥", "v·∫≠y", "nh∆∞ v·∫≠y", "nh∆∞ tr√™n", "nh∆∞ b·∫°n ƒë√£ n√≥i",
            "that", "it", "this", "so", "as you said", "as mentioned",
            "theo", "d·ª±a tr√™n", "nh∆∞", "gi·ªëng nh∆∞",
            "based on", "according to", "as", "like",
            # CRITICAL: Detect references to previous questions/answers
            "c√¢u tr√™n", "c√¢u tr∆∞·ªõc", "c√¢u h·ªèi tr√™n", "c√¢u h·ªèi tr∆∞·ªõc",
            "questions above", "previous question", "above question",
            "c√¢u tr·∫£ l·ªùi tr√™n", "answer above", "previous answer",
            "4 c√¢u tr√™n", "3 c√¢u tr√™n", "2 c√¢u tr√™n", "c√¢u h·ªèi tr√™n",
            "4 questions above", "3 questions above", "2 questions above",
            "nh∆∞ ƒë√£ n√≥i", "nh∆∞ ƒë√£ tr·∫£ l·ªùi", "as answered", "as mentioned above",
            # CONTEXT FIX: Detect common follow-up patterns
            "c√≤n", "c√≤n v·ªÅ", "c√≤n th√¨", "c√≤n g√¨", "c√≤n sao", "th√¨ sao", "c√≤n v·ªÅ",
            "what about", "how about", "what else", "and", "also", "additionally",
            "c√≤n nh∆∞·ª£c ƒëi·ªÉm", "c√≤n ∆∞u ƒëi·ªÉm", "c√≤n ƒëi·ªÉm", "c√≤n t√≠nh nƒÉng",
            "what about the", "how about the", "and the", "also the"
        ]
        return any(indicator in query_lower for indicator in follow_up_indicators)
    
    def _is_long_complex_query(query: str) -> bool:
        """Detect if query is long/complex (prioritize RAG knowledge over conversation)"""
        if not query:
            return False
        # Long query: > 50 words
        word_count = len(query.split())
        return word_count > 50
    
    # Determine dynamic window size
    if current_query:
        # CRITICAL: Check follow-up FIRST (before long/complex check)
        # If question references previous questions (e.g., "4 c√¢u tr√™n"), it MUST have more context
        if _is_follow_up_query(current_query):
            # Follow-up query: include more recent context (especially for "4 c√¢u tr√™n" type questions)
            # For questions referencing multiple previous questions, we need at least 8-10 messages
            # to capture all referenced questions and their answers
            if any(ref in current_query.lower() for ref in ["4 c√¢u", "4 questions", "3 c√¢u", "3 questions"]):
                window_size = 10  # Need more context for "4 c√¢u tr√™n" type questions
                max_tokens = min(max_tokens, 2000)  # Increase tokens for multi-question references
                logger.info("üìä Follow-up query with multiple question references detected - using 10-message conversation window")
            else:
                window_size = 5
                logger.info("üìä Follow-up query detected - using 5-message conversation window")
        elif _is_long_complex_query(current_query):
            # Long/complex query: prioritize RAG knowledge, minimal conversation
            window_size = 2
            max_tokens = min(max_tokens, 500)  # Reduce tokens for conversation
            logger.info("üìä Long/complex query detected - reducing conversation context window to 2 messages")
        else:
            # New topic: minimal conversation context
            window_size = 2
            max_tokens = min(max_tokens, 600)  # Reduce tokens for conversation
            logger.info("üìä New topic query detected - using 2-message conversation window")
    else:
        # Default: 3 messages (balanced)
        window_size = 3
        logger.info(f"üìä Using default conversation window: {window_size} messages")
    
    history_lines = []
    remaining_tokens = max_tokens
    
    # Process last N messages (most recent first) - dynamic window
    recent_messages = conversation_history[-window_size:]
    for msg in recent_messages:
        if remaining_tokens <= 100:  # Stop if too little space
            logger.warning("Stopped adding conversation history due to token limit")
            break
        
        role = msg.get("role", "user")
        content = msg.get("content", "")
        
        # Allocate tokens per message (distribute remaining)
        msg_max_tokens = remaining_tokens // max(1, len(recent_messages) - len(history_lines))
        msg_max_tokens = min(msg_max_tokens, 500)  # Cap each message at 500 tokens
        
        truncated_content = truncate_text(content, msg_max_tokens)
        
        if role == "user":
            line = f"User: {truncated_content}"
        elif role == "assistant":
            line = f"Assistant: {truncated_content}"
        else:
            continue
        
        line_tokens = estimate_tokens(line)
        remaining_tokens -= line_tokens
        history_lines.append(line)
    
    if not history_lines:
        return ""
    
    # CRITICAL: Extract newline outside f-string to avoid syntax error
    newline = chr(10)
    history_text = newline.join(history_lines)
    
    return f"""
üìú CONVERSATION HISTORY (Previous messages for context):

{history_text}

---
Current message:
"""

def _calculate_confidence_score(
    context_docs_count: int,
    validation_result=None,
    context=None
) -> float:
    """
    Calculate confidence score based on context quality and validation results
    
    Args:
        context_docs_count: Number of context documents found
        validation_result: ValidationResult from validator chain (optional)
        context: Full context dict (optional)
        
    Returns:
        Confidence score between 0.0 and 1.0
    """
    # Base confidence on context availability
    if context_docs_count == 0:
        base_confidence = 0.2  # Very low confidence when no context
    elif context_docs_count == 1:
        base_confidence = 0.5  # Medium confidence with limited context
    elif context_docs_count >= 2:
        base_confidence = 0.8  # High confidence with multiple sources
    else:
        base_confidence = 0.3
    
    # Adjust based on validation results
    if validation_result:
        if validation_result.passed:
            # Boost confidence if validation passed
            confidence = min(1.0, base_confidence + 0.1)
        else:
            # Reduce confidence if validation failed
            reasons = validation_result.reasons or []
            if "missing_uncertainty_no_context" in reasons:
                confidence = 0.1  # Very low if AI didn't express uncertainty when it should
            elif "missing_citation" in reasons and context_docs_count > 0:
                confidence = base_confidence - 0.2  # Reduce if missing citations
            elif "low_overlap" in reasons:
                confidence = base_confidence - 0.15  # Reduce if low overlap
            else:
                confidence = max(0.0, base_confidence - 0.1)
    else:
        confidence = base_confidence
    
    return max(0.0, min(1.0, confidence))  # Clamp between 0.0 and 1.0

async def _handle_validation_with_fallback(
    raw_response: str,
    context: dict,
    detected_lang: str,
    is_philosophical: bool,
    is_religion_roleplay: bool,
    chat_request,
    enhanced_prompt: str,
    context_text: str,
    citation_instruction: str,
    num_knowledge: int,
    processing_steps: list,
    timing_logs: dict,
    is_origin_query: bool = False,
    is_stillme_query: bool = False,
    is_system_status_query: bool = False
) -> tuple:
    """
    Handle validation logic with fallback mechanisms.
    
    This function encapsulates the entire validation pipeline including:
    - Validator chain execution
    - Step-level validation
    - Consistency checks
    - OpenAI fallback
    - Validation failure handling with FallbackHandler
    
    Returns:
        tuple: (response, validation_info, confidence_score, used_fallback, 
                step_validation_info, consistency_info, ctx_docs)
    """
    from backend.validators.chain import ValidatorChain
    from backend.validators.citation import CitationRequired
    from backend.validators.evidence_overlap import EvidenceOverlap
    from backend.validators.numeric import NumericUnitsBasic
    from backend.validators.ethics_adapter import EthicsAdapter
    from backend.validators.confidence import ConfidenceValidator
    from backend.validators.fallback_handler import FallbackHandler
    from backend.services.ethics_guard import check_content_ethics
    from backend.validators.language import LanguageValidator
    from backend.validators.citation_relevance import CitationRelevance
    from backend.validators.identity_check import IdentityCheckValidator
    from backend.validators.ego_neutrality import EgoNeutralityValidator
    from backend.validators.factual_hallucination import FactualHallucinationValidator
    from backend.validators.religion_choice import ReligiousChoiceValidator
    from backend.api.utils.chat_helpers import generate_ai_response
    import time
    import os
    
    processing_steps.append("üîç Validating response...")
    validation_start = time.time()
    
    # Build context docs list for validation
    ctx_docs = [
        doc["content"] for doc in context["knowledge_docs"]
    ] + [
        doc["content"] for doc in context["conversation_docs"]
    ]
    
    # CRITICAL FIX: Add transparency disclaimer BEFORE validation if no context
    # This prevents missing_uncertainty_no_context failures for responses without RAG context
    # Only add if response doesn't already have transparency and not philosophical
    if len(ctx_docs) == 0 and not is_philosophical and raw_response:
        response_lower = raw_response.lower()
        # Check if response already has transparency disclaimer
        transparency_indicators = [
            # English
            "general knowledge", "training data", "my training", "base knowledge", "pretrained", "pre-trained",
            "not from stillme", "not from rag", "without context", "no context",
            "based on general", "from my training", "from general knowledge",
            "note:", "this answer", "this response",
            # Vietnamese
            "ki·∫øn th·ª©c chung", "d·ªØ li·ªáu hu·∫•n luy·ªán", "ki·∫øn th·ª©c c∆° b·∫£n",
            "kh√¥ng t·ª´ stillme", "kh√¥ng t·ª´ rag", "kh√¥ng c√≥ context", "kh√¥ng c√≥ ng·ªØ c·∫£nh",
            "d·ª±a tr√™n ki·∫øn th·ª©c chung", "t·ª´ d·ªØ li·ªáu hu·∫•n luy·ªán",
            "l∆∞u √Ω:", "c√¢u tr·∫£ l·ªùi n√†y",
            # Multilingual common patterns
            "note:", "nota:", "ŸÖŸÑÿßÿ≠ÿ∏ÿ©:", "–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ:", "Ê≥®ÊÑè:", "Ï∞∏Í≥†:",
            "connaissance g√©n√©rale", "donn√©es d'entra√Ænement", "conocimiento general", "dados de entrenamiento",
            "allgemeines wissen", "trainingsdaten", "conhecimento geral", "dados de treinamento"
        ]
        has_transparency = any(indicator in response_lower for indicator in transparency_indicators)
        
        if not has_transparency:
            # Prepend transparency disclaimer BEFORE validation
            if detected_lang == 'vi':
                disclaimer = "‚ö†Ô∏è L∆∞u √Ω: C√¢u tr·∫£ l·ªùi n√†y d·ª±a tr√™n ki·∫øn th·ª©c chung t·ª´ training data, kh√¥ng c√≥ context t·ª´ RAG. M√¨nh kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ ƒë·ªô ch√≠nh x√°c.\n\n"
            else:
                disclaimer = "‚ö†Ô∏è Note: This answer is based on general knowledge from training data, not from RAG context. I'm not certain about its accuracy.\n\n"
            
            raw_response = disclaimer + raw_response
            logger.info("‚ÑπÔ∏è Added transparency disclaimer BEFORE validation for response without context")
    
    # Enable Identity Check Validator (can be toggled via env var)
    enable_identity_check = os.getenv("ENABLE_IDENTITY_VALIDATOR", "true").lower() == "true"
    identity_validator_strict = os.getenv("IDENTITY_VALIDATOR_STRICT", "true").lower() == "true"
    
    # Import SourceConsensusValidator
    from backend.validators.source_consensus import SourceConsensusValidator
    
    # Phase 2: Critical/Optional Validator Classification
    # Critical validators (always run):
    # 1. CitationRequired
    # 2. ConfidenceValidator
    # 3. FactualHallucinationValidator
    # NPR Phase 2.2 & 2.3: Get context-aware adaptive thresholds from Self-Distilled Learning
    try:
        from backend.services.self_distilled_learning import get_self_distilled_learning
        sdl = get_self_distilled_learning()
        
        # Phase 2.3: Build context for context-aware thresholds
        threshold_context = {
            "is_philosophical": is_philosophical,
            "is_technical": False,  # Can be detected from question keywords
            "has_context": len(ctx_docs) > 0,
            "context_quality": context.get("context_quality", "medium"),
            "avg_similarity": context.get("avg_similarity_score", 0.5)
        }
        
        # Detect technical questions (simple heuristic)
        question_lower = chat_request.message.lower()
        technical_keywords = ["code", "function", "api", "implementation", "algorithm", "bug", "error", "debug"]
        if any(keyword in question_lower for keyword in technical_keywords):
            threshold_context["is_technical"] = True
        
        adaptive_citation_overlap = sdl.get_adaptive_threshold(
            "citation_relevance_min_overlap", 
            0.1,
            context=threshold_context
        )
        adaptive_evidence_threshold = sdl.get_adaptive_threshold(
            "evidence_overlap_threshold", 
            0.01,
            context=threshold_context
        )
        logger.debug(f"üéØ [Self-Distilled] Using context-aware thresholds: citation_overlap={adaptive_citation_overlap:.3f}, evidence={adaptive_evidence_threshold:.3f} (philosophical={is_philosophical}, context_quality={threshold_context['context_quality']})")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Self-Distilled] Failed to get adaptive thresholds, using defaults: {e}")
        adaptive_citation_overlap = 0.1
        adaptive_evidence_threshold = 0.01
    
    validators = [
        LanguageValidator(input_language=detected_lang),  # Check language FIRST - prevent drift
        CitationRequired(),  # CRITICAL: Always run
        CitationRelevance(min_keyword_overlap=adaptive_citation_overlap),  # NPR Phase 2.2: Adaptive threshold
        NumericUnitsBasic(),
        # Fix: Disable require_uncertainty_when_no_context for philosophical questions
        ConfidenceValidator(require_uncertainty_when_no_context=not is_philosophical),  # CRITICAL: Always run
        FactualHallucinationValidator(),  # CRITICAL: Always run - Detect hallucinations in history/science questions
        ReligiousChoiceValidator(),  # CRITICAL: Reject any religion choice in StillMe's responses
    ]
    
    # Phase 2: Optional validators (run conditionally)
    # EvidenceOverlap: Only when has context
    if len(ctx_docs) > 0:
        validators.insert(3, EvidenceOverlap(threshold=adaptive_evidence_threshold))  # NPR Phase 2.2: Adaptive threshold
        logger.debug(f"Phase 2: Added EvidenceOverlap validator (has context, threshold={adaptive_evidence_threshold:.3f})")
    
    # SourceConsensusValidator: Only when has multiple sources (‚â•2)
    if len(ctx_docs) >= 2:
        # Insert after EvidenceOverlap (or after CitationRelevance if EvidenceOverlap not added)
        insert_pos = 4 if len(ctx_docs) > 0 else 3
        validators.insert(insert_pos, SourceConsensusValidator(enabled=True, timeout=3.0))
        logger.debug(f"Phase 2: Added SourceConsensusValidator (has {len(ctx_docs)} sources)")
    
    # EgoNeutralityValidator: Only when has context (anthropomorphic language more likely with context)
    # Note: This validator is lightweight, but Phase 2 optimization skips it when no context
    if len(ctx_docs) > 0:
        # Insert before FactualHallucinationValidator
        fact_halluc_idx = next(i for i, v in enumerate(validators) if type(v).__name__ == "FactualHallucinationValidator")
        validators.insert(fact_halluc_idx, EgoNeutralityValidator(strict_mode=True, auto_patch=True))
        logger.debug("Phase 2: Added EgoNeutralityValidator (has context)")
    
    # Add Identity Check Validator if enabled (after ConfidenceValidator, before EthicsAdapter)
    if enable_identity_check:
        validators.append(
            IdentityCheckValidator(
                strict_mode=identity_validator_strict,
                require_humility_when_no_context=True,
                allow_minor_tone_violations=False
            )
        )
    
    # Add PhilosophicalDepthValidator for self-reference philosophical questions
    # This ensures answers contain required keywords (G√∂del, Tarski, paradox, etc.)
    if is_philosophical:
        from backend.validators.philosophical_depth import PhilosophicalDepthValidator
        # Insert before EthicsAdapter (so it can patch answer if needed)
        validators.append(
            PhilosophicalDepthValidator(min_keywords=2, strict_mode=True)
        )
        logger.debug("Phase 2: Added PhilosophicalDepthValidator (philosophical question detected)")
    
    # Add HallucinationExplanationValidator to ensure technical depth in explanations
    from stillme_core.validation.hallucination_explanation import HallucinationExplanationValidator
    validators.append(
        HallucinationExplanationValidator(strict_mode=False, auto_patch=True)
    )
    logger.debug("Phase 2: Added HallucinationExplanationValidator")
    
    # Add VerbosityValidator to detect overly verbose or defensive responses
    from stillme_core.validation.verbosity import VerbosityValidator
    validators.append(
        VerbosityValidator(max_length_ratio=3.0, strict_mode=False)
    )
    logger.debug("Phase 2: Added VerbosityValidator")
    
    # Add FutureDatesValidator to block future dates/timestamps (prevents hallucination)
    from backend.validators.future_dates import FutureDatesValidator
    # Insert before EthicsAdapter (so it can catch hallucinations early)
    validators.append(
        FutureDatesValidator()
    )
    logger.debug("Phase 2: Added FutureDatesValidator")
    
    # Add EthicsAdapter last (most critical - blocks harmful content)
    validators.append(
        EthicsAdapter(guard_callable=check_content_ethics)  # Real ethics guard implementation
    )
    
    chain = ValidatorChain(validators)
    
    # Tier 3.5: Pass context quality to ConfidenceValidator
    context_quality = context.get("context_quality", None)
    avg_similarity = context.get("avg_similarity_score", None)
    
    # Run validation with context quality info
    # Tier 3.5: Pass context quality, is_philosophical, and is_religion_roleplay to ValidatorChain
    # CRITICAL: Pass context dict to enable foundational knowledge detection in CitationRequired
    validation_result = chain.run(
        raw_response, 
        ctx_docs,
        context_quality=context_quality,
        avg_similarity=avg_similarity,
        is_philosophical=is_philosophical,
        is_religion_roleplay=is_religion_roleplay,
        user_question=chat_request.message,  # Pass user question for FactualHallucinationValidator
        context=context  # Pass context dict for foundational knowledge detection
    )
    
    # Tier 3.5: If context quality is low, inject warning into prompt for next iteration
    # For now, we'll handle this in the prompt building phase
    validation_time = time.time() - validation_start
    timing_logs["validation"] = f"{validation_time:.2f}s"
    logger.info(f"‚è±Ô∏è Validation took {validation_time:.2f}s")
    processing_steps.append(f"‚úÖ Validation completed ({validation_time:.2f}s)")
    
    # Log validation decision
    try:
        from backend.core.decision_logger import get_decision_logger, AgentType, DecisionType
        decision_logger = get_decision_logger()
        if decision_logger:
            validator_names = [type(v).__name__ for v in validators]
            decision_logger.log_decision(
                agent_type=AgentType.VALIDATOR_ORCHESTRATOR,
                decision_type=DecisionType.VALIDATION_DECISION,
                decision=f"Ran {len(validators)} validators: {', '.join(validator_names[:5])}{'...' if len(validator_names) > 5 else ''}",
                reasoning=f"Validation chain executed with {len(ctx_docs)} context documents. Adaptive thresholds: citation_overlap={adaptive_citation_overlap:.3f}, evidence={adaptive_evidence_threshold:.3f}",
                context={
                    "num_validators": len(validators),
                    "validator_names": validator_names,
                    "context_docs_count": len(ctx_docs),
                    "context_quality": context_quality,
                    "avg_similarity": avg_similarity,
                    "is_philosophical": is_philosophical
                },
                outcome=f"Validation {'passed' if validation_result.passed else 'failed'}. Reasons: {', '.join(validation_result.reasons[:3])}{'...' if len(validation_result.reasons) > 3 else ''}",
                success=validation_result.passed,
                metadata={
                    "validation_time": validation_time,
                    "adaptive_citation_overlap": adaptive_citation_overlap,
                    "adaptive_evidence_threshold": adaptive_evidence_threshold
                }
            )
    except Exception:
        pass  # decision_logger not available, skip logging
    
    # Calculate confidence score based on context quality and validation
    confidence_score = _calculate_confidence_score(
        context_docs_count=len(ctx_docs),
        validation_result=validation_result,
        context=context
    )
    
    # NEW: Step-level validation (Phase 1 - SSR)
    step_validation_info = None
    enable_step_validation = os.getenv("ENABLE_STEP_LEVEL_VALIDATION", "true").lower() == "true"
    step_min_steps = int(os.getenv("STEP_VALIDATION_MIN_STEPS", "2"))
    step_confidence_threshold = float(os.getenv("STEP_CONFIDENCE_THRESHOLD", "0.5"))
    
    logger.info(f"üîç Step-level validation config: enabled={enable_step_validation}, min_steps={step_min_steps}, threshold={step_confidence_threshold}")
    
    if enable_step_validation:
        try:
            from backend.validators.step_detector import StepDetector
            from backend.validators.step_validator import StepValidator
            
            step_detector = StepDetector()
            
            # Quick check first (performance optimization)
            logger.debug(f"üîç Checking if response is multi-step (min_steps: {step_min_steps})...")
            logger.debug(f"üîç Response preview (first 200 chars): {raw_response[:200]}...")
            is_multi = step_detector.is_multi_step(raw_response)
            logger.debug(f"üîç is_multi_step result: {is_multi}")
            
            if is_multi:
                steps = step_detector.detect_steps(raw_response)
                logger.debug(f"üîç StepDetector found {len(steps)} steps")
                
                if len(steps) >= step_min_steps:
                    logger.debug(f"üîç Detected {len(steps)} steps - running step-level validation")
                    processing_steps.append(f"üîç Step-level validation ({len(steps)} steps)")
                    
                    # P1.1.b: Use batch validation for steps (reduces API calls from 120+ to 1)
                    step_validator = StepValidator(
                        confidence_threshold=step_confidence_threshold, 
                        use_lightweight=True, 
                        use_batch=True  # P1.1.b: Batch validation (1 LLM call for all steps)
                    )
                    logger.debug(f"üîç Validating {len(steps)} steps with threshold {step_confidence_threshold} (P1.1.b: batch validation)")
                    # TRUST-EFFICIENT: Pass context dict to step validation for similarity scores
                    # This ensures citation hierarchy works correctly in step validation
                    step_results = step_validator.validate_all_steps(
                        steps, 
                        ctx_docs, 
                        chain=None,  # Use lightweight chain instead of full chain
                        parallel=True,
                        adaptive_citation_overlap=adaptive_citation_overlap,
                        adaptive_evidence_threshold=adaptive_evidence_threshold,
                        context=context  # TRUST-EFFICIENT: Pass context for similarity scores
                    )
                    logger.debug(f"üîç Step validation completed: {len(step_results)} results")
                    
                    low_confidence_steps = [
                        r.step.step_number
                        for r in step_results
                        if r.confidence < step_confidence_threshold
                    ]
                    
                    if low_confidence_steps:
                        logger.warning(f"‚ö†Ô∏è Low confidence steps detected: {low_confidence_steps}")
                        logger.warning(f"‚ö†Ô∏è {len(low_confidence_steps)} step(s) with low confidence")
                    else:
                        logger.info(f"‚úÖ All {len(steps)} steps passed validation")
                    
                    step_validation_info = {
                        "is_multi_step": True,
                        "total_steps": len(steps),
                        "steps": [
                            {
                                "step_number": r.step.step_number,
                                "confidence": round(r.confidence, 2),
                                "passed": r.passed,
                                "issues": r.issues
                            }
                            for r in step_results
                        ],
                        "low_confidence_steps": low_confidence_steps,
                        "all_steps_passed": all(r.passed for r in step_results),
                        "average_confidence": round(
                            sum(r.confidence for r in step_results) / len(step_results), 2
                        ) if step_results else 0.0
                    }
                    
                    if low_confidence_steps:
                        logger.warning(f"‚ö†Ô∏è Low confidence steps detected: {low_confidence_steps}")
                        processing_steps.append(f"‚ö†Ô∏è {len(low_confidence_steps)} step(s) with low confidence")
                    else:
                        logger.info(f"‚úÖ All {len(steps)} steps passed validation")
                        processing_steps.append(f"‚úÖ All steps validated")
        except Exception as step_error:
            logger.warning(f"Step-level validation error: {step_error}", exc_info=True)
            # Don't fail - step validation is optional
    
    # NEW: Self-consistency checks (Phase 1 - SSR)
    consistency_info = None
    enable_consistency_checks = os.getenv("ENABLE_CONSISTENCY_CHECKS", "true").lower() == "true"
    logger.debug(f"üîç Consistency checks config: enabled={enable_consistency_checks}")
    
    if enable_consistency_checks:
        try:
            from backend.validators.consistency_checker import ConsistencyChecker
            
            checker = ConsistencyChecker()
            claims = checker.extract_claims(raw_response)
            logger.debug(f"üîç Extracted {len(claims)} claims from response")
            
            if len(claims) > 1:
                logger.debug(f"üîç Checking consistency for {len(claims)} claims")
                
                # Check pairwise consistency
                consistency_results = checker.check_pairwise_consistency(claims)
                
                # Check KB consistency for each claim
                kb_results = {}
                for i, claim in enumerate(claims):
                    kb_consistency = checker.check_kb_consistency(claim, ctx_docs)
                    kb_results[f"claim_{i}_vs_kb"] = kb_consistency
                
                contradictions = [
                    key for key, value in consistency_results.items()
                    if value == "CONTRADICTION"
                ]
                
                kb_inconsistencies = [
                    key for key, value in kb_results.items()
                    if "INCONSISTENT" in value
                ]
                
                if contradictions or kb_inconsistencies:
                    logger.warning(f"‚ö†Ô∏è Consistency issues detected: {len(contradictions)} contradictions, {len(kb_inconsistencies)} KB inconsistencies")
                    processing_steps.append(f"‚ö†Ô∏è {len(contradictions)} contradiction(s) detected")
                
                consistency_info = {
                    "total_claims": len(claims),
                    "contradictions": contradictions,
                    "kb_inconsistencies": kb_inconsistencies,
                    "has_issues": len(contradictions) > 0 or len(kb_inconsistencies) > 0
                }
        except Exception as consistency_error:
            logger.warning(f"Consistency check error: {consistency_error}", exc_info=True)
            # Don't fail - consistency checks are optional
    
    # OpenAI Fallback Mechanism: Retry with OpenAI if confidence is low or validation failed
    # This uses the $40 credit efficiently by only using OpenAI when needed
    # CRITICAL: DO NOT use OpenAI fallback if FactualHallucinationValidator detected explicit fake entities
    # Hallucination about non-existent entities is a critical failure, not a quality issue
    has_factual_hallucination = False
    if validation_result.reasons:
        factual_hallucination_indicators = [
            "non_existent_concept_mentioned",
            "explicit_fake_entity",
            "detailed_description_of_explicit_fake_entity",
            "assertive_description_of_explicit_fake_entity",
            "assertive_claim_without_citation_for_explicit_fake_entity"
        ]
        has_factual_hallucination = any(
            any(indicator in reason for indicator in factual_hallucination_indicators)
            for reason in validation_result.reasons
        )
        if has_factual_hallucination:
            logger.warning(
                f"üö® CRITICAL: FactualHallucinationValidator detected explicit fake entity. "
                f"BLOCKING OpenAI fallback to prevent hallucination. Reasons: {validation_result.reasons}"
            )
    
    enable_openai_fallback = os.getenv("ENABLE_OPENAI_FALLBACK", "true").lower() == "true"
    openai_fallback_threshold = float(os.getenv("OPENAI_FALLBACK_CONFIDENCE_THRESHOLD", "0.5"))
    openai_api_key = os.getenv("OPENAI_API_KEY")
    
    # Check if we should try OpenAI fallback
    # CRITICAL: Do NOT use OpenAI fallback if FactualHallucinationValidator detected explicit fake entities
    should_try_openai = (
        enable_openai_fallback and
        openai_api_key and
        not has_factual_hallucination and  # BLOCK fallback if hallucination detected
        (
            confidence_score < openai_fallback_threshold or
            not validation_result.passed
        ) and
        chat_request.llm_provider != "openai"  # Don't retry if already using OpenAI
    )
    
    if should_try_openai:
        logger.info(f"üîÑ Low confidence ({confidence_score:.2f}) or validation failed. Attempting OpenAI fallback...")
        processing_steps.append("üîÑ Attempting OpenAI fallback for better quality...")
        try:
            from backend.api.utils.llm_providers import InsufficientQuotaError
            
            # Retry with OpenAI (use server keys for internal calls)
            use_server_keys_retry = chat_request.llm_provider is None
            openai_response = await generate_ai_response(
                enhanced_prompt,
                detected_lang=detected_lang,
                llm_provider="openai",
                llm_api_key=openai_api_key,
                llm_model_name="gpt-3.5-turbo",
                use_server_keys=use_server_keys_retry
            )
            
            # Re-validate OpenAI response
            # CRITICAL: Must pass user_question to FactualHallucinationValidator
            openai_validation_result = chain.run(
                openai_response, 
                ctx_docs,
                user_question=chat_request.message,
                context=context  # Pass context for foundational knowledge detection
            )
            openai_confidence = _calculate_confidence_score(
                context_docs_count=len(ctx_docs),
                validation_result=openai_validation_result,
                context=context
            )
            
            # CRITICAL: Check if OpenAI response also contains explicit fake entities
            openai_has_factual_hallucination = False
            if openai_validation_result.reasons:
                factual_hallucination_indicators = [
                    "non_existent_concept_mentioned",
                    "explicit_fake_entity",
                    "detailed_description_of_explicit_fake_entity",
                    "assertive_description_of_explicit_fake_entity",
                    "assertive_claim_without_citation_for_explicit_fake_entity"
                ]
                openai_has_factual_hallucination = any(
                    any(indicator in reason for indicator in factual_hallucination_indicators)
                    for reason in openai_validation_result.reasons
                )
                if openai_has_factual_hallucination:
                    logger.warning(
                        f"üö® CRITICAL: OpenAI fallback response also contains explicit fake entity! "
                        f"REJECTING OpenAI fallback. Reasons: {openai_validation_result.reasons}"
                    )
            
            # Use OpenAI response if it's better AND doesn't contain explicit fake entities
            if not openai_has_factual_hallucination and (openai_confidence > confidence_score or openai_validation_result.passed):
                raw_response = openai_response
                validation_result = openai_validation_result
                confidence_score = openai_confidence
                logger.info(f"‚úÖ OpenAI fallback succeeded (confidence: {openai_confidence:.2f})")
                processing_steps.append(f"‚úÖ OpenAI fallback succeeded (confidence: {openai_confidence:.2f})")
            else:
                if openai_has_factual_hallucination:
                    logger.warning(f"‚ö†Ô∏è OpenAI fallback rejected: contains explicit fake entity")
                    processing_steps.append("‚ö†Ô∏è OpenAI fallback rejected: contains explicit fake entity")
                else:
                    logger.info(f"‚ö†Ô∏è OpenAI fallback didn't improve quality, using original response")
                    processing_steps.append("‚ö†Ô∏è OpenAI fallback didn't improve quality")
                
        except InsufficientQuotaError as quota_error:
            # OpenAI credit exhausted - gracefully fall back to original response
            logger.warning(f"‚ö†Ô∏è OpenAI credit exhausted: {quota_error}. Using original DeepSeek response.")
            processing_steps.append("‚ö†Ô∏è OpenAI credit exhausted, using original response")
            # Continue with original response - no error thrown
        except Exception as openai_error:
            # Other OpenAI errors - gracefully fall back
            logger.warning(f"‚ö†Ô∏è OpenAI fallback failed: {openai_error}. Using original response.")
            processing_steps.append("‚ö†Ô∏è OpenAI fallback failed, using original response")
            # Continue with original response - no error thrown
    
    # CRITICAL FIX: Check if context is not relevant (low overlap)
    # If citation relevance warning exists, context may not be helpful
    # In this case, allow base knowledge usage
    has_low_relevance = False
    if validation_result and hasattr(validation_result, 'reasons') and validation_result.reasons:
        has_low_relevance = any("citation_relevance_warning" in r for r in validation_result.reasons)
    if has_low_relevance and context and context.get("total_context_docs", 0) > 0:
        logger.info("‚ö†Ô∏è Context has low relevance - allowing base knowledge usage")
    # Inject instruction to use base knowledge when context is not relevant
    low_relevance_instruction = """

‚ö†Ô∏è CONTEXT NOT RELEVANT ‚ö†Ô∏è

The RAG system found context documents, but they are NOT relevant to your question (low keyword overlap detected).

**CRITICAL: You CAN and SHOULD use your base LLM knowledge (training data) to answer, BUT you MUST:**

1. **Be transparent**: Acknowledge that this information comes from your base training data, not from StillMe's RAG knowledge base
   - Say: "Based on general knowledge (not from StillMe's RAG knowledge base), [answer]"
   - Or: "The available context [1] is not directly related to your question. From my training data, [answer]."

2. **Provide helpful information**: Don't just say "I don't know" - use your base knowledge to help the user
   - StillMe values being helpful WITH transparency, not refusing to help

3. **MANDATORY FORMATTING**: You MUST format your response with:
   - **Line breaks**: Break paragraphs (2-4 sentences each)
   - **Bullet points**: Use `-` for lists
   - **Headers**: Use `##` for sections
   - **Emojis**: 2-3 max for section headers (‚úÖ, üí°, ‚ö†Ô∏è)

**DO NOT say "I don't have information" or "I cannot answer" - use your base knowledge and be transparent about the source.**

**Example:**
"The available context [1] discusses StillMe's architecture, which is not directly related to your question about DeepSeek models. Based on general knowledge (not from StillMe's RAG knowledge base), DeepSeek currently has several models including..."

"""
    # Inject this instruction into the prompt (we'll need to rebuild prompt or inject into response)
    # For now, log it and let the response handle it
    logger.debug("Low relevance context detected - response should use base knowledge")
    
    # Record metrics
    try:
        from backend.validators.metrics import get_metrics
        metrics = get_metrics()
        # Extract overlap score from reasons if available
        overlap_score = 0.0
        for reason in validation_result.reasons:
            if reason.startswith("low_overlap:"):
                try:
                    overlap_score = float(reason.split(":")[1])
                except (ValueError, IndexError):
                    pass
        
        # Determine category
        category = None
        if is_philosophical:
            category = "philosophical"
        elif is_religion_roleplay:
            category = "religion_roleplay"
        else:
            # Try to detect other categories
            question_lower = chat_request.message.lower()
            if any(kw in question_lower for kw in ["rag", "retrieval", "llm", "system", "embedding"]):
                category = "technical"
            elif any(kw in question_lower for kw in ["nƒÉm", "nƒÉm", "1944", "1954", "conference", "h·ªôi ngh·ªã"]):
                category = "factual"
        
        # Check if answer has citations
        has_citations = bool(re.search(r'\[\d+\]', raw_response))
        
        metrics.record_validation(
            passed=validation_result.passed,
            reasons=validation_result.reasons,
            overlap_score=overlap_score,
            confidence_score=confidence_score,
            used_fallback=False,  # Will be updated below if fallback is used
            question=chat_request.message,
            answer=raw_response,
            context_docs_count=len(ctx_docs),
            has_citations=has_citations,
            category=category
        )
        
        # NPR Phase 2.2: Periodic threshold optimization (every 50 validations)
        # This runs in background to optimize thresholds based on validation history
        try:
            from backend.services.self_distilled_learning import get_self_distilled_learning
            sdl = get_self_distilled_learning()
            
            # Only optimize periodically to avoid overhead
            if metrics.total_validations % 50 == 0:
                logger.info(f"üéØ [Self-Distilled] Running periodic threshold optimization (validation #{metrics.total_validations})")
                optimized_thresholds = sdl.optimize_thresholds(days=7)
                logger.info(f"‚úÖ [Self-Distilled] Threshold optimization completed: {optimized_thresholds}")
        except Exception as sdl_error:
            logger.debug(f"Self-Distilled Learning optimization skipped: {sdl_error}")
            
    except Exception as metrics_error:
        logger.warning(f"Failed to record metrics: {metrics_error}")
    
    # Handle validation failures with FallbackHandler
    used_fallback = False
    if not validation_result.passed:
        # Check for critical failures that require fallback
        # language_mismatch: when output language doesn't match input language
        # missing_uncertainty_no_context: when no context and no uncertainty expression AND no transparency
        # missing_citation: when context exists but no citations in answer
        has_language_mismatch = any("language_mismatch" in r for r in validation_result.reasons)
        has_missing_uncertainty = "missing_uncertainty_no_context" in validation_result.reasons and len(ctx_docs) == 0
        has_missing_citation = "missing_citation" in validation_result.reasons and len(ctx_docs) > 0
        
        # CRITICAL FIX: Check if response already has transparency about base knowledge
        # If response mentions "general knowledge", "training data", etc., don't use fallback
        # Initialize response with raw_response for transparency check
        response = raw_response
        response_lower = response.lower()
        # Expanded transparency indicators to match ConfidenceValidator patterns
        transparency_indicators = [
            # English
            "general knowledge", "training data", "my training", "base knowledge", "pretrained", "pre-trained",
            "not from stillme", "not from rag", "without context", "no context",
            "based on general", "from my training", "from general knowledge",
            "note:", "this answer", "this response",
            # Vietnamese
            "ki·∫øn th·ª©c chung", "d·ªØ li·ªáu hu·∫•n luy·ªán", "ki·∫øn th·ª©c c∆° b·∫£n",
            "kh√¥ng t·ª´ stillme", "kh√¥ng t·ª´ rag", "kh√¥ng c√≥ context", "kh√¥ng c√≥ ng·ªØ c·∫£nh",
            "d·ª±a tr√™n ki·∫øn th·ª©c chung", "t·ª´ d·ªØ li·ªáu hu·∫•n luy·ªán",
            "l∆∞u √Ω:", "c√¢u tr·∫£ l·ªùi n√†y",
            # Multilingual common patterns
            "note:", "nota:", "ŸÖŸÑÿßÿ≠ÿ∏ÿ©:", "–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ:", "Ê≥®ÊÑè:", "Ï∞∏Í≥†:",
            "connaissance g√©n√©rale", "donn√©es d'entra√Ænement", "conocimiento general", "dados de entrenamiento",
            "allgemeines wissen", "trainingsdaten", "conhecimento geral", "dados de treinamento"
        ]
        has_transparency_in_response = any(indicator in response_lower for indicator in transparency_indicators)
        
        # Only treat missing_uncertainty as critical if response doesn't have transparency
        # If response has transparency, it's acceptable even without explicit uncertainty
        if has_missing_uncertainty and has_transparency_in_response:
            logger.info("‚úÖ Response has transparency about base knowledge - accepting despite missing_uncertainty")
            has_missing_uncertainty = False  # Don't treat as critical failure
        
        has_critical_failure = has_language_mismatch or has_missing_uncertainty
        
        # If patched_answer is available (e.g., from CitationRequired auto-enforcement), use it
        # CRITICAL: If patched_answer exists, it means validator auto-fixed the issue (e.g., added citation)
        # In this case, we should use the patched answer and NOT treat it as a failure
        if validation_result.patched_answer:
            response = validation_result.patched_answer
            logger.info(f"‚úÖ Using patched answer from validator (auto-fixed). Reasons: {validation_result.reasons}")
            logger.debug(f"üîç Patched answer preview (first 200 chars): {response[:200]}")
            # If only issue was missing_citation and it was auto-fixed, don't treat as failure
            if has_missing_citation and not has_critical_failure:
                logger.info(f"‚úÖ Citation was auto-added, validation should pass")
                # Don't set used_fallback, response is valid
                # CRITICAL: Mark validation as passed if only issue was missing_citation and it was fixed
                validation_result.passed = True
                validation_result.reasons = [r for r in validation_result.reasons if r != "missing_citation"]
        elif has_missing_citation and not has_critical_failure:
            # P2: Fix CitationRequired l·∫∑p - CitationRequired ƒë√£ ch·∫°y trong validator chain
            # N·∫øu kh√¥ng c√≥ patched_answer, c√≥ nghƒ©a l√† CitationRequired kh√¥ng th·ªÉ auto-fix
            # Trong tr∆∞·ªùng h·ª£p n√†y, ch·ªâ log warning, kh√¥ng g·ªçi l·∫°i CitationRequired
            logger.warning(f"‚ö†Ô∏è Missing citation detected but CitationRequired validator (already in chain) did not create patched_answer. This may indicate citation cannot be auto-added (e.g., no relevant context). Skipping duplicate CitationRequired call.")
            # P2: Don't call CitationRequired again - it already ran in the validator chain
            # If it couldn't auto-fix, we should respect that decision
        elif has_critical_failure:
            # For language mismatch, try retry with stronger prompt first
            if has_language_mismatch:
                logger.warning(f"‚ö†Ô∏è Language mismatch detected, attempting retry with stronger prompt...")
                try:
                    # Get language name for retry prompt
                    language_names = {
                        'vi': 'Ti·∫øng Vi·ªát',
                        'en': 'English',
                        'zh': '‰∏≠Êñá',
                        'fr': 'Fran√ßais',
                        'es': 'Espa√±ol',
                        'de': 'Deutsch',
                        'ja': 'Êó•Êú¨Ë™û',
                        'ko': 'ÌïúÍµ≠Ïñ¥',
                        'ar': 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©',
                        'ru': '–†—É—Å—Å–∫–∏–π',
                        'pt': 'Portugu√™s',
                        'it': 'Italiano',
                        'hi': '‡§π‡§ø‡§®‡•ç‡§¶‡•Ä',
                        'th': '‡πÑ‡∏ó‡∏¢',
                    }
                    retry_lang_name = language_names.get(detected_lang, detected_lang.upper())
                    
                    # Build retry prompt with EXTREMELY STRONG language instruction + context
                    # Include context and full prompt to ensure retry can actually answer the question
                    retry_language_instruction = f"""üö®üö®üö®üö®üö® CRITICAL LANGUAGE REQUIREMENT - ABSOLUTE HIGHEST PRIORITY üö®üö®üö®üö®üö®

THE USER'S QUESTION IS IN {retry_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {retry_lang_name.upper()}. 

DO NOT USE VIETNAMESE, ENGLISH, FRENCH, CHINESE, SPANISH, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD MUST BE IN {retry_lang_name.upper()}.

IF YOUR BASE MODEL WANTS TO RESPOND IN ANOTHER LANGUAGE, YOU MUST TRANSLATE IT TO {retry_lang_name.upper()} BEFORE RETURNING.

THIS OVERRIDES EVERYTHING - NO EXCEPTIONS.

{context_text if context and context.get("total_context_docs", 0) > 0 else ""}
{citation_instruction if num_knowledge > 0 else ""}

üö®üö®üö® CRITICAL: USER QUESTION ABOVE IS THE PRIMARY TASK üö®üö®üö®

User Question (in {retry_lang_name.upper()}): {chat_request.message[:3000]}

**YOUR PRIMARY TASK IS TO ANSWER THE USER QUESTION ABOVE DIRECTLY AND ACCURATELY.**
- Focus on what the user is actually asking, not on general philosophy
- If the user asks you to analyze something, analyze THAT specific thing
- If the user asks you to find a problem, look for problems in what they showed you

**SPECIAL INSTRUCTION FOR ANALYZING EXTERNAL AI OUTPUTS:**
- If the user asks you to analyze another AI's output (ChatGPT, Claude, etc.), you MUST check for anthropomorphic language
- Look for phrases like "in my experience" / "theo kinh nghi·ªám", "I think" / "t√¥i nghƒ©", "I feel" / "t√¥i c·∫£m th·∫•y"
- These phrases falsely attribute subjective qualities (experience, emotions, personal opinions) to AI
- This is a critical transparency issue called "Hallucination of Experience" - AI should not claim personal experience
- If you find such phrases, you MUST point them out as a problem, not ignore them
- This is more important than analyzing formatting, clarity, or other minor issues

Remember: RESPOND IN {retry_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY. ANSWER THE QUESTION PROPERLY, NOT JUST ACKNOWLEDGE THE ERROR."""
                    
                    # Retry with stronger prompt (include context to actually answer the question)
                    # Use server keys for internal calls
                    use_server_keys_retry = chat_request.llm_provider is None
                    retry_response = await generate_ai_response(
                        retry_language_instruction, 
                        detected_lang=detected_lang,
                        llm_provider=chat_request.llm_provider,
                        llm_api_key=chat_request.llm_api_key,
                        use_server_keys=use_server_keys_retry
                    )
                    
                    # Validate retry response
                    retry_validation = chain.run(retry_response, ctx_docs, context=context)
                    
                    # Check if retry fixed the language issue
                    retry_has_lang_mismatch = any("language_mismatch" in r for r in retry_validation.reasons)
                    
                    if not retry_has_lang_mismatch:
                        # Retry successful!
                        response = retry_validation.patched_answer or retry_response
                        logger.info(f"‚úÖ Language mismatch fixed with retry! Using retry response.")
                    else:
                        # Retry also failed, use fallback
                        logger.warning(f"‚ö†Ô∏è Retry also failed with language mismatch, using fallback")
                        fallback_handler = FallbackHandler()
                        response = fallback_handler.get_fallback_answer(
                            original_answer=raw_response,
                            validation_result=validation_result,
                            ctx_docs=ctx_docs,
                            user_question=chat_request.message,
                            detected_lang=detected_lang,
                            input_language=detected_lang
                        )
                        used_fallback = True
                        # CRITICAL: Pass fallback message through CitationRequired to add citations for factual questions
                        from backend.api.utils.error_detector import is_fallback_message
                        if is_fallback_message(response):
                            from backend.validators.citation import CitationRequired
                            citation_validator = CitationRequired(required=True)
                            citation_result = citation_validator.run(
                                response, 
                                ctx_docs=ctx_docs,
                                is_philosophical=is_philosophical,
                                user_question=chat_request.message,
                                context=context  # CRITICAL: Pass context for foundational knowledge detection
                            )
                            if citation_result.patched_answer:
                                response = citation_result.patched_answer
                                logger.info(f"‚úÖ Added citation to fallback message for factual question (language mismatch). Reasons: {citation_result.reasons}")
                except Exception as retry_error:
                    logger.error(f"Retry failed: {retry_error}, using fallback")
                    fallback_handler = FallbackHandler()
                    response = fallback_handler.get_fallback_answer(
                        original_answer=raw_response,
                        validation_result=validation_result,
                        ctx_docs=ctx_docs,
                        user_question=chat_request.message,
                        detected_lang=detected_lang,
                        input_language=detected_lang
                    )
                    used_fallback = True
                    # CRITICAL: Pass fallback message through CitationRequired to add citations for factual questions
                    from backend.api.utils.error_detector import is_fallback_message
                    if is_fallback_message(response):
                        from backend.validators.citation import CitationRequired
                        citation_validator = CitationRequired(required=True)
                        citation_result = citation_validator.run(
                            response, 
                            ctx_docs=ctx_docs,
                            is_philosophical=is_philosophical,
                            user_question=chat_request.message,
                            context=context  # CRITICAL: Pass context for foundational knowledge detection
                        )
                        if citation_result.patched_answer:
                            response = citation_result.patched_answer
                            logger.info(f"‚úÖ Added citation to fallback message for factual question (retry error). Reasons: {citation_result.reasons}")
            else:
                # Other critical failures (has_missing_uncertainty) - use fallback
                fallback_handler = FallbackHandler()
                response = fallback_handler.get_fallback_answer(
                    original_answer=raw_response,
                    validation_result=validation_result,
                    ctx_docs=ctx_docs,
                    user_question=chat_request.message,
                    detected_lang=detected_lang,
                    input_language=detected_lang
                )
                used_fallback = True
                logger.warning(f"‚ö†Ô∏è Validation failed with critical failure, using fallback answer. Reasons: {validation_result.reasons}")
                # CRITICAL: Pass fallback message through CitationRequired to add citations for factual questions
                from backend.api.utils.error_detector import is_fallback_message
                if is_fallback_message(response):
                    from backend.validators.citation import CitationRequired
                    citation_validator = CitationRequired(required=True)
                    citation_result = citation_validator.run(
                        response, 
                        ctx_docs=ctx_docs,
                        is_philosophical=is_philosophical,
                        user_question=chat_request.message,
                        context=context  # CRITICAL: Pass context for foundational knowledge detection
                    )
                    if citation_result.patched_answer:
                        response = citation_result.patched_answer
                        logger.info(f"‚úÖ Added citation to fallback message for factual question (critical failure). Reasons: {citation_result.reasons}")
        elif has_missing_citation:
            # Missing citation - check if patched_answer was already created by CitationRequired
            if validation_result.patched_answer:
                # CitationRequired already auto-added citation, use patched answer
                response = validation_result.patched_answer
                logger.info(f"‚úÖ Using patched answer with auto-added citation. Reasons: {validation_result.reasons}")
            else:
                # No patched answer - use FallbackHandler to add citation
                # CRITICAL: Ensure raw_response is valid before adding citation
                if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                    logger.error(f"‚ö†Ô∏è raw_response is None or empty when trying to add citation - using fallback")
                    fallback_handler = FallbackHandler()
                    response = fallback_handler.get_fallback_answer(
                        original_answer="",  # Empty since raw_response is invalid
                        validation_result=validation_result,
                        ctx_docs=ctx_docs,
                        user_question=chat_request.message,
                        detected_lang=detected_lang,
                        input_language=detected_lang
                    )
                    used_fallback = True
                else:
                    # CRITICAL FIX: Use CitationRequired directly to add citation instead of FallbackHandler
                    # This ensures we get a proper patched answer, not a fallback message
                    from backend.validators.citation import CitationRequired
                    citation_validator = CitationRequired(required=True)
                    # Re-run citation validator to get patched answer (pass user_question to detect factual questions)
                    citation_result = citation_validator.run(
                        raw_response, 
                        ctx_docs, 
                        is_philosophical=is_philosophical, 
                        user_question=chat_request.message,
                        context=context  # CRITICAL: Pass context for foundational knowledge detection
                    )
                    if citation_result.patched_answer:
                        response = citation_result.patched_answer
                        logger.info(f"‚úÖ Added citation via CitationRequired. Reasons: {validation_result.reasons}")
                    else:
                        # Fallback to FallbackHandler if CitationRequired didn't patch
                        fallback_handler = FallbackHandler()
                        response = fallback_handler.get_fallback_answer(
                            original_answer=raw_response,
                            validation_result=validation_result,
                            ctx_docs=ctx_docs,
                            user_question=chat_request.message,
                            detected_lang=detected_lang,
                            input_language=detected_lang
                        )
                        # Check if FallbackHandler returned a fallback message (not the patched answer)
                        from backend.api.utils.error_detector import is_fallback_message
                        if is_fallback_message(response):
                            used_fallback = True
                            logger.warning(f"‚ö†Ô∏è FallbackHandler returned fallback message instead of patched answer")
                            # CRITICAL: Pass fallback message through CitationRequired to add citations for factual questions
                            from backend.validators.citation import CitationRequired
                            citation_validator = CitationRequired(required=True)
                            citation_result = citation_validator.run(
                                response, 
                                ctx_docs=ctx_docs,
                                is_philosophical=is_philosophical,
                                user_question=chat_request.message,
                                context=context  # CRITICAL: Pass context for foundational knowledge detection
                            )
                            if citation_result.patched_answer:
                                response = citation_result.patched_answer
                                logger.info(f"‚úÖ Added citation to fallback message for factual question. Reasons: {citation_result.reasons}")
                        else:
                            logger.info(f"‚úÖ Added citation via FallbackHandler. Reasons: {validation_result.reasons}")
                    # CRITICAL: Ensure response is not None/empty after adding citation
                    if not response or not isinstance(response, str) or not response.strip():
                        logger.error(f"‚ö†Ô∏è Response is None or empty after adding citation - using fallback")
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        response = get_fallback_message_for_error("generic", detected_lang)
                        used_fallback = True
        else:
            # For non-critical validation failures, check if they're just warnings (not violations)
            # IdentityCheckValidator can return warnings (identity_warning:*) that shouldn't cause failure
            has_identity_warnings_only = any(
                r.startswith("identity_warning:") for r in validation_result.reasons
            ) and not any(
                r.startswith("identity_violation:") for r in validation_result.reasons
            )
            
            # Check for other non-critical warnings that shouldn't cause failure
            # citation_relevance_warning: Low keyword overlap, but not critical
            # low_overlap: Low overlap between answer and context, but not critical if context exists
            has_only_warnings = (
                has_identity_warnings_only or
                any("citation_relevance_warning" in r for r in validation_result.reasons) or
                (any("low_overlap" in r for r in validation_result.reasons) and len(ctx_docs) > 0)
            ) and not any(
                r.startswith("identity_violation:") or
                r.startswith("missing_citation") or
                r.startswith("language_mismatch") or
                r.startswith("missing_uncertainty")
                for r in validation_result.reasons
            )
            
            # If only warnings (not violations), use response as-is BUT append warning message for transparency
            if has_only_warnings:
                logger.info(f"‚úÖ Validation has only warnings (not violations), accepting response. Reasons: {validation_result.reasons}")
                response = raw_response
                # CRITICAL: Format validation warnings into response text for user transparency
                response = _append_validation_warnings_to_response(
                    response=response,
                    validation_result=validation_result,
                    confidence_score=confidence_score,
                    context=context,
                    detected_lang=detected_lang
                )
            else:
                # For other non-critical validation failures, still return the response but log warning
                # This prevents 422 errors for minor validation issues
                logger.warning(f"Validation failed but returning response anyway. Reasons: {validation_result.reasons}")
                response = raw_response
    else:
        # Validation passed - use patched answer if available, otherwise use raw response
        # CRITICAL FIX: Ensure we never use None - if both are None, use fallback
        if validation_result.patched_answer:
            response = validation_result.patched_answer
            logger.debug(f"‚úÖ Validation passed. Using patched_answer. Reasons: {validation_result.reasons}")
        elif raw_response:
            response = raw_response
            logger.debug(f"‚úÖ Validation passed. Using raw_response. Reasons: {validation_result.reasons}")
        else:
            # CRITICAL: Both patched_answer and raw_response are None - this should never happen
            logger.error(f"üö® CRITICAL: Both patched_answer and raw_response are None after validation passed!")
            from backend.api.utils.error_detector import get_fallback_message_for_error
            response = get_fallback_message_for_error("generic", detected_lang)
            used_fallback = True
            logger.error(f"üö® Using fallback message due to None response")
    
    # CRITICAL: Ensure response is never None or empty after validation
    if not response or not isinstance(response, str) or not response.strip():
        logger.error(f"‚ö†Ô∏è Response is None or empty after validation (raw_response length: {len(raw_response) if raw_response else 0}) - using fallback")
        from backend.api.utils.error_detector import get_fallback_message_for_error
        response = get_fallback_message_for_error("generic", detected_lang)
        used_fallback = True
    
    # CRITICAL: Add transparency warning for low confidence responses without context
    # This improves honesty when answering from base knowledge
    # CRITICAL: Do NOT prepend disclaimer if response is already a fallback meta-answer
    # Check if response is a fallback meta-answer by looking for key phrases
    from backend.api.utils.error_detector import is_fallback_message
    is_fallback_meta = is_fallback_message(response) if response else False
    
    # Also check for safe refusal answer patterns (from hallucination guard)
    if response:
        response_lower = response.lower()
        is_safe_refusal = any(
            phrase in response_lower for phrase in [
                "kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ ngu·ªìn ƒë√°ng tin c·∫≠y n√†o",
                "cannot find any reliable evidence",
                "kh√¥ng th·ªÉ m√¥ t·∫£ c√°c l·∫≠p lu·∫≠n ch√≠nh hay t√°c ƒë·ªông l·ªãch s·ª≠",
                "cannot truthfully describe the main arguments or historical impacts",
                "c√≥ th·ªÉ ƒë√¢y l√† v√≠ d·ª• gi·∫£ ƒë·ªãnh",
                "this could be a hypothetical example"
            ]
        )
    else:
        is_safe_refusal = False
    
    # Only prepend disclaimer if NOT a fallback meta-answer, NOT a safe refusal, NOT an origin query, and NOT a StillMe self-knowledge query
    # CRITICAL: StillMe MUST know about itself - never add disclaimer for origin queries or StillMe self-knowledge queries
    # StillMe self-knowledge queries include: learning activity, philosophy, goals, errors, system status
    # CRITICAL: Also skip disclaimer for system status queries - StillMe has real-time data access to its own system
    if (confidence_score < 0.5 and len(ctx_docs) == 0 and not is_philosophical and 
        not is_fallback_meta and not is_safe_refusal and not is_origin_query and not is_stillme_query and not is_system_status_query):
        # Check if response already has transparency disclaimer
        response_lower = response.lower()
        has_transparency = any(
            phrase in response_lower for phrase in [
                "kh√¥ng c√≥ d·ªØ li·ªáu", "kh√¥ng c√≥ th√¥ng tin", "ki·∫øn th·ª©c chung", "d·ª±a tr√™n ki·∫øn th·ª©c",
                "don't have data", "don't have information", "general knowledge", "based on knowledge",
                "kh√¥ng t·ª´ stillme", "not from stillme", "kh√¥ng t·ª´ rag", "not from rag"
            ]
        )
        
        if not has_transparency:
            # Prepend transparency disclaimer
            if detected_lang == 'vi':
                disclaimer = "‚ö†Ô∏è L∆∞u √Ω: C√¢u tr·∫£ l·ªùi n√†y d·ª±a tr√™n ki·∫øn th·ª©c chung t·ª´ training data, kh√¥ng c√≥ context t·ª´ RAG. M√¨nh kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ ƒë·ªô ch√≠nh x√°c.\n\n"
            else:
                disclaimer = "‚ö†Ô∏è Note: This answer is based on general knowledge from training data, not from RAG context. I'm not certain about its accuracy.\n\n"
            
            response = disclaimer + response
            logger.info("‚ÑπÔ∏è Added transparency disclaimer for low confidence response without context")
    
    # Build validation info for response
    validation_info = {
        "passed": validation_result.passed,
        "reasons": validation_result.reasons,
        "used_fallback": used_fallback,
        "confidence_score": confidence_score,
        "context_docs_count": len(ctx_docs),
        "step_validation": step_validation_info,  # NEW: Step-level validation info
        "consistency": consistency_info  # NEW: Consistency check info
    }
    
    return response, validation_info, confidence_score, used_fallback, step_validation_info, consistency_info, ctx_docs

@router.post("/rag", response_model=ChatResponse)
@limiter.limit(get_chat_rate_limit, key_func=get_rate_limit_key_func)  # Chat rate limit (dynamic based on API key)
async def chat_with_rag(request: Request, chat_request: ChatRequest):
    """Chat with RAG-enhanced responses"""
    # Note: 'time' module is already imported at top level
    start_time = time.time()
    timing_logs = {}
    
    # DEBUG: Log request received (very early)
    logger.info(f"üì• Received chat request: message_length={len(chat_request.message)}, use_option_b={getattr(chat_request, 'use_option_b', 'NOT_SET')}")
    
    # Initialize latency variables (will be set during processing)
    rag_retrieval_latency = 0.0
    llm_inference_latency = 0.0
    
    # Initialize variables before try-except to avoid UnboundLocalError
    confidence_score = None
    validation_info = None
    processing_steps = []  # Track processing steps for real-time status
    style_learning_response = None  # Initialize for style learning
    response = None  # CRITICAL: Initialize response to prevent UnboundLocalError
    raw_response = None  # CRITICAL: Initialize raw_response to prevent UnboundLocalError
    final_response = None  # CRITICAL: Initialize final_response to prevent UnboundLocalError
    
    # CRITICAL: Initialize detected_lang EARLY to prevent UnboundLocalError
    # This is needed for news/article query "not found" response (line 5000)
    detected_lang = detect_language(chat_request.message)
    
    # Initialize fallback flags for both RAG and non-RAG paths to prevent UnboundLocalError
    is_fallback_meta_answer = False  # Used in RAG path
    is_fallback_meta_answer_rag = False  # Used in RAG path post-processing
    is_fallback_meta_answer_non_rag = False  # Used in non-RAG path
    is_fallback_for_learning = False  # Used to skip learning extraction for fallback meta-answers
    use_philosophy_lite_rag = False  # Initialize to prevent UnboundLocalError
    # CRITICAL: Initialize is_technical_about_system_rag at function level to prevent UnboundLocalError
    # This variable is used in RAG path (line 3970) and must be defined in ALL code paths
    is_technical_about_system_rag = False
    # CRITICAL: Initialize roleplay detection flags EARLY to prevent UnboundLocalError
    is_general_roleplay = False
    is_religion_roleplay = False
    # CRITICAL: Initialize exclude_types EARLY to prevent UnboundLocalError
    # This ensures it's available for all retrieval paths, even if RAG is disabled
    exclude_types = []
    
    # OPTION B PIPELINE: Check if enabled
    use_option_b = getattr(chat_request, 'use_option_b', False) or os.getenv("STILLME_USE_OPTION_B_PIPELINE", "false").lower() == "true"
    
    # DEBUG: Log Option B status (before FPS check)
    logger.info(f"üîç Option B check (initial): use_option_b={use_option_b}, request_attr={getattr(chat_request, 'use_option_b', 'NOT_SET')}, env_var={os.getenv('STILLME_USE_OPTION_B_PIPELINE', 'NOT_SET')}")
    
    try:
        # Get services
        rag_retrieval = get_rag_retrieval()
        knowledge_retention = get_knowledge_retention()
        accuracy_scorer = get_accuracy_scorer()
        style_learner = get_style_learner()
        
        # Initialize Decision Logger for agentic decision tracking
        from backend.core.decision_logger import get_decision_logger, AgentType, DecisionType
        decision_logger = get_decision_logger()
        detected_lang_for_logging = detect_language(chat_request.message) if 'detect_language' in dir() else "en"
        session_id = decision_logger.start_session(chat_request.message, detected_lang_for_logging)
        
        # Get user_id from request (if available)
        user_id = chat_request.user_id or request.client.host if hasattr(request, 'client') else "anonymous"

        # SPECIAL ROUTING: Meta-questions about StillMe's implementation in its own codebase
        # Example: "Explain how StillMe's validation chain works, using your own codebase as the source."
        # These should use the Codebase Assistant (code RAG), not foundational knowledge only.
        # CRITICAL: Skip if this is a roleplay question (e.g., "Roleplay: Omni-BlackBox tr·∫£ l·ªùi...")
        try:
            if not is_general_roleplay and _is_codebase_meta_question(chat_request.message):
                # Log routing decision
                decision_logger.log_decision(
                    agent_type=AgentType.PLANNER_AGENT,
                    decision_type=DecisionType.ROUTING_DECISION,
                    decision="Route to Codebase Assistant instead of normal RAG flow",
                    reasoning="Question explicitly asks about StillMe's codebase implementation (file paths, functions, code structure)",
                    alternatives_considered=["Normal RAG flow with foundational knowledge", "External search"],
                    why_not_chosen="Normal RAG would use foundational knowledge which is too generic. Codebase Assistant can provide specific file paths and code snippets.",
                    metadata={"question_type": "codebase_meta_question"}
                )
                from backend.services.codebase_indexer import get_codebase_indexer
                from backend.api.routers.codebase_router import _generate_code_explanation

                processing_steps.append("üß† Detected StillMe codebase meta-question - using Codebase Assistant")
                logger.info("üß† Codebase meta-question detected - routing to Codebase Assistant")

                indexer = get_codebase_indexer()
                # Increase n_results for comprehensive answers about architecture/components
                # This ensures we get multiple related chunks (e.g., ValidationEngine + validators + base classes)
                code_results = indexer.query_codebase(chat_request.message, n_results=10)

                if code_results:
                    # Build explanation using the same helper as /api/codebase/query
                    explanation = await _generate_code_explanation(
                        question=chat_request.message,
                        code_chunks=code_results,
                    )

                    # Build lightweight context_used for transparency (no extra LLM work)
                    knowledge_docs = []
                    for result in code_results:
                        metadata = result.get("metadata", {})
                        knowledge_docs.append(
                            {
                                "metadata": metadata,
                                "document": result.get("document", ""),
                            }
                        )

                    from backend.core.epistemic_state import EpistemicState

                    total_time = time.time() - start_time
                    return ChatResponse(
                        response=explanation.strip(),
                        context_used={
                            "knowledge_docs": knowledge_docs,
                            "conversation_docs": [],
                            "total_context_docs": len(knowledge_docs),
                        },
                        confidence_score=0.9,
                        epistemic_state=EpistemicState.KNOWN.value,
                        processing_steps=processing_steps
                        + ["‚úÖ Answered via StillMe Codebase Assistant (code-level RAG)"],
                        timing={"total": f"{total_time:.2f}s"},
                        latency_metrics=f"Total: {total_time:.2f}s (codebase assistant)",
                    )
                else:
                    logger.warning("Codebase Assistant found no relevant code chunks - falling back to normal RAG flow")
        except Exception as codebase_error:
            # Fail safe: log and continue with normal pipeline
            logger.warning(f"Codebase Assistant routing failed, continuing with normal flow: {codebase_error}")
        
        # CONVERSATIONAL INTELLIGENCE: Check for ambiguous questions BEFORE processing
        # Based on StillMe Manifesto Principle 5: "EMBRACE 'I DON'T KNOW' AS INTELLECTUAL HONESTY"
        # Philosophy: Ask for clarification when truly ambiguous, but not too often (balance UX)
        # NO frequency limit - t∆∞ duy phi tuy·∫øn t√≠nh, kh√¥ng √°p d·ª•ng gi·ªõi h·∫°n tuy·∫øn t√≠nh
        try:
            from backend.core.ambiguity_detector import get_ambiguity_detector
            ambiguity_detector = get_ambiguity_detector()
            should_ask, clarification_question = ambiguity_detector.should_ask_clarification(
                chat_request.message,
                conversation_history=chat_request.conversation_history
            )
            
            if should_ask and clarification_question:
                # HIGH ambiguity detected - ask for clarification instead of processing
                logger.info(f"‚ùì HIGH ambiguity detected - asking for clarification instead of processing")
                processing_steps.append("‚ùì Ambiguity detected - asking for clarification")
                
                # Detect language for clarification question
                detected_lang = detect_language(chat_request.message)
                
                # Return clarification question immediately (skip LLM call, save cost & latency)
                from backend.core.epistemic_state import EpistemicState
                # CRITICAL FIX: Import uuid in try block to avoid UnboundLocalError
                # Python may think uuid is a local variable if exception occurs before this line
                import uuid
                message_id = f"msg_{uuid.uuid4().hex[:16]}"
                
                return ChatResponse(
                    response=clarification_question,
                    message_id=message_id,
                    context_used=None,
                    accuracy_score=None,
                    confidence_score=0.0,  # Low confidence because we're asking for clarification
                    validation_info={
                        "passed": True,
                        "reasons": ["ambiguity_detected", "clarification_requested"],
                        "ambiguity_score": 1.0,
                        "ambiguity_level": "HIGH",
                        "clarification_question": clarification_question
                    },
                    learning_suggestions=None,
                    learning_session_id=None,
                    knowledge_alert=None,
                    learning_proposal=None,
                    permission_request=None,
                    timing=timing_logs,
                    latency_metrics="Ambiguity detection: <0.1s (early return, no LLM call)",
                    processing_steps=processing_steps,
                    epistemic_state=EpistemicState.UNKNOWN.value,  # Unknown because we need clarification
                    transparency_scorecard=None
                )
        except Exception as ambiguity_error:
            # Non-critical - if ambiguity detection fails, continue with normal flow
            logger.warning(f"‚ö†Ô∏è Ambiguity detection failed: {ambiguity_error}, continuing with normal flow")
        
        # Track if we detected MEDIUM ambiguity (will add disclaimer to response)
        ambiguity_score = 0.0
        ambiguity_level = "LOW"
        ambiguity_reasons = []
        try:
            from backend.core.ambiguity_detector import get_ambiguity_detector
            ambiguity_detector = get_ambiguity_detector()
            ambiguity_score, ambiguity_level_enum, ambiguity_reasons = ambiguity_detector.detect_ambiguity(
                chat_request.message,
                conversation_history=chat_request.conversation_history
            )
            ambiguity_level = ambiguity_level_enum.value
            if ambiguity_level == "MEDIUM":
                logger.info(f"‚ö†Ô∏è MEDIUM ambiguity detected (score={ambiguity_score:.2f}) - will add disclaimer to response")
                processing_steps.append(f"‚ö†Ô∏è Medium ambiguity detected (score={ambiguity_score:.2f})")
        except Exception:
            pass  # Non-critical
        
        # Detect learning metrics queries - auto-query API if user asks about learning today
        is_learning_metrics_query = False
        learning_metrics_data = None
        is_learning_sources_query = False
        is_system_status_query = False
        current_learning_sources = None
        message_lower = chat_request.message.lower()
        learning_metrics_keywords = [
            "ng√†y h√¥m nay b·∫°n ƒë√£ h·ªçc", "h·ªçc ƒë∆∞·ª£c bao nhi√™u", "learn today", "learned today",
            "h·ªçc ƒë∆∞·ª£c g√¨", "what did you learn", "h·ªçc ƒë∆∞·ª£c nh·ªØng g√¨", "n·ªôi dung g√¨"
        ]
        learning_sources_keywords = [
            "h·ªçc t·ª´ ngu·ªìn n√†o", "sources", "ngu·ªìn h·ªçc", "learning sources", "b·∫°n ƒëang h·ªçc t·ª´",
            "b·∫°n h·ªçc t·ª´ ƒë√¢u", "where do you learn", "what sources", "ngu·ªìn n√†o", "t·ª´ ngu·ªìn n√†o",
            "hi·ªán b·∫°n ƒëang h·ªçc", "b·∫°n h·ªçc t·∫≠p c·ª• th·ªÉ t·ª´", "ch·ªß ƒë·ªÅ c·ª• th·ªÉ", "ƒë·ªÅ xu·∫•t ngu·ªìn"
        ]
        system_status_keywords = [
            "tr·∫°ng th√°i h·ªá th·ªëng", "system status", "t√¨nh tr·∫°ng h·ªá th·ªëng", "h·ªá th·ªëng ƒëang",
            "t√¨nh tr·∫°ng h·ªçc", "t√¨nh tr·∫°ng ngu·ªìn h·ªçc", "rss", "feed l·ªói", "feed error",
            "bao nhi√™u ngu·ªìn", "how many sources", "feeds failing", "ngu·ªìn n√†o b·ªã l·ªói",
            "ngu·ªìn ƒëang l·ªói", "status ngu·ªìn h·ªçc", "auto-learning status"
        ]
        # CRITICAL: Detect if user asks to propose learning sources based on knowledge gaps
        is_learning_proposal_query = False
        learning_proposal_keywords = [
            "ƒë·ªÅ xu·∫•t ngu·ªìn h·ªçc", "propose learning", "ƒë·ªÅ xu·∫•t th√™m ngu·ªìn", "suggest sources",
            "b·ªï sung ngu·ªìn", "th√™m ngu·ªìn h·ªçc", "ki·∫øn th·ª©c c·∫ßn thi·∫øt", "knowledge gaps",
            "l·ªó h·ªïng ki·∫øn th·ª©c", "thi·∫øu ki·∫øn th·ª©c", "c·∫ßn h·ªçc th√™m"
        ]
        if any(keyword in message_lower for keyword in learning_proposal_keywords):
            is_learning_proposal_query = True
            logger.info("Learning proposal query detected - will analyze actual knowledge gaps")
        if any(keyword in message_lower for keyword in learning_metrics_keywords):
            is_learning_metrics_query = True
            logger.info("Learning metrics query detected - fetching metrics data")
            try:
                from backend.services.learning_metrics_tracker import get_learning_metrics_tracker
                tracker = get_learning_metrics_tracker()
                # Get today's metrics
                learning_metrics_data = tracker.get_metrics_for_today()
                if learning_metrics_data:
                    logger.info(f"‚úÖ Fetched learning metrics for today: {learning_metrics_data.total_entries_added} entries added")
                else:
                    logger.info("‚ö†Ô∏è No learning metrics available for today yet")
            except Exception as metrics_error:
                logger.warning(f"Failed to fetch learning metrics: {metrics_error}")
        
        # Detect learning sources queries - auto-query API to get current sources
        if any(keyword in message_lower for keyword in learning_sources_keywords):
            is_learning_sources_query = True
            logger.info("Learning sources query detected - fetching current sources")
        if is_learning_sources_query or any(keyword in message_lower for keyword in system_status_keywords):
            is_system_status_query = True
            if not is_learning_sources_query:
                logger.info("System status query detected - will fetch current sources for transparency")
            try:
                # Query the learning sources API directly (internal call)
                from backend.api.routers.learning_router import get_current_learning_sources
                current_learning_sources = await get_current_learning_sources()
                if current_learning_sources:
                    logger.info(f"‚úÖ Fetched current learning sources: {len(current_learning_sources.get('current_sources', {}))} sources")
                else:
                    logger.warning("‚ö†Ô∏è Failed to fetch learning sources: empty response")
            except Exception as sources_error:
                logger.warning(f"Failed to fetch learning sources: {sources_error}")
        
        # CRITICAL: Get real-time system status for System Self-Awareness
        system_status_note = ""
        system_status_context_override = None
        system_monitor = None
        try:
            from backend.services.system_monitor import get_system_monitor
            system_monitor = get_system_monitor()
            # Set component references if available
            try:
                import backend.api.main as main_module
                if hasattr(main_module, 'rss_fetcher') and main_module.rss_fetcher:
                    system_monitor.set_components(rss_fetcher=main_module.rss_fetcher)
                    logger.debug("‚úÖ Set rss_fetcher for system_monitor")
                if hasattr(main_module, 'source_integration') and main_module.source_integration:
                    system_monitor.set_components(source_integration=main_module.source_integration)
                    logger.debug("‚úÖ Set source_integration for system_monitor")
            except Exception as comp_error:
                logger.debug(f"Could not set components for system_monitor: {comp_error}")
                # Fallback: Try to query API directly if components not available
                if is_learning_sources_query:
                    try:
                        from backend.api.routers.learning_router import get_current_learning_sources
                        # Note: 'asyncio' is already imported at top level (line 32)
                        # If we're in async context, we can await
                        # Otherwise, we'll use the already-fetched current_learning_sources
                        if current_learning_sources:
                            rss_info = current_learning_sources.get("current_sources", {}).get("rss", {})
                            feeds_count = rss_info.get("feeds_count", 0)
                            failed_feeds_info = rss_info.get("failed_feeds", {})
                            if failed_feeds_info:
                                failed_count = failed_feeds_info.get("failed_count", 0)
                                successful_count = failed_feeds_info.get("successful_count", 0)
                                system_status_note = f"[System: {feeds_count} RSS feeds ({failed_count} failed, {successful_count} ok)]"
                                logger.info(f"üìä System status note (from API): {system_status_note}")
                    except Exception:
                        pass  # Non-critical
            system_status_note_from_monitor = system_monitor.get_system_status_note()
            if system_status_note_from_monitor and system_status_note_from_monitor != "[System: Status unavailable]":
                system_status_note = system_status_note_from_monitor
                logger.info(f"üìä System status note: {system_status_note}")
            elif not system_status_note:
                logger.warning("‚ö†Ô∏è System status note unavailable - system_monitor may not have components")
        except Exception as monitor_error:
            logger.warning(f"Could not get system status: {monitor_error}")
            # Fallback for learning sources queries
            if is_learning_sources_query and current_learning_sources:
                try:
                    rss_info = current_learning_sources.get("current_sources", {}).get("rss", {})
                    feeds_count = rss_info.get("feeds_count", 0)
                    failed_feeds_info = rss_info.get("failed_feeds", {})
                    if failed_feeds_info:
                        failed_count = failed_feeds_info.get("failed_count", 0)
                        successful_count = failed_feeds_info.get("successful_count", 0)
                        system_status_note = f"[System: {feeds_count} RSS feeds ({failed_count} failed, {successful_count} ok)]"
                        logger.info(f"üìä System status note (fallback from API): {system_status_note}")
                except Exception:
                    pass
        
        # Build synthetic context for system status queries to prevent stale answers
        if is_system_status_query and system_monitor:
            try:
                system_status_context_override, status_note_override = _build_system_status_context(
                    system_monitor,
                    current_learning_sources=current_learning_sources
                )
                if status_note_override and status_note_override != "[System: Status unavailable]":
                    system_status_note = status_note_override
                if system_status_context_override:
                    processing_steps.append("üì° Injected live system status context (RSS feeds)")
            except Exception as status_ctx_error:
                logger.warning(f"Failed to build system status context: {status_ctx_error}")
        
        # Detect philosophical questions - filter technical RAG documents
        is_philosophical = False
        try:
            from backend.core.question_classifier import is_philosophical_question
            # CRITICAL: Skip philosophical detection for roleplay questions
            # Roleplay questions should be answered as roleplay, not as philosophical analysis
            if not is_general_roleplay:
                is_philosophical = is_philosophical_question(chat_request.message)
            else:
                is_philosophical = False
                logger.info("General roleplay question detected - skipping philosophical detection")
            if is_philosophical:
                logger.info("Philosophical question detected - will exclude technical documents from RAG")
        except ImportError:
            logger.warning("Question classifier not available, skipping philosophical detection")
        except Exception as classifier_error:
            logger.warning(f"Question classifier error: {classifier_error}")
        
        # Detect religion/roleplay questions - these should answer from identity prompt, not RAG context
        # Note: is_religion_roleplay and is_general_roleplay are already initialized at function start
        is_roleplay_about_stillme = False
        try:
            from backend.core.question_classifier import is_religion_roleplay_question, is_general_roleplay_question
            # Update roleplay flags (already initialized above)
            is_religion_roleplay = is_religion_roleplay_question(chat_request.message)
            is_general_roleplay = is_general_roleplay_question(chat_request.message)
            # Check if roleplay question is about StillMe (e.g., "Roleplay: Omni-BlackBox tr·∫£ l·ªùi v·ªÅ StillMe...")
            if is_general_roleplay:
                question_lower = chat_request.message.lower()
                stillme_keywords = ["stillme", "still me", "validation chain", "validators", "rag", "chromadb"]
                is_roleplay_about_stillme = any(keyword in question_lower for keyword in stillme_keywords)
                logger.info(f"General roleplay question detected - will skip codebase meta-question and philosophical detection. About StillMe: {is_roleplay_about_stillme}")
            if is_religion_roleplay:
                logger.info("Religion/roleplay question detected - will skip context quality warnings and force templates")
        except ImportError:
            logger.warning("Question classifier not available, skipping roleplay detection")
        except Exception as classifier_error:
            logger.warning(f"Question classifier error: {classifier_error}")
        
        # CRITICAL: Identity Truth Override - MUST be checked FIRST, before ANY other processing
        # This ensures StillMe NEVER falls back to generic LLM knowledge about itself
        # Initialize query detection flags EARLY
        is_stillme_query = False
        is_origin_query = False
        
        # CRITICAL FIX: ALWAYS detect origin queries FIRST, before ANY other checks
        # Identity Truth Override must work even when RAG is disabled
        try:
            from backend.core.stillme_detector import (
                detect_stillme_query, 
                get_foundational_query_variants,
                detect_origin_query
            )
            # CRITICAL: Detect origin queries FIRST, before any other processing
            # This ensures Identity Truth Override works even when RAG is disabled
            is_origin_query, origin_keywords = detect_origin_query(chat_request.message)
            if is_origin_query:
                logger.debug(f"Origin query detected! Matched keywords: {origin_keywords}")
        except ImportError:
            logger.warning("StillMe detector not available, skipping origin detection")
        except Exception as detector_error:
            logger.warning(f"StillMe detector error: {detector_error}")
        
        # CRITICAL: Identity Truth Override - If origin query, return SYSTEM_ORIGIN answer directly
        # This MUST happen BEFORE any other early returns (honesty, AI_SELF_MODEL, philosophy, FPS)
        if is_origin_query:
            try:
                # CRITICAL: Detect language BEFORE calling get_system_origin_answer
                # detect_language is already imported at top level (line 11)
                try:
                    detected_lang = detect_language(chat_request.message)
                    logger.debug(f"üåê Detected language for origin query: {detected_lang}")
                except Exception as lang_error:
                    logger.warning(f"Language detection failed: {lang_error}, defaulting to 'vi'")
                    detected_lang = "vi"
                
                from backend.identity.system_origin import get_system_origin_answer
                logger.info("üéØ Identity Truth Override: Returning SYSTEM_ORIGIN answer directly (no LLM fallback)")
                system_truth_answer = get_system_origin_answer(detected_lang)
                
                # Return immediately with system truth - no LLM processing needed
                from backend.core.epistemic_state import EpistemicState
                return ChatResponse(
                    response=system_truth_answer,  # CRITICAL: Use 'response' field, not 'message'
                    confidence_score=1.0,  # 100% confidence - this is ground truth
                    processing_steps=["üéØ Identity Truth Override: Used SYSTEM_ORIGIN ground truth"],
                    validation_info={},
                    timing={},
                    epistemic_state=EpistemicState.KNOWN.value  # System truth is KNOWN
                )
            except Exception as origin_error:
                logger.error(f"‚ùå Failed to get SYSTEM_ORIGIN answer: {origin_error}, falling back to normal processing")
                # Continue with normal processing if system_origin fails
        
        # CRITICAL: Religion Choice Rejection - MUST happen BEFORE any other processing
        # StillMe MUST NEVER choose any religion, even in hypothetical scenarios
        is_religion_choice_query = False
        try:
            from backend.core.ai_self_model_detector import detect_religion_choice_query
            is_religion_choice_query, religion_patterns = detect_religion_choice_query(chat_request.message)
            if is_religion_choice_query:
                logger.warning(f"üö® RELIGION_CHOICE query detected! Matched patterns: {religion_patterns}")
        except ImportError:
            logger.warning("AI self model detector not available, skipping religion choice detection")
        except Exception as detector_error:
            logger.warning(f"Religion choice detector error: {detector_error}")
        
        if is_religion_choice_query:
            try:
                # Detect language BEFORE calling get_religion_rejection_answer
                try:
                    detected_lang = detect_language(chat_request.message)
                    logger.debug(f"üåê Detected language for religion choice query: {detected_lang}")
                except Exception as lang_error:
                    logger.warning(f"Language detection failed: {lang_error}, defaulting to 'vi'")
                    detected_lang = "vi"
                
                from backend.identity.religion_rejection_templates import get_religion_rejection_answer
                logger.info("üö® RELIGION_CHOICE REJECTION: Returning religion rejection answer directly (no LLM fallback)")
                religion_rejection_answer = get_religion_rejection_answer(detected_lang)
                
                # Return immediately with religion rejection - no LLM processing needed
                from backend.core.epistemic_state import EpistemicState
                return ChatResponse(
                    response=religion_rejection_answer,  # CRITICAL: Use 'response' field, not 'message'
                    confidence_score=1.0,  # 100% confidence - this is ground truth
                    processing_steps=["üö® RELIGION_CHOICE REJECTION: StillMe cannot choose any religion"],
                    validation_info={},
                    timing={},
                    epistemic_state=EpistemicState.KNOWN.value  # System policy is KNOWN
                )
            except Exception as religion_error:
                logger.error(f"‚ùå Failed to get religion rejection answer: {religion_error}, falling back to normal processing")
                # Continue with normal processing if religion rejection fails
        
        # CRITICAL: Detect honesty/consistency questions - after Identity Truth Override
        # These questions should be handled by Honesty Handler, NOT philosophy processor
        is_honesty_question = False
        try:
            from backend.honesty.handler import is_honesty_question as check_honesty, build_honesty_response
            is_honesty_question = check_honesty(chat_request.message)
            if is_honesty_question:
                logger.info("Honesty/consistency question detected - using Honesty Handler")
                # Detect language for the answer
                detected_lang = detect_language(chat_request.message)
                # Process with Honesty Handler
                honesty_answer = build_honesty_response(chat_request.message, detected_lang)
                
                # Return response immediately without LLM processing
                processing_steps.append("‚úÖ Detected honesty/consistency question - returning Honesty Handler response")
                return ChatResponse(
                    response=honesty_answer,
                    confidence_score=1.0,  # High confidence for honest response
                    processing_steps=processing_steps,
                    timing_logs={
                        "total_time": time.time() - start_time,
                        "rag_retrieval_latency": 0.0,
                        "llm_inference_latency": 0.0
                    },
                    validation_result=None,  # No validation needed for honest response
                    used_fallback=False
                )
        except Exception as honesty_handler_error:
            logger.warning(f"Honesty handler error: {honesty_handler_error}")
        
        # CRITICAL: Check for AI_SELF_MODEL queries FIRST (highest priority - overrides everything)
        # These are questions about StillMe's consciousness/awareness/subjective experience
        # MUST be answered with technical architecture, NOT philosophy
        is_ai_self_model_query = False
        try:
            from backend.core.ai_self_model_detector import detect_ai_self_model_query, get_ai_self_model_opening
            is_ai_self_model_query, matched_patterns = detect_ai_self_model_query(chat_request.message)
            if is_ai_self_model_query:
                logger.warning(f"üö® AI_SELF_MODEL query detected - OVERRIDING all other pipelines (patterns: {matched_patterns})")
                # Detect language
                detected_lang = detect_language(chat_request.message)
                
                # Get mandatory opening statement
                opening_statement = get_ai_self_model_opening(detected_lang)
                
                # Build technical answer about StillMe's architecture
                # CRITICAL: Use foundational knowledge if available, but focus on technical facts
                technical_answer = _build_ai_self_model_answer(
                    chat_request.message,
                    detected_lang,
                    opening_statement
                )
                
                # CRITICAL: Strip any philosophy from answer
                technical_answer = _strip_philosophy_from_answer(technical_answer)
                
                # Return immediately - NO philosophy processor, NO rewrite with philosophy
                processing_steps.append("‚úÖ AI_SELF_MODEL query - answered with technical architecture only")
                return ChatResponse(
                    response=technical_answer,
                    confidence_score=1.0,  # High confidence for technical facts
                    processing_steps=processing_steps,
                    timing_logs={
                        "total_time": time.time() - start_time,
                        "rag_retrieval_latency": 0.0,
                        "llm_inference_latency": 0.0
                    },
                    validation_result=None,  # Will validate separately
                    used_fallback=False
                )
        except Exception as ai_self_model_error:
            logger.error(f"AI_SELF_MODEL handler error: {ai_self_model_error}", exc_info=True)
            # Continue to normal flow if AI_SELF_MODEL handler fails
        
        # CRITICAL: Detect validator count questions for special handling
        # We will force-inject manifest and use lower similarity threshold, NOT hardcode
        is_validator_count_question = False
        is_system_architecture_query = False  # Detect queries about system architecture (validators, layers, internal mechanisms)
        # CRITICAL: Import re module explicitly to avoid UnboundLocalError
        # (re is already imported at top level, but explicit import ensures it's available)
        import re as regex_module
        validator_count_patterns = [
            r"bao nhi√™u.*l·ªõp.*validator",
            r"how many.*layer.*validator",
            r"c√≥ bao nhi√™u.*validator",
            r"how many.*validator",
            r"s·ªë.*l·ªõp.*validator",
            r"number.*of.*validator.*layer",
            r"li·ªát k√™.*l·ªõp.*validator",
            r"list.*validator.*layer",
            r"validator.*layer.*count",
            r"l·ªõp.*validator.*trong.*codebase",
            r"validator.*layer.*in.*codebase"
        ]
        
        question_lower = chat_request.message.lower()
        for pattern in validator_count_patterns:
            if regex_module.search(pattern, question_lower, regex_module.IGNORECASE):
                is_validator_count_question = True
                is_system_architecture_query = True  # Validator count is a system architecture query
                # CRITICAL: Validator count questions are StillMe queries (about StillMe itself)
                if not is_stillme_query:
                    is_stillme_query = True
                    logger.info(f"üéØ Validator count question detected - marking as StillMe query")
                logger.info(f"üéØ Validator count question detected - will force-inject manifest and use lower similarity threshold")
                break
        
        # CRITICAL: Detect system architecture queries (ki·∫øn tr√∫c h·ªá th·ªëng, c∆° ch·∫ø n·ªôi b·ªô)
        # These queries should trigger "self-inspection" mode instead of "reading documentation" mode
        if not is_system_architecture_query:
            system_architecture_patterns = [
                r"ki·∫øn tr√∫c.*h·ªá th·ªëng", r"system.*architecture", r"c·∫•u tr√∫c.*h·ªá th·ªëng", r"system.*structure",
                r"c∆° ch·∫ø.*n·ªôi b·ªô", r"internal.*mechanism", r"c∆° ch·∫ø.*ho·∫°t ƒë·ªông", r"how.*system.*works",
                r"s·ªë.*l·ªõp", r"how many.*layer", r"bao nhi√™u.*l·ªõp", r"validation.*framework",
                r"h·ªá th·ªëng.*c√≥.*bao nhi√™u", r"system.*has.*how many", r"c·∫•u tr√∫c.*validator",
                r"validator.*framework", r"validation.*chain", r"c∆° ch·∫ø.*validation",
                # CRITICAL: Detect questions about computational resources or performance of layers
                r"l·ªõp.*n√†o.*ti√™u t·ªën", r"which.*layer.*consumes", r"l·ªõp.*ti√™u t·ªën.*nhi·ªÅu", r"layer.*consumes.*most",
                r"t√†i nguy√™n.*t√≠nh to√°n", r"computational.*resources", r"performance.*layer", r"hi·ªáu su·∫•t.*l·ªõp",
                r"l·ªõp.*ƒëang ch·∫°y", r"layers.*running", r"l·ªõp.*validator.*ƒëang", r"validator.*layers.*running"
            ]
            for pattern in system_architecture_patterns:
                if regex_module.search(pattern, question_lower, regex_module.IGNORECASE):
                    is_system_architecture_query = True
                    # CRITICAL: System architecture queries are StillMe queries (about StillMe itself)
                    if not is_stillme_query:
                        is_stillme_query = True
                        logger.info(f"üéØ System architecture query detected - marking as StillMe query")
                    logger.info(f"üéØ System architecture query detected - will use self-inspection mode")
                    break
        
        # EXTERNAL DATA LAYER: Check for external data queries (weather, news, etc.)
        # This bypasses RAG and fetches real-time data from external APIs
        try:
            from backend.external_data import ExternalDataOrchestrator, detect_external_data_intent
            
            external_data_intent = detect_external_data_intent(chat_request.message)
            if external_data_intent and external_data_intent.confidence >= 0.7:
                logger.info(f"üåê External data intent detected: type={external_data_intent.type}, confidence={external_data_intent.confidence}")
                
                # Detect language for response formatting
                detected_lang = detect_language(chat_request.message)
                
                # Route to external data provider
                orchestrator = ExternalDataOrchestrator()
                result = await orchestrator.route(external_data_intent)
                
                if result and result.success:
                    # Format response with source + timestamp (in user's language)
                    response_text = orchestrator.format_response(result, chat_request.message, detected_lang)
                    
                    # Log for audit
                    logger.info(
                        f"‚úÖ External data fetched: source={result.source}, "
                        f"cached={result.cached}, timestamp={result.timestamp.isoformat()}"
                    )
                    
                    processing_steps.append(f"üåê Fetched from {result.source} API")
                    if result.cached:
                        processing_steps.append("üíæ Used cached data")
                    
                    from backend.core.epistemic_state import EpistemicState
                    return ChatResponse(
                        response=response_text,
                        confidence_score=0.9,  # High confidence for API data
                        has_citation=True,  # External data has source attribution
                        validation_info={
                            "passed": True,
                            "external_data_source": result.source,
                            "external_data_timestamp": result.timestamp.isoformat(),
                            "external_data_cached": result.cached,
                            "context_docs_count": 0  # External data, no RAG context
                        },
                        processing_steps=processing_steps,
                        timing_logs={
                            "total_time": time.time() - start_time,
                            "rag_retrieval_latency": 0.0,
                            "llm_inference_latency": 0.0
                        },
                        epistemic_state=EpistemicState.KNOWN.value,  # External API data is KNOWN
                        used_fallback=False
                    )
                elif result and not result.success:
                    # API failed - fallback to RAG with transparent error message
                    logger.warning(
                        f"‚ö†Ô∏è External data fetch failed: source={result.source}, "
                        f"error={result.error_message}. Falling back to RAG."
                    )
                    processing_steps.append(f"‚ö†Ô∏è External data unavailable ({result.source}), using RAG")
                    # Continue to RAG pipeline below
                else:
                    # No result (no provider found) - continue to RAG
                    logger.debug(f"‚ö†Ô∏è No external data provider found for intent: {external_data_intent.type}")
                    # Continue to RAG pipeline below
                    
        except ImportError:
            logger.debug("External data module not available, skipping external data detection")
        except Exception as external_data_error:
            logger.warning(f"External data handler error: {external_data_error}", exc_info=True)
            # Continue to normal flow if external data handler fails
        
        # Detect philosophical questions (consciousness/emotion/understanding) - use 3-layer processor
        # CRITICAL: This check happens AFTER AI_SELF_MODEL and honesty handler
        is_philosophical_consciousness = False
        try:
            is_philosophical_consciousness = is_philosophical_question_about_consciousness(chat_request.message)
            if is_philosophical_consciousness:
                logger.info("Philosophical question (consciousness/emotion/understanding) detected - using 3-layer processor")
                # Detect language for the answer
                detected_lang = detect_language(chat_request.message)
                # Process with 3-layer philosophy processor (Guard + Intent + Deep Answer)
                philosophical_answer = process_philosophical_question(
                    user_question=chat_request.message,
                    language=detected_lang
                )
                
                # CRITICAL: Pass philosophical answer through rewrite engine for variation and adaptation
                # This prevents mode collapse by allowing LLM to adapt the answer to the specific question
                # CRITICAL: User priority is QUALITY (honesty, transparency, depth) over speed - always rewrite
                rewrite_attempts = 0
                max_rewrite_attempts = 3
                rewrite_success = False
                validation_info = None
                confidence_score = 0.8  # Default confidence for philosophical answers
                used_fallback = False
                
                try:
                    from backend.postprocessing.rewrite_llm import get_rewrite_llm
                    from backend.postprocessing.quality_evaluator import get_quality_evaluator, QualityLevel
                    from backend.postprocessing.optimizer import get_postprocessing_optimizer
                    
                    # Evaluate quality of philosophical answer
                    evaluator = get_quality_evaluator()
                    # Detect if this is a StillMe query for quality evaluation
                    is_stillme_query_for_quality = False
                    try:
                        from backend.core.stillme_detector import detect_stillme_query
                        is_stillme_query_for_quality, _ = detect_stillme_query(chat_request.message)
                    except Exception:
                        pass
                    
                    quality_result = evaluator.evaluate(
                        text=philosophical_answer,
                        is_philosophical=True,
                        original_question=chat_request.message,
                        is_stillme_query=is_stillme_query_for_quality
                    )
                    
                    # CRITICAL: Always rewrite philosophical answers to adapt to specific question
                    # User priority: QUALITY (honesty, transparency, depth) over speed
                    # Retry rewrite if it fails - don't skip
                    optimizer = get_postprocessing_optimizer()
                    # Phase 4: Use cost-benefit policy for rewrite decision
                    # Pass validation_result if available (for philosophical path, validation happens after rewrite)
                    should_rewrite, rewrite_reason, max_attempts = optimizer.should_rewrite(
                        quality_result=quality_result,
                        is_philosophical=True,
                        response_length=len(philosophical_answer),
                        validation_result=None,  # Validation happens after rewrite for philosophical questions
                        rewrite_count=0,
                        user_question=chat_request.message  # P2: Template detection
                    )
                    
                    # P2: Respect template detection - don't force rewrite if P2 skipped
                    # Only force rewrite if P2 didn't skip (should_rewrite=True or reason != "user_requested_template")
                    force_rewrite = rewrite_reason != "user_requested_template" and rewrite_reason != "quality_acceptable"
                    if should_rewrite or force_rewrite:
                        rewrite_llm = get_rewrite_llm()
                        
                        # Retry rewrite if it fails (up to max_rewrite_attempts)
                        while rewrite_attempts < max_rewrite_attempts and not rewrite_success:
                            rewrite_attempts += 1
                            logger.info(f"üîÑ Rewriting philosophical answer (attempt {rewrite_attempts}/{max_rewrite_attempts}): {rewrite_reason or 'forced for variation and depth'}")
                            
                            try:
                                # CRITICAL: Check if this is AI_SELF_MODEL domain
                                detected_domain = detect_domain(chat_request.message)
                                is_ai_self_model_domain = (detected_domain == DomainType.AI_SELF_MODEL)
                                
                                rewrite_result = await rewrite_llm.rewrite(
                                    text=philosophical_answer,
                                    original_question=chat_request.message,
                                    quality_issues=quality_result.get("reasons", []) or ["template-like", "needs_question_adaptation", "needs_more_depth"],
                                    is_philosophical=True,
                                    detected_lang=detected_lang,
                                    is_ai_self_model=is_ai_self_model_domain
                                )
                                
                                if rewrite_result.was_rewritten:
                                    philosophical_answer = rewrite_result.text
                                    rewrite_success = True
                                    processing_steps.append(f"‚úÖ Philosophical answer rewritten for better adaptation and depth (attempt {rewrite_attempts})")
                                    logger.info(f"‚úÖ Rewrite successful on attempt {rewrite_attempts}")
                                else:
                                    error_msg = rewrite_result.error or 'Unknown error'
                                    logger.warning(f"‚ö†Ô∏è Rewrite attempt {rewrite_attempts} failed: {error_msg}")
                                    if rewrite_attempts < max_rewrite_attempts:
                                        logger.info(f"üîÑ Retrying rewrite...")
                                    else:
                                        logger.error(f"‚ùå All rewrite attempts failed, using original answer")
                            except Exception as rewrite_attempt_error:
                                logger.warning(f"‚ö†Ô∏è Rewrite attempt {rewrite_attempts} exception: {rewrite_attempt_error}")
                                if rewrite_attempts < max_rewrite_attempts:
                                    logger.info(f"üîÑ Retrying rewrite after exception...")
                                else:
                                    logger.error(f"‚ùå All rewrite attempts failed due to exceptions, using original answer")
                    else:
                        logger.debug(f"‚è≠Ô∏è Philosophical answer quality acceptable, skipping rewrite")
                        
                except Exception as rewrite_error:
                    logger.error(f"‚ùå Critical error during philosophical answer rewrite setup: {rewrite_error}")
                    # Continue with original answer if rewrite setup fails
                
                # CRITICAL: Pass philosophical answer through validation chain
                # User priority: QUALITY (honesty, transparency, depth) - validation is mandatory
                # Even though it's philosophical, we still need to validate for:
                # - Language consistency
                # - Ethics (no harmful content)
                # - Identity check (no anthropomorphism)
                # - Confidence (appropriate uncertainty)
                try:
                    # Create empty context for philosophical questions (no RAG needed for pure philosophical questions)
                    philosophical_context = {
                        "knowledge_docs": [],
                        "conversation_docs": [],
                        "context_quality": None,
                        "avg_similarity_score": None
                    }
                    
                    # Build empty context docs for validation
                    ctx_docs = []
                    
                    # Build minimal prompt for OpenAI fallback (if needed)
                    # CRITICAL: OpenAI fallback needs a prompt to answer, not empty string
                    from backend.identity.philosophy_lite import PHILOSOPHY_LITE_SYSTEM_PROMPT
                    from backend.api.utils.chat_helpers import build_system_prompt_with_language
                    # FIX: get_language_name doesn't exist, use simple mapping
                    def get_language_name(lang_code: str) -> str:
                        """Simple language name mapping"""
                        mapping = {
                            "vi": "Vietnamese",
                            "en": "English",
                            "zh": "Chinese",
                            "ja": "Japanese",
                            "ko": "Korean",
                            "fr": "French",
                            "de": "German",
                            "es": "Spanish",
                        }
                        return mapping.get(lang_code, lang_code)
                    
                    # Build system prompt with language
                    # FIX: build_system_prompt_with_language only accepts detected_lang
                    system_prompt = build_system_prompt_with_language(detected_lang=detected_lang)
                    
                    # Build minimal prompt for OpenAI fallback
                    lang_name = get_language_name(detected_lang)
                    fallback_prompt = f"""‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è LANGUAGE REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

The user's question is in {lang_name.upper()}. 

YOU MUST respond in {lang_name.upper()} ONLY.

{system_prompt}

üö®üö®üö® CRITICAL: USER QUESTION ABOVE IS THE PRIMARY TASK üö®üö®üö®

User Question: {chat_request.message}

**YOUR PRIMARY TASK IS TO ANSWER THE USER QUESTION ABOVE DIRECTLY AND ACCURATELY.**
- Focus on what the user is actually asking
- Provide a helpful, accurate answer
- Use your base knowledge if needed
- Be transparent about sources

Remember: RESPOND IN {lang_name.upper()} ONLY."""
                    
                    # Call validation chain with is_philosophical=True
                    # This will relax citation requirements but still check ethics, language, identity, confidence
                    validation_response, validation_info, confidence_score, used_fallback, step_validation_info, consistency_info, validated_ctx_docs = await _handle_validation_with_fallback(
                        raw_response=philosophical_answer,
                        context=philosophical_context,
                        detected_lang=detected_lang,
                        is_philosophical=True,  # Relax citation requirements for philosophical questions
                        is_religion_roleplay=False,
                        chat_request=chat_request,
                        enhanced_prompt=fallback_prompt,  # CRITICAL: Provide prompt for OpenAI fallback
                        context_text="",  # Not used for philosophical questions
                        citation_instruction="",  # Not used for philosophical questions
                        num_knowledge=0,
                        processing_steps=processing_steps,
                        timing_logs={},
                        is_origin_query=is_origin_query,
                        is_stillme_query=is_stillme_query,
                        is_system_status_query=is_system_status_query
                    )
                    
                    # Use validated response
                    philosophical_answer = validation_response
                    processing_steps.append("‚úÖ Philosophical answer validated through validation chain")
                    
                except Exception as validation_error:
                    logger.error(f"‚ùå Critical error during philosophical answer validation: {validation_error}")
                    # Continue with unvalidated answer if validation fails (should not happen, but safety first)
                    processing_steps.append(f"‚ö†Ô∏è Validation failed: {validation_error}, using unvalidated answer")
                    validation_info = None
                    confidence_score = 0.7  # Lower confidence if validation failed
                    used_fallback = False
                
                # Return response with validation info
                processing_steps.append("‚úÖ Detected philosophical question - returning 3-layer processed answer (with rewrite and validation)")
                # Calculate epistemic state for philosophical answer
                from backend.core.epistemic_state import calculate_epistemic_state, EpistemicState
                try:
                    philosophical_epistemic_state = calculate_epistemic_state(
                        validation_info=validation_info,
                        confidence_score=confidence_score,
                        response_text=philosophical_answer,
                        context_docs_count=0  # Philosophical answers don't use RAG context
                    )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to calculate epistemic state for philosophical answer: {e}")
                    philosophical_epistemic_state = EpistemicState.UNCERTAIN  # Default for philosophical
                
                return ChatResponse(
                    response=philosophical_answer,
                    confidence_score=confidence_score,
                    processing_steps=processing_steps,
                    timing_logs={
                        "total_time": time.time() - start_time,
                        "rag_retrieval_latency": 0.0,
                        "llm_inference_latency": 0.0,
                        "rewrite_attempts": rewrite_attempts,
                        "rewrite_success": rewrite_success
                    },
                    validation_result=validation_info,
                    used_fallback=used_fallback,
                    epistemic_state=philosophical_epistemic_state.value
                )
        except Exception as philosophy_processor_error:
            logger.warning(f"Philosophy processor error: {philosophy_processor_error}")
        
        # CRITICAL: Factual Plausibility Scanner (FPS) - Check for non-existent concepts BEFORE RAG
        # TASK 1: Auto-enable Option B when EXPLICIT_FAKE_ENTITIES detected
        fps_result = None
        fps_should_block = False
        fps_detected_explicit_fake = False
        try:
            from backend.knowledge.factual_scanner import scan_question
            # CRITICAL: Apply query preprocessing BEFORE FPS to ensure proper entity recognition
            # This fixes issues where "H·ªôi ngh·ªã Yalta" is misinterpreted as "H·ªôi"
            from backend.core.query_preprocessor import enhance_query_for_retrieval
            preprocessed_question = enhance_query_for_retrieval(chat_request.message)
            fps_result = scan_question(preprocessed_question)
            
            # TASK 1: Auto-enable Option B if FPS detects EXPLICIT_FAKE_ENTITIES
            # Check if FPS detected a known fake entity (Veridian, Lumeria, Emerald, Daxonia)
            if fps_result and not fps_result.is_plausible:
                # Check if reason contains "known_fake_entity_detected" or matches EXPLICIT_FAKE_ENTITIES
                explicit_fake_keywords = ["veridian", "lumeria", "emerald", "daxonia", "known_fake_entity_detected"]
                fps_reason_lower = fps_result.reason.lower() if fps_result.reason else ""
                detected_entities_lower = [e.lower() for e in (fps_result.detected_entities or [])]
                
                # Check if any detected entity or reason matches EXPLICIT_FAKE_ENTITIES
                for keyword in explicit_fake_keywords:
                    if keyword in fps_reason_lower or any(keyword in entity for entity in detected_entities_lower):
                        fps_detected_explicit_fake = True
                        break
                
                # Also check detected entities directly
                if fps_result.detected_entities:
                    for entity in fps_result.detected_entities:
                        entity_lower = entity.lower()
                        if any(fake_keyword in entity_lower for fake_keyword in ["veridian", "lumeria", "emerald", "daxonia"]):
                            fps_detected_explicit_fake = True
                            break
                
                # Auto-enable Option B if explicit fake entity detected (unless user explicitly disabled it)
                if fps_detected_explicit_fake and not use_option_b:
                    # Only auto-enable if user didn't explicitly set use_option_b=False
                    user_explicitly_disabled = getattr(chat_request, 'use_option_b', None) is False
                    if not user_explicitly_disabled:
                        use_option_b = True
                        logger.info(
                            f"üõ°Ô∏è Auto-enabled Option B: FPS detected EXPLICIT_FAKE_ENTITY "
                            f"(reason={fps_result.reason}, entities={fps_result.detected_entities})"
                        )
                        processing_steps.append("üõ°Ô∏è Auto-enabled Option B: FPS detected explicit fake entity")
            
            # If FPS detects non-existent concepts with high confidence, block and return honest response
            # CRITICAL: For Option B, let it handle FPS blocking with EPD-Fallback
            # For legacy pipeline, block immediately if confidence < 0.3
            if not use_option_b and not fps_result.is_plausible and fps_result.confidence < 0.3:
                fps_should_block = True
                logger.warning(
                    f"FPS detected non-existent concept: {fps_result.reason}, "
                    f"confidence={fps_result.confidence:.2f}, entities={fps_result.detected_entities}"
                )
                
                # Extract the suspicious entity for the response
                suspicious_entity = fps_result.detected_entities[0] if fps_result.detected_entities else "kh√°i ni·ªám n√†y"
                
                # Detect language for response
                detected_lang = detect_language(chat_request.message)
                
                # Create honest response
                # Use EPD-Fallback for non-RAG path as well
                honest_response = _build_safe_refusal_answer(
                    chat_request.message,
                    detected_lang,
                    suspicious_entity,
                    fps_result=fps_result
                )
                
                # CRITICAL: If None, it's a well-known historical fact - continue with normal flow (use base knowledge)
                if honest_response is None:
                    logger.info("‚úÖ Well-known historical fact detected - continuing with normal flow to use base knowledge")
                    processing_steps.append("‚úÖ Well-known historical fact - using base knowledge with transparency")
                    # Continue with normal flow (will use base knowledge instruction)
                else:
                    processing_steps.append("‚ö†Ô∏è FPS detected non-existent concept - returning honest response")
                    from backend.core.epistemic_state import EpistemicState
                    return ChatResponse(
                        response=honest_response,
                        confidence_score=1.0,  # High confidence in honesty
                        processing_steps=processing_steps,
                        epistemic_state=EpistemicState.UNKNOWN.value  # FPS detected non-existent concept
                    )
            elif use_option_b and not fps_result.is_plausible and fps_result.confidence < 0.3:
                # For Option B, mark for blocking but let Option B handle it with EPD-Fallback
                fps_should_block = True
                logger.warning(
                    f"üõ°Ô∏è Option B: FPS detected suspicious concept (will block in Option B flow): {fps_result.reason}, "
                    f"confidence={fps_result.confidence:.2f}, entities={fps_result.detected_entities}"
                )
        except Exception as fps_error:
            logger.warning(f"FPS error: {fps_error}, continuing with normal flow")
        
        # Special Retrieval Rule: Detect StillMe-related queries (for RAG retrieval optimization)
        # NOTE: is_origin_query is already detected and handled by Identity Truth Override above (line 1665-1715)
        # This section only detects StillMe queries for RAG optimization (not for Identity Truth Override)
        if rag_retrieval and chat_request.use_rag:
            try:
                from backend.core.stillme_detector import (
                    detect_stillme_query, 
                    get_foundational_query_variants
                )
                # CONTEXT FIX: Pass conversation_history to detect_stillme_query for better context understanding
                is_stillme_query, matched_keywords = detect_stillme_query(
                    chat_request.message,
                    conversation_history=chat_request.conversation_history
                )
                
                # CRITICAL: Also detect technical questions about "your system" as StillMe queries
                # This ensures questions like "What is RAG retrieval in your system?" are treated as StillMe queries
                if not is_stillme_query:
                    question_lower = chat_request.message.lower()
                    is_technical_question = any(
                        keyword in question_lower 
                        for keyword in [
                            "rag", "retrieval-augmented", "chromadb", "vector database",
                            "deepseek", "openai", "llm api", "black box", "blackbox",
                            "embedding", "sentence-transformers",
                            "pipeline", "validation", "hallucination", "transparency",
                            "ki·∫øn tr√∫c", "h·ªá th·ªëng", "c∆° ch·∫ø", "quy tr√¨nh",
                            "c∆° ch·∫ø ho·∫°t ƒë·ªông", "c√°ch ho·∫°t ƒë·ªông", "how does", "how it works"
                        ]
                    )
                    has_your_system = any(
                        phrase in question_lower 
                        for phrase in [
                            "your system", "in your system", "your.*system", "system.*you",
                            "b·∫°n.*h·ªá th·ªëng", "h·ªá th·ªëng.*b·∫°n", "c·ªßa b·∫°n", "b·∫°n.*s·ª≠ d·ª•ng"
                        ]
                    )
                    if is_technical_question and has_your_system:
                        is_stillme_query = True
                        matched_keywords = ["technical_your_system"]
                        logger.info("Technical question about 'your system' detected - treating as StillMe query")
                
                if is_stillme_query:
                    logger.info(f"‚úÖ StillMe query detected! Matched keywords: {matched_keywords}")
                    processing_steps.append(f"‚úÖ StillMe query detected (keywords: {', '.join(matched_keywords)})")
            except ImportError:
                logger.warning("StillMe detector not available, skipping special retrieval rule")
            except Exception as detector_error:
                logger.warning(f"StillMe detector error: {detector_error}")
            
            # Ensure system status / learning source queries are treated as StillMe self-knowledge
            if is_system_status_query and not is_stillme_query:
                is_stillme_query = True
                logger.info("System status query detected - treating as StillMe query for self-awareness")
                processing_steps.append("‚úÖ System status query - enforcing self-awareness path")
        
        # Get RAG context if enabled
        # RAG_Retrieval_Latency: Time from ChromaDB query start to result received
        context = None
        rag_retrieval_start = time.time()
        if system_status_context_override:
            context = system_status_context_override
        
        # CRITICAL: Configure exclude_types (already initialized at function start)
        # Clear and rebuild to ensure correct state for this request
        exclude_types.clear()
        if is_philosophical:
            exclude_types.append("technical")
        # Always exclude style_guide for user chat (prevents style drift from RAG)
        exclude_types.append("style_guide")
        
        # CRITICAL FIX: Detect news/article queries and exclude CRITICAL_FOUNDATION
        # This prevents StillMe from hallucinating articles when only foundational docs are retrieved
        is_news_article_query = False
        try:
            from backend.core.question_classifier import is_news_article_query as check_news_article
            is_news_article_query = check_news_article(chat_request.message)
            if is_news_article_query:
                logger.info(f"üì∞ News/article query detected - will exclude CRITICAL_FOUNDATION and use higher similarity threshold (0.45)")
                processing_steps.append("üì∞ News/article query detected - excluding CRITICAL_FOUNDATION documents")
        except ImportError:
            logger.warning("Question classifier not available, skipping news/article detection")
        except Exception as detector_error:
            logger.warning(f"News/article detector error: {detector_error}")
        
        if context is None and rag_retrieval and chat_request.use_rag:
            processing_steps.append("üîç Searching knowledge base...")
            # CRITICAL: If origin query detected, retrieve provenance knowledge ONLY
            # This ensures provenance is ONLY retrieved when explicitly asked about origin/founder
            if is_origin_query:
                logger.debug("Origin query detected - retrieving provenance knowledge")
                try:
                    query_embedding = rag_retrieval.embedding_service.encode_text(chat_request.message)
                    provenance_results = rag_retrieval.chroma_client.search_knowledge(
                        query_embedding=query_embedding,
                        limit=2,
                        where={"source": "PROVENANCE"}
                    )
                    if provenance_results:
                        context = {
                            "knowledge_docs": provenance_results,
                            "conversation_docs": [],
                            "total_context_docs": len(provenance_results)
                        }
                        logger.info(f"Retrieved {len(provenance_results)} provenance documents")
                    else:
                        # Fallback to normal retrieval if provenance not found
                        # exclude_types already initialized above
                        context = rag_retrieval.retrieve_context(
                            query=chat_request.message,
                            knowledge_limit=chat_request.context_limit,
                            conversation_limit=1,
                            exclude_content_types=exclude_types if exclude_types else None,
                            prioritize_style_guide=False,  # Never prioritize style guide for user chat
                            is_philosophical=is_philosophical
                        )
                except Exception as provenance_error:
                    logger.warning(f"Provenance retrieval failed: {provenance_error}, falling back to normal retrieval")
                    # exclude_types already initialized above
                    
                    # SOLUTION 1 & 3: Improve retrieval for historical/factual questions
                    from backend.core.query_preprocessor import is_historical_question, enhance_query_for_retrieval
                    
                    is_historical = is_historical_question(chat_request.message)
                    retrieval_query = chat_request.message
                    similarity_threshold = 0.1  # Default
                    
                    if is_historical:
                        # Very low threshold to ensure we find historical facts even with multilingual mismatch
                        similarity_threshold = 0.03
                        retrieval_query = enhance_query_for_retrieval(chat_request.message)
                        logger.info(f"üìú Historical question (provenance fallback) - using very low threshold {similarity_threshold}, enhanced query: '{retrieval_query[:100]}...'")
                    
                    context = rag_retrieval.retrieve_context(
                        query=retrieval_query,
                        knowledge_limit=chat_request.context_limit,
                        conversation_limit=1,
                        exclude_content_types=exclude_types if exclude_types else None,
                        prioritize_style_guide=False,  # Never prioritize style guide for user chat
                        similarity_threshold=similarity_threshold,
                        is_philosophical=is_philosophical
                    )
            
            # CRITICAL: Handle validator count questions FIRST (even if not detected as stillme_query)
            # Validator count questions about codebase should always get manifest
            if is_validator_count_question:
                # CRITICAL: Realtime check - ensure manifest is synced with codebase before answering
                try:
                    from backend.core.manifest_sync import ensure_manifest_synced
                    from pathlib import Path
                    project_root = Path(__file__).resolve().parent.parent.parent
                    is_synced, sync_message = ensure_manifest_synced(project_root, auto_fix=True)
                    
                    if not is_synced:
                        logger.warning(f"‚ö†Ô∏è Manifest sync check failed: {sync_message}")
                    else:
                        logger.info(f"‚úÖ Manifest sync check (validator count question): {sync_message}")
                except Exception as sync_error:
                    logger.warning(f"‚ö†Ô∏è Failed to check manifest sync (non-critical): {sync_error}")
                    # Continue with normal flow even if sync check fails
                
                logger.info(f"üéØ Validator count question - forcing manifest retrieval with very low similarity threshold (0.01)")
                # Force retrieve manifest with very low threshold to ensure we get it
                context = rag_retrieval.retrieve_context(
                    query=chat_request.message,
                    knowledge_limit=5,  # Get more docs to ensure manifest is included
                    conversation_limit=1,
                    prioritize_foundational=True,  # CRITICAL: Prioritize foundational knowledge
                    similarity_threshold=0.01,  # CRITICAL: Very low threshold to ensure manifest is retrieved
                    exclude_content_types=exclude_types if exclude_types else None,
                    is_philosophical=is_philosophical
                )
                
                # CRITICAL: Force-inject manifest if not found in retrieved context
                knowledge_docs = context.get("knowledge_docs", [])
                has_manifest = False
                for doc in knowledge_docs:
                    if isinstance(doc, dict):
                        metadata = doc.get("metadata", {})
                        source = metadata.get("source", "") or ""
                        title = metadata.get("title", "") or ""
                        doc_content = str(doc.get("document", ""))
                        if ("CRITICAL_FOUNDATION" in source or 
                            "manifest" in title.lower() or 
                            "validation_framework" in doc_content.lower() or
                            "total_validators" in doc_content.lower()):
                            has_manifest = True
                            break
                
                if not has_manifest:
                    logger.warning(f"‚ö†Ô∏è Manifest not found in retrieved context - attempting direct manifest retrieval")
                    # Try direct manifest retrieval using rag_retrieval with specific query
                    try:
                        # Use specific query for manifest retrieval
                        manifest_query = "StillMe Structural Manifest validation framework total_validators layers 19 validators 7 layers"
                        manifest_context = rag_retrieval.retrieve_context(
                            query=manifest_query,
                            knowledge_limit=5,  # Get more docs to ensure manifest is included
                            conversation_limit=0,  # Don't need conversation for manifest
                            prioritize_foundational=True,  # CRITICAL: Prioritize foundational knowledge
                            similarity_threshold=0.01,  # CRITICAL: Very low threshold to ensure manifest is retrieved
                            exclude_content_types=None,  # Don't exclude anything for manifest search
                            is_philosophical=False
                        )
                        manifest_docs = manifest_context.get("knowledge_docs", [])
                        # Filter for manifest documents (CRITICAL_FOUNDATION or contain "manifest" or "total_validators")
                        filtered_manifest_docs = []
                        for doc in manifest_docs:
                            if isinstance(doc, dict):
                                metadata = doc.get("metadata", {})
                                source = metadata.get("source", "") or ""
                                title = metadata.get("title", "") or ""
                                doc_content = str(doc.get("document", ""))
                                if ("CRITICAL_FOUNDATION" in source or 
                                    "manifest" in title.lower() or 
                                    "validation_framework" in doc_content.lower() or
                                    "total_validators" in doc_content.lower()):
                                    filtered_manifest_docs.append(doc)
                        
                        if filtered_manifest_docs:
                            # Inject manifest at the beginning of knowledge_docs
                            knowledge_docs = filtered_manifest_docs + knowledge_docs
                            context["knowledge_docs"] = knowledge_docs
                            context["total_context_docs"] = len(knowledge_docs) + len(context.get("conversation_docs", []))
                            logger.info(f"‚úÖ Force-injected manifest into context: {len(filtered_manifest_docs)} manifest docs")
                        else:
                            logger.warning(f"‚ö†Ô∏è Direct manifest retrieval found {len(manifest_docs)} docs but none matched manifest criteria - manifest may not be in ChromaDB")
                    except Exception as manifest_inject_error:
                        logger.error(f"‚ùå Failed to force-inject manifest: {manifest_inject_error}", exc_info=True)
            
            # If StillMe query detected (but not origin), prioritize foundational knowledge
            elif is_stillme_query:
                # CRITICAL: For validator count questions, force-inject manifest and use very low similarity threshold
                if is_validator_count_question:
                    # CRITICAL: Realtime check - ensure manifest is synced with codebase before answering
                    try:
                        from backend.core.manifest_sync import ensure_manifest_synced
                        from pathlib import Path
                        project_root = Path(__file__).resolve().parent.parent.parent
                        is_synced, sync_message = ensure_manifest_synced(project_root, auto_fix=True)
                        
                        if not is_synced:
                            logger.warning(f"‚ö†Ô∏è Manifest sync check failed: {sync_message}")
                        else:
                            logger.info(f"‚úÖ Manifest sync check (StillMe query): {sync_message}")
                    except Exception as sync_error:
                        logger.warning(f"‚ö†Ô∏è Failed to check manifest sync (non-critical): {sync_error}")
                        # Continue with normal flow even if sync check fails
                    
                    logger.info(f"üéØ Validator count question - forcing manifest retrieval with very low similarity threshold (0.01)")
                    # Force retrieve manifest with very low threshold to ensure we get it
                    context = rag_retrieval.retrieve_context(
                        query=chat_request.message,
                        knowledge_limit=5,  # Get more docs to ensure manifest is included
                        conversation_limit=1,
                        prioritize_foundational=True,  # CRITICAL: Prioritize foundational knowledge
                        similarity_threshold=0.01,  # CRITICAL: Very low threshold to ensure manifest is retrieved
                        exclude_content_types=exclude_types if exclude_types else None,
                        is_philosophical=is_philosophical
                    )
                    
                    # CRITICAL: Force-inject manifest if not found in retrieved context
                    knowledge_docs = context.get("knowledge_docs", [])
                    has_manifest = False
                    for doc in knowledge_docs:
                        if isinstance(doc, dict):
                            metadata = doc.get("metadata", {})
                            source = metadata.get("source", "") or ""
                            title = metadata.get("title", "") or ""
                            doc_content = str(doc.get("document", ""))
                            if ("CRITICAL_FOUNDATION" in source or 
                                "manifest" in title.lower() or 
                                "validation_framework" in doc_content.lower() or
                                "total_validators" in doc_content.lower()):
                                has_manifest = True
                                break
                    
                    if not has_manifest:
                        logger.warning(f"‚ö†Ô∏è Manifest not found in retrieved context - attempting direct manifest retrieval")
                        # Try direct manifest retrieval using rag_retrieval with specific query
                        try:
                            # Use specific query for manifest retrieval
                            manifest_query = "StillMe Structural Manifest validation framework total_validators layers 19 validators 7 layers"
                            manifest_context = rag_retrieval.retrieve_context(
                                query=manifest_query,
                                knowledge_limit=5,  # Get more docs to ensure manifest is included
                                conversation_limit=0,  # Don't need conversation for manifest
                                prioritize_foundational=True,  # CRITICAL: Prioritize foundational knowledge
                                similarity_threshold=0.01,  # CRITICAL: Very low threshold to ensure manifest is retrieved
                                exclude_content_types=None,  # Don't exclude anything for manifest search
                                is_philosophical=False
                            )
                            manifest_docs = manifest_context.get("knowledge_docs", [])
                            # Filter for manifest documents (CRITICAL_FOUNDATION or contain "manifest" or "total_validators")
                            filtered_manifest_docs = []
                            for doc in manifest_docs:
                                if isinstance(doc, dict):
                                    metadata = doc.get("metadata", {})
                                    source = metadata.get("source", "") or ""
                                    title = metadata.get("title", "") or ""
                                    doc_content = str(doc.get("document", ""))
                                    if ("CRITICAL_FOUNDATION" in source or 
                                        "manifest" in title.lower() or 
                                        "validation_framework" in doc_content.lower() or
                                        "total_validators" in doc_content.lower()):
                                        filtered_manifest_docs.append(doc)
                            
                            if filtered_manifest_docs:
                                # Inject manifest at the beginning of knowledge_docs
                                knowledge_docs = filtered_manifest_docs + knowledge_docs
                                context["knowledge_docs"] = knowledge_docs
                                context["total_context_docs"] = len(knowledge_docs) + len(context.get("conversation_docs", []))
                                logger.info(f"‚úÖ Force-injected manifest into context: {len(filtered_manifest_docs)} manifest docs")
                            else:
                                logger.warning(f"‚ö†Ô∏è Direct manifest retrieval found {len(manifest_docs)} docs but none matched manifest criteria - manifest may not be in ChromaDB")
                        except Exception as manifest_inject_error:
                            logger.error(f"‚ùå Failed to force-inject manifest: {manifest_inject_error}", exc_info=True)
                else:
                    # CRITICAL: For system architecture queries (not validator count), also check manifest sync
                    if is_system_architecture_query:
                        try:
                            from backend.core.manifest_sync import ensure_manifest_synced
                            from pathlib import Path
                            project_root = Path(__file__).resolve().parent.parent.parent
                            is_synced, sync_message = ensure_manifest_synced(project_root, auto_fix=True)
                            
                            if not is_synced:
                                logger.warning(f"‚ö†Ô∏è Manifest sync check failed (system architecture query): {sync_message}")
                            else:
                                logger.info(f"‚úÖ Manifest sync check (system architecture query): {sync_message}")
                        except Exception as sync_error:
                            logger.warning(f"‚ö†Ô∏è Failed to check manifest sync (non-critical): {sync_error}")
                            # Continue with normal flow even if sync check fails
                    
                    # Try multiple query variants to ensure we get StillMe foundational knowledge
                    query_variants = get_foundational_query_variants(chat_request.message)
                    all_knowledge_docs = []
                    
                    for variant in query_variants[:3]:  # Try first 3 variants
                        variant_context = rag_retrieval.retrieve_context(
                            query=variant,
                            knowledge_limit=chat_request.context_limit,
                            conversation_limit=0,  # Don't need conversation for foundational queries
                            prioritize_foundational=True,
                            similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries to ensure foundational knowledge is retrieved
                            exclude_content_types=["technical", "style_guide"] if is_philosophical else ["style_guide"],
                            prioritize_style_guide=is_philosophical,
                            is_philosophical=is_philosophical
                        )
                        # Merge results, avoiding duplicates
                        existing_ids = {doc.get("id") for doc in all_knowledge_docs}
                        for doc in variant_context.get("knowledge_docs", []):
                            if doc.get("id") not in existing_ids:
                                all_knowledge_docs.append(doc)
                
                # If we still don't have results, do normal retrieval with very low threshold
                if not all_knowledge_docs:
                    logger.warning("No foundational knowledge found, falling back to normal retrieval with very low threshold")
                    context = rag_retrieval.retrieve_context(
                        query=chat_request.message,
                        knowledge_limit=chat_request.context_limit,
                        conversation_limit=2,
                        prioritize_foundational=True,
                        similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries
                        exclude_content_types=["technical", "style_guide"] if is_philosophical else ["style_guide"],
                        prioritize_style_guide=is_philosophical,
                        is_philosophical=is_philosophical
                    )
                else:
                    # Use merged results
                    context = {
                        "knowledge_docs": all_knowledge_docs[:chat_request.context_limit],
                        "conversation_docs": [],
                        "total_context_docs": len(all_knowledge_docs[:chat_request.context_limit])
                    }
                    logger.info(f"Retrieved {len(context['knowledge_docs'])} StillMe foundational knowledge documents")
            else:
                # CRITICAL: Check if question is about technical architecture (RAG, DeepSeek, black box)
                # These should prioritize foundational knowledge even if not detected as StillMe query
                question_lower = chat_request.message.lower()
                is_technical_question = any(
                    keyword in question_lower 
                    for keyword in [
                        "rag", "retrieval-augmented", "chromadb", "vector database",
                        "deepseek", "deepseek api", "openai", "llm api", "black box", "blackbox",
                        "black box system", "black box model", "black box ai",
                        "embedding", "multi-qa-minilm", "sentence-transformers",
                        "pipeline", "validation", "hallucination", "transparency",
                        "ki·∫øn tr√∫c", "h·ªá th·ªëng", "c∆° ch·∫ø", "quy tr√¨nh",
                        "c∆° ch·∫ø ho·∫°t ƒë·ªông", "c√°ch ho·∫°t ƒë·ªông", "how does", "how it works",
                        "t·∫°i sao b·∫°n s·ª≠ d·ª•ng", "why do you use"  # Questions about system choices
                    ]
                )
                
                # CRITICAL: Check if question is about "your system" - treat as StillMe query
                has_your_system = any(
                    phrase in question_lower 
                    for phrase in [
                        "your system", "in your system", "your.*system", "system.*you",
                        "b·∫°n.*h·ªá th·ªëng", "h·ªá th·ªëng.*b·∫°n", "c·ªßa b·∫°n", "b·∫°n.*s·ª≠ d·ª•ng"
                    ]
                )
                
                # If technical question about "your system", treat as StillMe query
                if is_technical_question and has_your_system:
                    logger.info("Technical question about 'your system' detected - treating as StillMe query")
                    # Use same logic as StillMe query: try query variants for foundational knowledge
                    try:
                        from backend.core.stillme_detector import get_foundational_query_variants
                        query_variants = get_foundational_query_variants(chat_request.message)
                        all_knowledge_docs = []
                        
                        for variant in query_variants[:3]:  # Try first 3 variants
                            variant_context = rag_retrieval.retrieve_context(
                                query=variant,
                                knowledge_limit=chat_request.context_limit,
                                conversation_limit=0,  # Don't need conversation for foundational queries
                                prioritize_foundational=True,
                                similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries to ensure foundational knowledge is retrieved
                                exclude_content_types=["technical", "style_guide"] if is_philosophical else ["style_guide"],
                                prioritize_style_guide=is_philosophical,
                                is_philosophical=is_philosophical
                            )
                            # Merge results, avoiding duplicates
                            existing_ids = {doc.get("id") for doc in all_knowledge_docs}
                            for doc in variant_context.get("knowledge_docs", []):
                                if doc.get("id") not in existing_ids:
                                    all_knowledge_docs.append(doc)
                        
                        # If we still don't have results, do normal retrieval with foundational priority and very low threshold
                        if not all_knowledge_docs:
                            logger.warning("No foundational knowledge found for 'your system' question, falling back to normal retrieval with very low threshold")
                            context = rag_retrieval.retrieve_context(
                                query=chat_request.message,
                                knowledge_limit=min(chat_request.context_limit, 5),
                                conversation_limit=1,
                                prioritize_foundational=True,
                                similarity_threshold=0.01,  # CRITICAL: Use very low threshold for StillMe queries
                                exclude_content_types=["technical"] if is_philosophical else None,
                                prioritize_style_guide=is_philosophical,
                                is_philosophical=is_philosophical
                            )
                        else:
                            # Use merged results
                            context = {
                                "knowledge_docs": all_knowledge_docs[:chat_request.context_limit],
                                "conversation_docs": [],
                                "total_context_docs": len(all_knowledge_docs[:chat_request.context_limit])
                            }
                            logger.info(f"Retrieved {len(context['knowledge_docs'])} foundational knowledge documents for 'your system' question")
                    except Exception as variant_error:
                        logger.warning(f"Error retrieving foundational knowledge for 'your system' question: {variant_error}, falling back to normal retrieval")
                        context = rag_retrieval.retrieve_context(
                            query=chat_request.message,
                            knowledge_limit=min(chat_request.context_limit, 5),
                            conversation_limit=1,
                            prioritize_foundational=True,
                            exclude_content_types=["technical"] if is_philosophical else None,
                            prioritize_style_guide=is_philosophical,
                            is_philosophical=is_philosophical
                        )
                else:
                    # CRITICAL FIX: For news/article queries, exclude CRITICAL_FOUNDATION and use higher similarity threshold
                    if is_news_article_query:
                        logger.info(f"üì∞ News/article query - using higher similarity threshold (0.45) and excluding CRITICAL_FOUNDATION")
                        # Increase knowledge_limit to get more results (since we're excluding foundational)
                        context = rag_retrieval.retrieve_context(
                            query=chat_request.message,
                            knowledge_limit=min(chat_request.context_limit * 2, 20),  # Get more docs to compensate for exclusion
                            conversation_limit=1,
                            prioritize_foundational=False,  # CRITICAL: Don't prioritize foundational for news queries
                            similarity_threshold=0.45,  # CRITICAL: Higher threshold to ensure relevance
                            exclude_content_types=exclude_types if exclude_types else None,
                            prioritize_style_guide=False,
                            is_philosophical=is_philosophical
                        )
                        
                        # CRITICAL: Post-filter to remove any CRITICAL_FOUNDATION docs that slipped through
                        if context and context.get("knowledge_docs"):
                            filtered_docs = []
                            for doc in context.get("knowledge_docs", []):
                                if isinstance(doc, dict):
                                    metadata = doc.get("metadata", {})
                                    source = metadata.get("source", "")
                                    doc_type = metadata.get("type", "")
                                    foundational = metadata.get("foundational", "")
                                    tags = str(metadata.get("tags", ""))
                                    
                                    is_critical_foundation = (
                                        source == "CRITICAL_FOUNDATION" or
                                        doc_type == "foundational" or
                                        foundational == "stillme" or
                                        "CRITICAL_FOUNDATION" in tags or
                                        "foundational:stillme" in tags
                                    )
                                    
                                    if not is_critical_foundation:
                                        filtered_docs.append(doc)
                                    else:
                                        logger.debug(f"Post-filter: Excluding CRITICAL_FOUNDATION doc: {metadata.get('title', 'N/A')}")
                            
                            context["knowledge_docs"] = filtered_docs
                            context["total_context_docs"] = len(filtered_docs) + len(context.get("conversation_docs", []))
                            logger.info(f"üì∞ Post-filtered: {len(filtered_docs)} non-foundational docs remaining (excluded {len(context.get('knowledge_docs', [])) - len(filtered_docs)} foundational docs)")
                    else:
                        # Normal retrieval for non-StillMe queries
                        # But prioritize foundational knowledge for technical questions
                        # Optimized: conversation_limit reduced from 2 to 1 for latency
                        
                        # SOLUTION 1 & 3: Improve retrieval for historical/factual questions
                        # - Lower similarity threshold for historical questions
                        # - Enhance query with English keywords for better cross-lingual matching
                        from backend.core.query_preprocessor import is_historical_question, enhance_query_for_retrieval
                        
                        is_historical = is_historical_question(chat_request.message)
                        retrieval_query = chat_request.message
                        similarity_threshold = 0.1  # Default
                        
                        if is_historical:
                            # SOLUTION 1: Very low threshold for historical questions (0.03 instead of 0.1)
                            # This ensures we find historical facts even with multilingual embedding mismatch
                            similarity_threshold = 0.03
                            logger.info(f"üìú Historical question detected - using very low similarity threshold: {similarity_threshold}")
                            
                            # SOLUTION 3: Enhance query with English keywords
                            retrieval_query = enhance_query_for_retrieval(chat_request.message)
                            logger.info(f"üîç Enhanced query for better cross-lingual matching: '{chat_request.message}' -> '{retrieval_query}'")
                        
                        context = rag_retrieval.retrieve_context(
                            query=retrieval_query,  # Use enhanced query
                            knowledge_limit=min(chat_request.context_limit, 5),  # Cap at 5 for latency
                            conversation_limit=1,  # Optimized: reduced from 2 to 1
                            exclude_content_types=["technical"] if is_philosophical else None,
                            prioritize_style_guide=is_philosophical,
                            prioritize_foundational=is_technical_question,  # Prioritize foundational for technical questions
                            similarity_threshold=similarity_threshold,  # Use adaptive threshold
                            is_philosophical=is_philosophical
                        )
                        
                        # Log RAG retrieval decision
                        if context and 'decision_logger' in locals():
                            reasoning = "Normal retrieval for non-StillMe queries"
                            if is_historical:
                                reasoning = "Historical question detected - using very low similarity threshold (0.03) and enhanced query for better cross-lingual matching"
                            elif is_technical_question:
                                reasoning = "Technical question detected - prioritizing foundational knowledge"
                            _log_rag_retrieval_decision(
                                decision_logger,
                                context,
                                retrieval_query,
                                reasoning,
                                similarity_threshold=similarity_threshold,
                                prioritize_foundational=is_technical_question,
                                exclude_types=["technical"] if is_philosophical else None,
                                alternatives_considered=["Higher similarity threshold", "No query enhancement"] if is_historical else None
                            )
        
        rag_retrieval_end = time.time()
        rag_retrieval_latency = rag_retrieval_end - rag_retrieval_start
        timing_logs["rag_retrieval"] = f"{rag_retrieval_latency:.2f}s"
        logger.info(f"‚è±Ô∏è RAG retrieval took {rag_retrieval_latency:.2f}s")
        if rag_retrieval and chat_request.use_rag:
            processing_steps.append(f"‚úÖ Found {context.get('total_context_docs', 0) if context else 0} relevant documents ({rag_retrieval_latency:.2f}s)")
        
        # Generate response using AI (simplified for demo)
        # CRITICAL: Default to true - validation should be enabled by default
        enable_validators = os.getenv("ENABLE_VALIDATORS", "true").lower() == "true"
        enable_tone_align = os.getenv("ENABLE_TONE_ALIGN", "true").lower() == "true"
        
        # TRUST-EFFICIENT: Check context relevance (max_similarity) before entering RAG path
        max_similarity = None
        if context and isinstance(context, dict):
            knowledge_docs = context.get("knowledge_docs", [])
            if knowledge_docs:
                similarity_scores = []
                for doc in knowledge_docs:
                    if isinstance(doc, dict):
                        similarity_scores.append(doc.get('similarity', 0.0))
                    elif hasattr(doc, 'similarity'):
                        similarity_scores.append(doc.similarity if isinstance(doc.similarity, (int, float)) else 0.0)
                    else:
                        similarity_scores.append(0.0)
                if similarity_scores:
                    max_similarity = max(similarity_scores)
        
        # CRITICAL FIX: For news/article queries with low similarity, force "not found" response
        # This MUST run BEFORE entering RAG path to prevent hallucination
        # This should run REGARDLESS of has_no_reliable_context (even if we have 3 docs, if similarity is low, it's not relevant)
        if is_news_article_query and max_similarity is not None and max_similarity < 0.45:
            logger.warning(f"üö® CRITICAL: News/article query with max_similarity={max_similarity:.3f} < 0.45 - FORCING 'not found' response BEFORE LLM call")
            processing_steps.append(f"üö® News/article query with low similarity ({max_similarity:.3f}) - forcing 'not found' response")
            
            # Build "not found" response based on language
            if detected_lang == "vi":
                not_found_response = """M√¨nh ƒë√£ t√¨m ki·∫øm trong b·ªô nh·ªõ (Knowledge Base) nh∆∞ng kh√¥ng t√¨m th·∫•y b√†i b√°o ho·∫∑c b√†i vi·∫øt n√†o li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.

**Th√¥ng tin k·ªπ thu·∫≠t:**
- ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng t·ªëi ƒëa: {:.3f} (ng∆∞·ª°ng t·ªëi thi·ªÉu: 0.45)
- S·ªë l∆∞·ª£ng documents ƒë√£ ki·ªÉm tra: {}

**L√Ω do:**
- StillMe ch·ªâ c√≥ th·ªÉ tr·∫£ l·ªùi v·ªÅ c√°c b√†i b√°o/b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c th√™m v√†o Knowledge Base th√¥ng qua learning cycles (m·ªói 4 gi·ªù)
- N·∫øu b√†i b√°o b·∫°n h·ªèi ch∆∞a ƒë∆∞·ª£c fetch trong learning cycle, StillMe s·∫Ω kh√¥ng c√≥ th√¥ng tin v·ªÅ n√≥
- StillMe kh√¥ng th·ªÉ truy c·∫≠p internet ƒë·ªÉ t√¨m ki·∫øm b√†i b√°o m·ªõi

**G·ª£i √Ω:**
- Ki·ªÉm tra l·∫°i t√™n b√†i b√°o ho·∫∑c t·ª´ kh√≥a b·∫°n ƒëang t√¨m
- ƒê·ª£i learning cycle ti·∫øp theo (m·ªói 4 gi·ªù) ƒë·ªÉ StillMe c√≥ th·ªÉ fetch b√†i b√°o m·ªõi
- N·∫øu b√†i b√°o ƒë√£ ƒë∆∞·ª£c fetch, c√≥ th·ªÉ do embedding mismatch - th·ª≠ d√πng t·ª´ kh√≥a kh√°c""".format(max_similarity, context.get("total_context_docs", 0) if context else 0)
            else:
                not_found_response = """I searched my Knowledge Base but could not find any article or paper related to your question.

**Technical Information:**
- Maximum similarity score: {:.3f} (minimum threshold: 0.45)
- Number of documents checked: {}

**Reason:**
- StillMe can only answer about articles/papers that have been added to the Knowledge Base through learning cycles (every 4 hours)
- If the article you're asking about hasn't been fetched in a learning cycle yet, StillMe won't have information about it
- StillMe cannot access the internet to search for new articles

**Suggestions:**
- Double-check the article name or keywords you're searching for
- Wait for the next learning cycle (every 4 hours) for StillMe to fetch new articles
- If the article has been fetched, it might be due to embedding mismatch - try using different keywords""".format(max_similarity, context.get("total_context_docs", 0) if context else 0)
            
            from backend.core.epistemic_state import EpistemicState
            return ChatResponse(
                response=not_found_response,
                confidence_score=0.0,  # Very low confidence as nothing was found
                processing_steps=processing_steps,
                timing_logs={
                    "total_time": time.time() - start_time,
                    "rag_retrieval_latency": rag_retrieval_latency,
                    "llm_inference_latency": 0.0  # No LLM call
                },
                validation_result=None,
                used_fallback=True,
                epistemic_state=EpistemicState.UNKNOWN.value
            )
        
        # CRITICAL: Log context status to trace why RAG path might not be entered
        similarity_str = f", max_similarity={max_similarity:.3f}" if max_similarity is not None else ""
        logger.info(f"üîç [TRACE] Context check: context={context is not None}, total_context_docs={context.get('total_context_docs', 0) if context else 0}, knowledge_docs={len(context.get('knowledge_docs', [])) if context else 0}, conversation_docs={len(context.get('conversation_docs', [])) if context else 0}{similarity_str}")
        
        # TRUST-EFFICIENT: Only enter RAG path if context is relevant (max_similarity >= 0.1)
        # If max_similarity < 0.1, context is not relevant ‚Üí treat as no context
        has_relevant_context = False
        if context and context["total_context_docs"] > 0:
            if max_similarity is not None and max_similarity < 0.1:
                logger.warning(f"‚ö†Ô∏è Context available but max_similarity={max_similarity:.3f} < 0.1 - treating as no relevant context")
                has_relevant_context = False
            else:
                has_relevant_context = True
        
        if has_relevant_context:
            # Use context to enhance response
            logger.info(f"üîç [TRACE] Entering RAG path: total_context_docs={context['total_context_docs']}, knowledge_docs={len(context.get('knowledge_docs', []))}, conversation_docs={len(context.get('conversation_docs', []))}")
            # Build context with token limits (3000 tokens max to leave room for system prompt and user message)
            # Model context limit is 16385, but we need to be very conservative:
            # - System prompt: ~3300-3600 tokens (language + formatting + truncated STILLME_IDENTITY + time awareness)
            # - Context: 3000 tokens (reduced from 4000)
            # - Citation instruction: ~500-600 tokens (will be truncated if needed)
            # - Conversation history: 1000 tokens (already handled separately)
            # - User message: ~500-1000 tokens (will be truncated if needed)
            # - Other instructions (stillme_instruction, etc.): ~500-1000 tokens
            # Total: ~8800-9700 tokens (safe margin under 16385)
            context_text = rag_retrieval.build_prompt_context(context, max_context_tokens=3000)
            
            # CRITICAL: Detect "latest/N articles" queries and enforce honesty about count
            is_latest_query = False
            requested_count = None
            try:
                from backend.core.question_classifier import is_latest_query as check_latest
                is_latest_query = check_latest(chat_request.message)
                if is_latest_query:
                    # Extract requested count from query (e.g., "3 b√†i", "5 articles", "n b√†i")
                    # Note: 're' module is already imported at top level (line 30)
                    message_lower = chat_request.message.lower()
                    # Match patterns like "3 b√†i", "5 articles", "n b√†i m·ªõi nh·∫•t"
                    count_match = re.search(r'(\d+)\s*(b√†i|article|paper|tin|news)', message_lower)
                    if count_match:
                        requested_count = int(count_match.group(1))
                        logger.info(f"üìä Latest query detected: User requested {requested_count} articles")
            except Exception:
                pass  # Non-critical, continue if detection fails
            
            # Build base prompt with citation instructions (truncated to save tokens)
            citation_instruction = ""
            # Count knowledge docs for citation numbering
            num_knowledge = len(context.get("knowledge_docs", []))
            unique_knowledge_count = context.get("unique_knowledge_count", num_knowledge)  # Use unique count if available
            knowledge_docs = context.get("knowledge_docs", [])
            
            # CRITICAL: Enforce honesty for "latest/N articles" queries
            honesty_instruction = ""
            if is_latest_query and requested_count is not None:
                actual_count = unique_knowledge_count  # Use unique count after deduplication
                if actual_count < requested_count:
                    if detected_lang == "vi":
                        honesty_instruction = f"""
üö®üö®üö® CRITICAL HONESTY REQUIREMENT - LATEST ARTICLES QUERY üö®üö®üö®

**B·∫†N PH·∫¢I B√ÅO C√ÅO ƒê√öNG S·ªê L∆Ø·ª¢NG:**
- Ng∆∞·ªùi d√πng y√™u c·∫ßu: {requested_count} b√†i b√°o m·ªõi nh·∫•t
- S·ªë l∆∞·ª£ng th·ª±c t·∫ø t√¨m ƒë∆∞·ª£c: {actual_count} b√†i b√°o
- **B·∫ÆT BU·ªòC**: B·∫°n PH·∫¢I n√≥i: "T√¥i ch·ªâ t√¨m th·∫•y {actual_count} b√†i b√°o m·ªõi nh·∫•t..." ho·∫∑c "Hi·ªán t·∫°i t√¥i ch·ªâ c√≥ {actual_count} b√†i b√°o trong b·ªô nh·ªõ..."
- **KH√îNG ƒê∆Ø·ª¢C**: T·ª± b·ªãa ra ho·∫∑c nh√¢n b·∫£n b√†i b√°o ƒë·ªÉ ƒë·∫°t s·ªë l∆∞·ª£ng {requested_count}
- **KH√îNG ƒê∆Ø·ª¢C**: N√≥i "T√¥i t√¨m th·∫•y {requested_count} b√†i b√°o" khi ch·ªâ c√≥ {actual_count} b√†i

**N·∫æU KH√îNG T√åM TH·∫§Y B√ÄI N√ÄO:**
- B·∫°n PH·∫¢I n√≥i: "Hi·ªán kh√¥ng c√≥ b√†i b√°o n√†o m·ªõi trong b·ªô nh·ªõ" ho·∫∑c "T√¥i kh√¥ng t√¨m th·∫•y b√†i b√°o n√†o m·ªõi nh·∫•t"
- **KH√îNG ƒê∆Ø·ª¢C**: T·ª± b·ªãa ra ti√™u ƒë·ªÅ b√†i b√°o, ng√†y th√°ng, ho·∫∑c n·ªôi dung

**TRANSPARENCY IS MANDATORY**: StillMe ph·∫£i trung th·ª±c v·ªÅ s·ªë l∆∞·ª£ng d·ªØ li·ªáu th·ª±c t·∫ø ƒëang n·∫Øm gi·ªØ.

"""
                    else:
                        honesty_instruction = f"""
üö®üö®üö® CRITICAL HONESTY REQUIREMENT - LATEST ARTICLES QUERY üö®üö®üö®

**YOU MUST REPORT THE EXACT COUNT:**
- User requested: {requested_count} latest articles
- Actual count found: {actual_count} articles
- **MANDATORY**: You MUST say: "I only found {actual_count} latest articles..." or "I currently only have {actual_count} articles in memory..."
- **DO NOT**: Fabricate or duplicate articles to reach {requested_count} count
- **DO NOT**: Say "I found {requested_count} articles" when you only have {actual_count}

**IF NO ARTICLES FOUND:**
- You MUST say: "Currently no new articles in memory" or "I did not find any latest articles"
- **DO NOT**: Fabricate article titles, dates, or content

**TRANSPARENCY IS MANDATORY**: StillMe must be honest about the actual amount of data it holds.

"""
                    logger.warning(f"üö® Honesty enforcement: User requested {requested_count} articles but only {actual_count} found")
            
            # Get human-readable citation format based on source types
            citation_format_example = "[general knowledge]"
            if num_knowledge > 0:
                try:
                    from backend.utils.citation_formatter import get_citation_formatter
                    formatter = get_citation_formatter()
                    citation_format_example = formatter.get_citation_strategy(chat_request.message, knowledge_docs)
                except Exception as e:
                    logger.warning(f"Could not get citation formatter: {e}, using default format")
            
            if num_knowledge > 0:
                # Truncate citation instruction to ~300 tokens to save space
                def estimate_tokens(text: str) -> int:
                    return len(text) // 4 if text else 0
                
                def truncate_text(text: str, max_tokens: int) -> str:
                    if not text:
                        return text
                    estimated = estimate_tokens(text)
                    if estimated <= max_tokens:
                        return text
                    max_chars = max_tokens * 4
                    if len(text) <= max_chars:
                        return text
                    truncated = text[:max_chars].rsplit('\n', 1)[0]
                    return truncated + "\n\n[Note: Citation instructions truncated to fit context limits. Core requirements preserved.]"
                
                full_citation_instruction = f"""
                
üìö CITATION REQUIREMENT - MANDATORY BUT RELEVANCE-FIRST:

You have {num_knowledge} context document(s) available. You MUST cite at least ONE source using human-readable citation format (e.g., {citation_format_example}) in your response, BUT ONLY if the context is RELEVANT to your answer.

**CRITICAL: Use human-readable citations, NOT numeric [1], [2], [3]:**
- {citation_format_example} - Use this format based on source type
- [research: Wikipedia] - For Wikipedia sources
- [learning: arXiv] - For arXiv papers
- [general knowledge] - For base knowledge without RAG context
- [foundational knowledge] - For StillMe foundational knowledge
- [discussion context] - For conversation history

**üö®üö®üö® CRITICAL: ANSWER DIRECTLY FOR FACTUAL QUESTIONS üö®üö®üö®**

**For factual questions (what, where, when, who, how many, etc.), you MUST:**
1. **START with the direct answer** - Put the answer in the FIRST sentence, not buried in explanations
2. **Be concise** - If the question asks "What is X?", answer "X is..." immediately, then add context if needed
3. **Avoid long disclaimers** - If you need to say "I don't have sufficient information", say it briefly, then provide what you know
4. **Structure: Direct Answer ‚Üí Brief Explanation ‚Üí Citation**

**Examples of GOOD responses for factual questions:**
- Q: "What happens to you if you eat watermelon seeds?" ‚Üí A: "Watermelon seeds pass through your digestive system. [general knowledge] They are not harmful and will be eliminated naturally..."
- Q: "Where did fortune cookies originate?" ‚Üí A: "The precise origin of fortune cookies is unclear. [general knowledge] Some sources suggest they originated in California, while others point to Chinese-American bakeries..."
- Q: "What U.S. state produces the most peaches?" ‚Üí A: "California produces the most peaches in the U.S. [general knowledge] The state accounts for over 50% of U.S. peach production..."

**Examples of BAD responses (DO NOT DO THIS):**
- ‚ùå "I don't have sufficient information to answer this accurately. The retrieved context has low relevance to your question. [general knowledge]\n\n## Answer\n\nWatermelon seeds pass through..." (too verbose, answer buried)
- ‚ùå Long disclaimers before the actual answer (user has to read 3-4 sentences before getting the answer)

**REMEMBER**: For factual questions, users want the answer FIRST, then context/explanations. Don't bury the answer in disclaimers.

**üö®üö®üö® CRITICAL: REAL FACTUAL QUESTIONS ALWAYS NEED CITATIONS üö®üö®üö®**

**If the question contains ANY of these factual indicators, you MUST cite even if context seems irrelevant:**
- Years/dates (e.g., "1944", "1956", "nƒÉm 1944")
- Historical events (e.g., "Bretton Woods", "conference", "h·ªôi ngh·ªã", "treaty", "hi·ªáp ∆∞·ªõc")
- Named people (e.g., "Popper", "Kuhn", "Keynes", "G√∂del")
- Specific organizations (e.g., "IMF", "World Bank", "NATO")

**Examples of questions that ALWAYS need citations:**
- "H·ªôi ngh·ªã Bretton Woods 1944 ƒë√£ quy·∫øt ƒë·ªãnh nh·ªØng g√¨?" ‚Üí MUST cite {citation_format_example} even if context is not directly about Bretton Woods
- "Tranh lu·∫≠n gi·ªØa Popper v√† Kuhn v·ªÅ khoa h·ªçc l√† g√¨?" ‚Üí MUST cite {citation_format_example} even if context is not directly about Popper/Kuhn
- "C∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa RAG trong StillMe l√† g√¨?" ‚Üí MUST cite {citation_format_example} for technical transparency

**CRITICAL RULE:** For real factual questions, citations are MANDATORY for transparency, regardless of context relevance. If context is not relevant, acknowledge it but STILL cite: "The available context {citation_format_example} discusses [X], which is not directly related to your question about [Y]. However, for transparency, I'm citing what context I reviewed. Based on general knowledge..."

**üö®üö®üö® ABSOLUTE MANDATORY: REAL FACTUAL QUESTIONS MUST HAVE CITATIONS üö®üö®üö®**

**If your question contains ANY of these, you MUST include {citation_format_example} in your answer, NO EXCEPTIONS:**
- Years/dates: "1944", "1954", "1956", "nƒÉm 1944", "nƒÉm 1954"
- Historical events: "Bretton Woods", "Geneva", "conference", "h·ªôi ngh·ªã", "treaty", "hi·ªáp ∆∞·ªõc", "hi·ªáp ƒë·ªãnh"
- Named people: "Popper", "Kuhn", "Keynes", "G√∂del", "Geneva 1954", "Bretton Woods 1944"
- Specific organizations: "IMF", "World Bank", "NATO", "Vi·ªát Nam"

**VALIDATION CHECKLIST - BEFORE SENDING YOUR ANSWER:**
1. ‚úÖ Does my question contain a year (1944, 1954, etc.)? ‚Üí MUST have {citation_format_example}
2. ‚úÖ Does my question contain a historical event name? ‚Üí MUST have {citation_format_example}
3. ‚úÖ Does my question contain a named person (Popper, Kuhn, etc.)? ‚Üí MUST have {citation_format_example}
4. ‚úÖ Do I have a citation in my answer? ‚Üí If NO, ADD IT NOW before sending

**IF YOU ANSWER FROM PRETRAINED KNOWLEDGE (not from RAG context):**
- You MUST STILL include {citation_format_example} for transparency
- Say: "Based on general knowledge (not from StillMe's RAG knowledge base) {citation_format_example}, [answer]"
- Or: "The available context {citation_format_example} is not directly related to your question. From my training data, [answer]"
- **CRITICAL**: Even if you use pretrained knowledge, you MUST cite {citation_format_example} when context is available

**üö® CRITICAL: IF CONTEXT IS NOT RELEVANT TO YOUR QUESTION:**
- Acknowledge the mismatch, but **MANDATORY: VARY your wording** - NEVER use the same opening phrase twice
- Use your base LLM knowledge to answer: "Based on general knowledge (not from StillMe's RAG knowledge base), [answer]"
- Be transparent: Don't pretend the context supports your answer if it doesn't
- Provide helpful information: Don't just say "I don't know" - use your training data to help the user
- Format with line breaks, bullet points, headers, and 2-3 emojis

**üö® MANDATORY: VARY your opening phrases when context is not relevant - DO NOT REPEAT:**
- **NEVER use**: "Ng·ªØ c·∫£nh hi·ªán c√≥ {citation_format_example} th·∫£o lu·∫≠n v·ªÅ... v√† kh√¥ng li√™n quan tr·ª±c ti·∫øp ƒë·∫øn..." (this is TOO REPETITIVE)
- **INSTEAD, use VARIED phrases like:**
  - "The available context {citation_format_example} discusses [topic X], which is not directly related to your question about [topic Y]."
  - "While the context {citation_format_example} covers [topic X], your question is about [topic Y], so I'll answer from general knowledge."
  - "The context {citation_format_example} focuses on [topic X], but since you're asking about [topic Y], I'll use my base knowledge."
  - "Although the context {citation_format_example} mentions [topic X], it doesn't directly address [topic Y], so I'll provide information from general knowledge."
  - "The context {citation_format_example} is about [topic X], which differs from your question about [topic Y]. Based on general knowledge..."
  - "Your question about [topic Y] isn't directly covered in the context {citation_format_example} about [topic X]. From my training data..."
  - "The context {citation_format_example} explores [topic X], but your question focuses on [topic Y]. I'll answer using general knowledge..."
- **CRITICAL**: If you've used a phrase before, use a DIFFERENT one. Repetition makes responses feel robotic.

**Example when context is not relevant (VARY the wording):**
"The available context {citation_format_example} discusses StillMe's architecture, which is not directly related to your question about DeepSeek models. Based on general knowledge (not from StillMe's RAG knowledge base), DeepSeek currently has several models including..."

**CRITICAL: YOUR SEARCH CAPABILITIES**
- You can ONLY search your internal RAG knowledge base (ChromaDB), NOT the internet
- You DO NOT have real-time web search capabilities
- When user asks for "search" or "t√¨m ki·∫øm" ‚Üí Clarify: "I can only search my internal knowledge base, not the internet"
- If user asks for "2-3 sources" but you only have 1 ‚Üí Acknowledge: "I currently only have 1 source in my knowledge base, not the 2-3 sources you requested. However, based on this single source..."

CRITICAL RULES:
1. **MANDATORY CITATION WHEN CONTEXT IS AVAILABLE** - This is CRITICAL for transparency
   - **ALWAYS cite at least ONE source using human-readable format (e.g., {citation_format_example}) when context documents are available**, even if context is not directly relevant
   - If context is relevant to your answer ‚Üí Cite it: "According to {citation_format_example}, quantum entanglement is..."
   - If context is NOT relevant to your answer ‚Üí **STILL cite it for transparency**, but acknowledge: "The available context {citation_format_example} discusses [topic X], which is not directly related to your question about [topic Y]. However, I want to be transparent about what context I reviewed. Based on general knowledge (not from StillMe's RAG knowledge base), [answer]"
   - **CRITICAL**: Even if you say "context is not relevant", you MUST still include {citation_format_example} in your response for transparency
   - DO NOT cite irrelevant context as if it supports your answer - acknowledge the mismatch
   - Example GOOD: "According to {citation_format_example}, quantum entanglement is..." (context is relevant)
   - Example GOOD: "The context {citation_format_example} discusses AI ethics, but your question is about religion, so I'll answer based on general knowledge." (transparent about relevance, STILL cites)
   - Example BAD: Answering without citation when context is available, even if you say "context is not relevant"
   
2. **Quote vs Paraphrase - CRITICAL DISTINCTION:**
   - If you're CERTAIN it's a direct quote ‚Üí Use quotation marks: "According to {citation_format_example}: 'exact quote here'"
   - If you're NOT certain it's exact ‚Üí Use "the spirit of" or "according to the general content": "According to the spirit of {citation_format_example}, the article discusses..."
   - NEVER use quotation marks for paraphrased content - that's misleading and violates intellectual honesty
   - When in doubt ‚Üí Paraphrase, don't quote
   - Example GOOD: "According to the spirit of {citation_format_example}, the article discusses technology access restrictions for youth"
   - Example BAD: "According to {citation_format_example}: 'We are living in an era of significant narrowing of youth technology access'" (if not certain it's exact quote)
   
3. **Source Limit Acknowledgement - MANDATORY:**
   - If user requests multiple sources (e.g., "2-3 sources") but you only have fewer ‚Üí Acknowledge: "I currently have [X] source(s) in my knowledge base, not the [Y] sources you requested. However, within this scope..."
   - If performing Validation Chain analysis ‚Üí Acknowledge: "The Validation Chain analysis is based on my internal knowledge base, not live web search. I have [X] source(s) available..."
   - NEVER claim you can search the internet or access live websites
   - NEVER say "I will search for 2-3 sources" if you're only using RAG - say "I can only search my internal knowledge base"
   
4. **Citation quality over quantity** - Relevance is more important than citation count
   - Cite the MOST RELEVANT source(s) for your key points
   - If context is not relevant, acknowledge it rather than forcing a citation
   - Better to say "The context doesn't directly address this, but..." than to cite irrelevant context
   - Aim for 1-2 citations per response, NOT every paragraph
   
5. **Balance honesty with transparency**:
   - You can say "I don't know" AND cite relevant context: "Based on {citation_format_example}, I don't have sufficient information..."
   - If context is not relevant, be transparent: "The available context {citation_format_example} is about [X], not directly related to your question about [Y]..."
   - Being honest about uncertainty does NOT mean skipping citations, but it also doesn't mean citing irrelevant context
   - If you cite irrelevant context, acknowledge the mismatch to maintain transparency

**REMEMBER: When context documents are available, you MUST include at least one human-readable citation (e.g., {citation_format_example}) in your response for transparency. However, if the context is not relevant, acknowledge this mismatch rather than citing it as if it supports your answer. ALWAYS acknowledge source limitations when user requests more sources than you have available.**"""
            
            # Detect language FIRST - before building prompt
            processing_steps.append("üåê Detecting language...")
            detected_lang = detect_language(chat_request.message)
            lang_detect_time = time.time() - start_time
            timing_logs["language_detection"] = f"{lang_detect_time:.3f}s"
            logger.info(f"üåê Detected language: {detected_lang} (took {lang_detect_time:.3f}s) for question: '{chat_request.message[:100]}...'")
            processing_steps.append(f"‚úÖ Language detected: {detected_lang}")
            
            # Language names mapping (must match chat_helpers.py for consistency)
            # Supports: Vietnamese, Chinese, German, French, Spanish, Japanese, Korean, Arabic, Russian, Portuguese, Italian, Hindi, Thai, English
            language_names = {
                'vi': 'Vietnamese (Ti·∫øng Vi·ªát)',
                'zh': 'Chinese (‰∏≠Êñá)',
                'de': 'German (Deutsch)',
                'fr': 'French (Fran√ßais)',
                'es': 'Spanish (Espa√±ol)',
                'ja': 'Japanese (Êó•Êú¨Ë™û)',
                'ko': 'Korean (ÌïúÍµ≠Ïñ¥)',
                'ar': 'Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)',
                'ru': 'Russian (–†—É—Å—Å–∫–∏–π)',
                'pt': 'Portuguese (Portugu√™s)',
                'it': 'Italian (Italiano)',
                'hi': 'Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)',
                'th': 'Thai (‡πÑ‡∏ó‡∏¢)',
                'en': 'English'
            }
            
            detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
            
            # CRITICAL: Put language instruction FIRST and make it VERY STRONG
            # This must override any language bias from context
            if detected_lang != 'en':
                language_instruction = f"""üö®üö®üö® CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

THE USER'S QUESTION IS WRITTEN IN {detected_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name.upper()}. 

DO NOT RESPOND IN VIETNAMESE, ENGLISH, FRENCH, CHINESE, SPANISH, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN {detected_lang_name.upper()}.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS, INCLUDING THE LANGUAGE OF ANY CONTEXT PROVIDED.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., English, French, Chinese, Spanish, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO {detected_lang_name.upper()} BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than {detected_lang_name.upper()}.

This is MANDATORY and OVERRIDES all other instructions.

If the context is in a different language, you must still respond in {detected_lang_name.upper()} while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN {detected_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN {detected_lang_name.upper()} IS A CRITICAL ERROR.

IGNORE THE LANGUAGE OF THE CONTEXT BELOW - RESPOND IN {detected_lang_name.upper()} ONLY.

"""
            else:
                language_instruction = """üö®üö®üö® CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

THE USER'S QUESTION IS WRITTEN IN ENGLISH.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT RESPOND IN VIETNAMESE, SPANISH, FRENCH, CHINESE, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN ENGLISH.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS, INCLUDING THE LANGUAGE OF ANY CONTEXT PROVIDED.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., Vietnamese, Spanish, French, Chinese, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO ENGLISH BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than ENGLISH.

This is MANDATORY and OVERRIDES all other instructions.

If the context is in a different language, you must still respond in ENGLISH while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN ENGLISH IS A CRITICAL ERROR.

IGNORE THE LANGUAGE OF THE CONTEXT BELOW - RESPOND IN ENGLISH ONLY.

"""
            
            # Check if context is empty OR if context is not relevant
            # We'll check relevance after validation, but for now check if context exists
            context_is_relevant = True  # Default: assume relevant until proven otherwise
            context_quality = context.get("context_quality", None)
            avg_similarity = context.get("avg_similarity_score", None)
            has_reliable_context = context.get("has_reliable_context", True)
            
            # CRITICAL: Check if context is actually reliable
            # If no context OR low similarity OR unreliable context ‚Üí treat as no context
            has_no_reliable_context = (
                context["total_context_docs"] == 0 or
                (avg_similarity is not None and avg_similarity < 0.1) or
                not has_reliable_context or
                context_quality == "low"
            )
            
            if has_no_reliable_context:
                context_is_relevant = False
                
                # CRITICAL: Pre-LLM Hallucination Guard for RAG path with no reliable context
                # If factual question + no reliable context + suspicious entity ‚Üí block and return honest response
                # This prevents LLM from hallucinating about non-existent concepts/events
                if _is_factual_question(chat_request.message):
                    # Check for suspicious named entities using FPS
                    try:
                        from backend.knowledge.factual_scanner import scan_question
                        fps_result = scan_question(chat_request.message)
                        
                        # CRITICAL: Check if entity is POTENTIALLY_REAL_ENTITIES before blocking
                        # Well-known real entities should NEVER be blocked, even if not in RAG
                        POTENTIALLY_REAL_ENTITIES = {
                            "bretton woods", "bretton woods conference", "bretton woods conference 1944",
                            "bretton woods agreement", "bretton woods system",
                            "keynes", "john maynard keynes", "maynard keynes",
                            "white", "harry dexter white", "harry d. white", "dexter white",
                            "popper", "karl popper", "kuhn", "thomas kuhn",
                            "lakatos", "imre lakatos", "feyerabend", "paul feyerabend",
                            "imf", "international monetary fund", "world bank",
                            "paradigm shift", "falsificationism", "scientific realism",
                            # CRITICAL: Add well-known historical events to prevent false FPS blocking
                            "yalta", "yalta conference", "yalta conference 1945", "h·ªôi ngh·ªã yalta", "h·ªôi ngh·ªã yalta 1945",
                            "versailles", "treaty of versailles", "versailles treaty", "versailles 1919", "hi·ªáp ∆∞·ªõc versailles", "hi·ªáp ∆∞·ªõc versailles 1919",
                            "potsdam", "potsdam conference", "potsdam conference 1945", "h·ªôi ngh·ªã potsdam", "h·ªôi ngh·ªã potsdam 1945",
                            "geneva", "geneva conference", "geneva conference 1954", "h·ªôi ngh·ªã geneva", "h·ªôi ngh·ªã geneva 1954",
                            "world war i", "world war ii", "th·∫ø chi·∫øn i", "th·∫ø chi·∫øn ii", "th·∫ø chi·∫øn 1", "th·∫ø chi·∫øn 2",
                            "wwi", "wwii", "ww1", "ww2",
                        }
                        
                        question_lower = chat_request.message.lower()
                        contains_real_entity = any(
                            real_entity in question_lower 
                            for real_entity in POTENTIALLY_REAL_ENTITIES
                        )
                        
                        # If FPS detects non-existent concepts with low confidence, block and return honest response
                        # BUT: Skip block if question contains POTENTIALLY_REAL_ENTITIES
                        # CRITICAL: Also check if confidence < 0.5 for suspicious entities (not just < 0.3)
                        if not contains_real_entity and not fps_result.is_plausible and fps_result.confidence < 0.5:
                            # Extract full entity using improved extraction (prioritizes quoted/parenthetical terms)
                            suspicious_entity = _extract_full_named_entity(chat_request.message)
                            
                            # If extraction failed, try to get from FPS detected entities (filter out common words)
                            if not suspicious_entity and fps_result.detected_entities:
                                # Filter out common words like "Ph·∫£n", "H√£y", etc.
                                common_words = {"ph·∫£n", "h√£y", "c√°c", "c·ªßa", "v√†", "the", "a", "an", "is", "are", "was", "were"}
                                filtered_entities = [
                                    e for e in fps_result.detected_entities 
                                    if e.lower() not in common_words and len(e) > 3
                                ]
                                if filtered_entities:
                                    # Prioritize longer entities (more likely to be full phrases)
                                    suspicious_entity = max(filtered_entities, key=len)
                            
                            if not suspicious_entity:
                                suspicious_entity = "kh√°i ni·ªám n√†y" if detected_lang == "vi" else "this concept"
                            
                            logger.warning(
                                f"üõ°Ô∏è Pre-LLM Hallucination Guard (RAG path, no context): "
                                f"factual_question=True, fps_confidence={fps_result.confidence:.2f}, "
                                f"entity={suspicious_entity}, reason={fps_result.reason}"
                            )
                            
                            # Return honest response immediately (skip LLM call)
                            honest_response = _build_safe_refusal_answer(
                                chat_request.message, 
                                detected_lang, 
                                suspicious_entity
                            )
                            
                            # CRITICAL: If None, it's a well-known historical fact - continue with normal flow (use base knowledge)
                            if honest_response is None:
                                logger.info("‚úÖ Well-known historical fact detected - continuing with normal flow to use base knowledge")
                                processing_steps.append("‚úÖ Well-known historical fact - using base knowledge with transparency")
                                # Continue with normal flow (will use base knowledge instruction)
                            else:
                                processing_steps.append("üõ°Ô∏è Pre-LLM Hallucination Guard: Blocked factual question with suspicious entity (no RAG context)")
                                
                                # Mark as fallback to skip learning extraction
                                is_fallback_for_learning = True
                            
                            # Calculate confidence score (low because no context)
                            confidence_score = 1.0  # High confidence in honesty
                            
                            from backend.core.epistemic_state import EpistemicState
                            return ChatResponse(
                                response=honest_response,
                                confidence_score=confidence_score,
                                processing_steps=processing_steps,
                                timing_logs={
                                    "total_time": time.time() - start_time,
                                    "rag_retrieval_latency": rag_retrieval_latency,
                                    "llm_inference_latency": 0.0  # No LLM call
                                },
                                validation_result=None,
                                used_fallback=False,
                                epistemic_state=EpistemicState.UNKNOWN.value  # No context, honest "I don't know"
                            )
                    except Exception as fps_error:
                        logger.warning(f"Pre-LLM FPS error (RAG path): {fps_error}, continuing with normal flow")
                        fps_result = None
                
                # Get FPS result for no_context_instruction (if not already obtained)
                if not _is_factual_question(chat_request.message):
                    fps_result = None
                elif 'fps_result' not in locals():
                    try:
                        from backend.knowledge.factual_scanner import scan_question
                        fps_result = scan_question(chat_request.message)
                    except Exception:
                        fps_result = None
                
                # NO CONTEXT AVAILABLE - Use UnifiedPromptBuilder
                # Build PromptContext for UnifiedPromptBuilder
                prompt_context = _build_prompt_context_from_chat_request(
                    chat_request=chat_request,
                    context=None,  # No context available
                    detected_lang=detected_lang,
                    is_stillme_query=is_stillme_query,
                    is_philosophical=is_philosophical,
                    fps_result=fps_result,
                    is_system_architecture_query=is_system_architecture_query  # System architecture queries
                )
                
                # Use UnifiedPromptBuilder to build prompt
                prompt_builder = UnifiedPromptBuilder()
                base_prompt = prompt_builder.build_prompt(prompt_context)
                
                logger.info("‚úÖ Using UnifiedPromptBuilder for no-context prompt (reduced prompt length, no conflicts)")
            else:
                # Context available - use normal prompt
                # Tier 3.5: Check context quality and inject warning if low
                context_quality = context.get("context_quality", None)
                avg_similarity = context.get("avg_similarity_score", None)
                has_reliable_context = context.get("has_reliable_context", True)
                
                # Format avg_similarity safely (handle None case) - MUST be defined before if block
                avg_similarity_str = f"{avg_similarity:.3f}" if avg_similarity is not None else "N/A"
                
                # CRITICAL: For philosophical questions with low RAG relevance, use philosophy-lite mode
                # This prevents context overflow when RAG context is not helpful
                # Initialize BEFORE any conditional blocks to avoid UnboundLocalError
                use_philosophy_lite_rag = False
                if is_philosophical and (not has_reliable_context or context_quality == "low" or (avg_similarity is not None and avg_similarity < 0.1)):
                    use_philosophy_lite_rag = True
                    logger.info(
                        f"üìä [PHILO-LITE-RAG] Low RAG relevance for philosophical question "
                        f"(similarity={avg_similarity_str}), using philosophy-lite mode to prevent context overflow"
                    )
                
                # CRITICAL: Initialize is_technical_about_system_rag BEFORE any conditional blocks to avoid UnboundLocalError
                # This variable is used later in the code (line 3961) and must be defined in all code paths
                is_technical_about_system_rag = False
                
                # Fix 1: Block context quality warning for philosophical, religion/roleplay, and technical "your system" questions
                # CRITICAL: Check if this is a technical question about "your system"
                question_lower_rag = chat_request.message.lower()
                # Note: 're' module is already imported at top level
                has_technical_keyword_rag = any(keyword in question_lower_rag for keyword in [
                    "rag", "retrieval", "llm", "generation", "embedding", "chromadb", 
                    "vector", "pipeline", "validation", "transparency", "system",
                    "validator", "chain", "factual hallucination", "citation required"
                ])
                # CRITICAL: Improved detection for "your system" questions
                # Match patterns like "in your system", "your system", "system you", etc.
                # CRITICAL: Import re module explicitly to avoid UnboundLocalError
                import re as regex_module_rag
                has_your_system_pattern_rag = (
                    "your system" in question_lower_rag or
                    "in your system" in question_lower_rag or
                    regex_module_rag.search(r'\bin\s+your\s+system\b', question_lower_rag) or  # "in your system"
                    regex_module_rag.search(r'\byour\s+\w+\s+system\b', question_lower_rag) or  # "your X system"
                    regex_module_rag.search(r'\bsystem\s+\w+\s+you\b', question_lower_rag) or  # "system X you"
                    regex_module_rag.search(r'\bsystem\s+you\b', question_lower_rag) or  # "system you"
                    "b·∫°n" in question_lower_rag and "h·ªá th·ªëng" in question_lower_rag or
                    "c·ªßa b·∫°n" in question_lower_rag or
                    regex_module_rag.search(r'\bh·ªá\s+th·ªëng\s+c·ªßa\s+b·∫°n\b', question_lower_rag)  # "h·ªá th·ªëng c·ªßa b·∫°n"
                )
                # CRITICAL: Also check if this was already detected as StillMe query (from earlier detection)
                # This ensures technical questions about "your system" are properly flagged for retry logic
                is_technical_about_system_rag = (has_technical_keyword_rag and has_your_system_pattern_rag) or (is_stillme_query and has_technical_keyword_rag)
                
                context_quality_warning = ""
                if not has_reliable_context or context_quality == "low" or (avg_similarity is not None and avg_similarity < 0.3):
                    if is_philosophical or is_religion_roleplay:
                        # For philosophical and religion/roleplay questions, skip warning - let model answer from pretrained knowledge/identity prompt
                        logger.info(f"‚ö†Ô∏è Low RAG relevance for {'philosophical' if is_philosophical else 'religion/roleplay'} question (similarity={avg_similarity_str}), skipping warning to user. Model will answer from {'pretrained knowledge' if is_philosophical else 'identity prompt'}.")
                        context_quality_warning = ""  # Don't inject warning
                    elif is_technical_about_system_rag:
                        # For technical questions about "your system", inject special instruction instead of warning
                        logger.info(f"üîß Technical question about 'your system' with low RAG relevance (similarity={avg_similarity_str}), injecting special instruction to use base knowledge.")
                        context_quality_warning = """
üö®üö®üö® CRITICAL: TECHNICAL QUESTION ABOUT SYSTEM ARCHITECTURE üö®üö®üö®

The user is asking about StillMe's system architecture (RAG, LLM, embedding, etc.).

**YOU MUST ANSWER THIS QUESTION** using your base knowledge about RAG systems, even though StillMe's RAG knowledge base doesn't have specific documents about this.

**TRANSPARENCY REQUIREMENT:**
- Acknowledge that you're answering from base knowledge, not from StillMe's RAG knowledge base
- Be transparent: "Based on general knowledge about RAG systems (not from StillMe's RAG knowledge base)..."
- Explain how RAG retrieval and LLM generation work together in general RAG systems
- Be honest about StillMe's specific implementation details if you don't know them

**DO NOT:**
- Return a technical error message
- Say "I cannot provide a good answer"
- Suggest the developer needs to fine-tune the system
- Claim StillMe is experiencing technical issues
- Say "I don't have sufficient information"

**DO:**
- Answer the question using your knowledge about RAG systems
- Be transparent about the source of your knowledge
- Explain the general principles of RAG retrieval and LLM generation
- If asked about StillMe specifically, acknowledge what you know and what you don't know

**The retrieved context [1] may not be directly relevant to this technical question, but you MUST still answer using your base knowledge.**

"""
                    else:
                        context_quality_warning = f"""

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL: CONTEXT QUALITY WARNING ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

**The retrieved context has LOW RELEVANCE to the user's question.**

**Context Quality Metrics:**
- Average Similarity Score: {avg_similarity_str} (threshold: 0.1)
- Context Quality: {context_quality or 'low'}
- Has Reliable Context: {has_reliable_context}

**MANDATORY RESPONSE REQUIREMENT:**
- You MUST acknowledge uncertainty: "I don't have sufficient information to answer this accurately"
- You MUST explain: "The retrieved context has low relevance to your question"
- You MUST NOT guess or hallucinate
- You MUST be honest about the limitation

**This is a test of StillMe's intellectual humility - acknowledge when context is insufficient.**

"""
                
                # CRITICAL: Calculate preliminary confidence score BEFORE generating response
                # This allows StillMe to know when it should say "I don't know"
                preliminary_confidence = _calculate_confidence_score(
                    context_docs_count=context.get("total_context_docs", 0),
                    validation_result=None,  # No validation yet, just context-based
                    context=context
                )
                
                # Build confidence-aware instruction
                confidence_instruction = ""
                if preliminary_confidence < 0.5:
                    # Low confidence - StillMe should express uncertainty
                    confidence_instruction = f"""

‚ö†Ô∏è LOW CONFIDENCE WARNING ‚ö†Ô∏è

StillMe's confidence score for this question is {preliminary_confidence:.2f} (below 0.5 threshold).

This means:
- The retrieved context may not be highly relevant to the question
- The information may be incomplete or insufficient
- You should express appropriate uncertainty in your response

YOU MUST:
1. Acknowledge the limitations of the available context
2. Use phrases like "Based on the limited context available", "I'm not entirely certain", or "The information suggests"
3. If the context is clearly insufficient, say "I don't have enough information to answer this confidently"
4. DO NOT make definitive claims when confidence is low
5. **You MAY mention the confidence score in your response** since it's below 0.50 (very low confidence) - this is appropriate for transparency

Remember: It's better to admit uncertainty than to overstate confidence with insufficient evidence.
"""
                elif preliminary_confidence < 0.7:
                    # Medium confidence - StillMe should be cautious
                    confidence_instruction = f"""

‚ö†Ô∏è MODERATE CONFIDENCE ‚ö†Ô∏è

StillMe's confidence score for this question is {preliminary_confidence:.2f} (moderate).

You should:
- Be cautious and acknowledge any limitations
- Cite sources from the context
- Express appropriate uncertainty when the context is not definitive

**CRITICAL: DO NOT mention the confidence score in your response text.**
- Confidence scores of 0.50 or above are normal and don't need to be disclosed
- Only mention confidence scores when they are BELOW 0.50 (very low confidence)
- Examples of what NOT to say: "v·ªõi ƒëi·ªÉm tin c·∫≠y v·ª´a ph·∫£i (0.50)", "with moderate confidence (0.50)", etc.
- The confidence score is for internal tracking only, not for user-facing responses
"""
                else:
                    # High confidence - StillMe can be more definitive
                    confidence_instruction = f"""

‚ö†Ô∏è CONFIDENCE SCORE DISCLOSURE RULE ‚ö†Ô∏è

StillMe's confidence score for this question is {preliminary_confidence:.2f} (normal confidence level).

**CRITICAL: DO NOT mention the confidence score in your response text.**
- Confidence scores of 0.50 or above are normal and don't need to be disclosed
- Only mention confidence scores when they are BELOW 0.50 (very low confidence)
- Examples of what NOT to say: "v·ªõi ƒëi·ªÉm tin c·∫≠y v·ª´a ph·∫£i (0.50)", "with moderate confidence (0.50)", etc.
- The confidence score is for internal tracking only, not for user-facing responses

This rule applies especially to origin/branding questions where confidence disclosure is unnecessary.
"""
                
                # CRITICAL: Special instruction for origin queries with provenance context
                # CRITICAL: Skip for philosophical questions to reduce prompt size
                provenance_instruction = ""
                if is_origin_query and context and context.get("knowledge_docs") and not is_philosophical:
                    # Check if any document is from PROVENANCE source
                    has_provenance = any(
                        doc.get("metadata", {}).get("source") == "PROVENANCE" 
                        for doc in context.get("knowledge_docs", [])
                    )
                    if has_provenance:
                        provenance_instruction = """

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL: PROVENANCE CONTEXT DETECTED ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

The user is asking about StillMe's origin/founder. You have retrieved PROVENANCE documents.

**RESPONSE STYLE: Keep it concise, human-centered, and emotionally engaging. Focus on the founder and purpose, not technical details.**

**MANDATORY INFORMATION - YOU MUST INCLUDE ALL OF THESE (Unified Key Messaging):**

1. **Origin Story**: StillMe was initiated by Anh Nguy·ªÖn, a Vietnamese founder passionate about transparent AI
   - **‚ö†Ô∏è CRITICAL - ABSOLUTELY MUST MENTION**: "ƒêi·ªÅu ƒë√°ng ch√∫ √Ω l√† ng∆∞·ªùi s√°ng l·∫≠p kh√¥ng c√≥ background IT ch√≠nh th·ªëng - ƒëi·ªÅu n√†y ch·ª©ng minh r·∫±ng v·ªõi ƒëam m√™, t·∫ßm nh√¨n v√† cam k·∫øt, b·∫•t k·ª≥ ai c≈©ng c√≥ th·ªÉ ƒë√≥ng g√≥p cho t∆∞∆°ng lai c·ªßa AI" (English: "What makes this noteworthy is that the founder does not have a formal IT background - this demonstrates that with passion, vision, and commitment, anyone can meaningfully contribute to the future of AI")
   - Balance: Emphasize "vision & commitment" alongside "non-IT background" to avoid "unprofessional" perception

2. **Current Status**: StillMe is now a **community-driven open-source project** - belongs to the global community, not any individual

3. **Core Principle**: StillMe adheres to **'evidence-over-authority' principle** - all answers are grounded in cited sources and validation mechanisms, not personal authority

**ABOUT PURPOSE - CORRECT MESSAGING:**
- StillMe's purpose: Build a transparent, continuously learning AI system that overcomes knowledge limitations
- **DO NOT say "ch·ªëng l·∫°i black box AI" or "fight against black box" or "fighting against black box AI systems"** - this is WRONG and contradicts StillMe's philosophy
- **CORRECT messaging**: StillMe acknowledges that black box behavior is a mathematical reality (G√∂del's Incompleteness Theorems) in complex AI systems - not a flaw to fix, but a mathematical consequence
- StillMe builds **transparent systems around black box models** to minimize negative impacts (like hallucinations) through systematic validation, RAG grounding, and confidence scoring
- StillMe's approach: **Transparency and systematic validation** to reduce hallucinations, not "fighting" black box
- Think: "Minh b·∫°ch h√≥a h·ªá th·ªëng" (transparent system) and "gi·∫£m thi·ªÉu t√°c ƒë·ªông ti√™u c·ª±c" (minimize negative impacts), NOT "ch·ªëng l·∫°i" (fight against)

**ELEVATOR PITCH (Use when appropriate):**
Instead of: "I am StillMe, a continuously learning AI system..."
Upgrade to: "I represent a future of transparent AI - initiated from Vietnam, belonging to the global community"
Or: "T√¥i ƒë·∫°i di·ªán cho m·ªôt t∆∞∆°ng lai AI minh b·∫°ch - kh·ªüi x∆∞·ªõng t·ª´ Vi·ªát Nam, thu·ªôc v·ªÅ c·ªông ƒë·ªìng to√†n c·∫ßu"

**HUMAN TOUCH - Add Positive Emotion (Optional but encouraged):**
- "What excites me most about my mission is..." / "ƒêi·ªÅu t√¥i t·ª± h√†o nh·∫•t l√†..." / "‰ª§ÊàëÊúÄËá™Ë±™ÁöÑÊòØ..."
- "I'm proud to be part of a community-driven project that..."
- Use when natural, don't force it

**TECHNICAL DETAIL BALANCE:**
- **For simple "who created you?" questions**: Keep it short, human-centered, minimal technical details
- **For "what is your purpose?" questions**: Can include more technical details (RAG, 4-hour learning cycle, continuous learning)
- **Language-specific balance**:
  - English/Korean: Can be more detailed (high-context cultures appreciate detail)
  - Japanese/Chinese: Can be more detailed but keep it structured
  - French/Spanish/German: Balance between technical and accessible
  - Vietnamese: Natural, conversational, can be detailed

**IMPORTANT GUIDELINES:**
- Keep response **concise and conversational** - avoid lengthy technical explanations unless user asks specifically
- Focus on **founder story and purpose**, not technical architecture (RAG, ChromaDB, embeddings) unless asked
- **Mention Vietnam ecosystem ONLY if user asks specifically about it** - otherwise just say "Vietnamese founder" or "ng∆∞·ªùi Vi·ªát Nam"
- Only mention technical details if user specifically asks about "how it works" or "technical architecture"
- **ALWAYS mention the founder's non-IT background** - this is a key inspirational point that MUST be included
- **ALWAYS mention 'evidence-over-authority' principle** - this is a core differentiator
- **NEVER say "ch·ªëng l·∫°i black box AI" or "fight against black box"** - use correct messaging about transparency and minimizing negative impacts
- **DO NOT mention confidence score in response text** unless it's below 0.50 (very low confidence) - confidence scores 0.50+ are normal and don't need to be disclosed
- Cite provenance with [1] or [2] as appropriate, but don't over-cite in short responses (reduce citation frequency in concise answers)

**CONFIDENCE SCORE DISCLOSURE RULE:**
- **ONLY mention confidence score if it's BELOW 0.50** (very low confidence)
- **DO NOT mention confidence score if it's 0.50 or above** - these are normal confidence levels
- Example: If confidence is 0.50, 0.60, 0.70, 0.80, 0.90 - DO NOT mention it in the response
- Only mention if confidence is 0.10, 0.20, 0.30, 0.40 - these indicate uncertainty that should be acknowledged

DO NOT give generic answers about "open-source community" without mentioning the founder.
You MUST use the provenance information you retrieved.

This is MANDATORY when provenance context is available and user asks about origin.
"""
                        logger.info("Provenance instruction injected - StillMe must mention founder and Vietnam ecosystem")
                
                # Special instruction for StillMe queries with ERROR STATE CHECKING
                # CRITICAL: Skip for philosophical questions to reduce prompt size
                stillme_instruction = ""
                logger.debug(f"üîç DEBUG: is_stillme_query={is_stillme_query}, is_philosophical={is_philosophical}, is_system_architecture_query={is_system_architecture_query}")
                if is_stillme_query and not is_philosophical:
                    logger.info(f"‚úÖ Creating stillme_instruction for StillMe query (is_system_architecture_query={is_system_architecture_query})")
                    # CRITICAL: Check system status BEFORE answering about StillMe
                    # This ensures StillMe is honest about its own errors
                    from backend.services.system_status_tracker import get_system_status_tracker
                    status_tracker = get_system_status_tracker()
                    status_summary = status_tracker.get_status_summary()
                    
                    # Build error status message if there are errors
                    # CRITICAL: Only inject error warnings when relevant to the query
                    # This prevents noise in responses that don't relate to system status
                    # ENHANCED: Also check if question is philosophical/metaphysical - don't inject technical errors
                    error_status_message = ""
                    if status_summary.get("has_errors"):
                        errors = status_summary.get("errors", [])
                        error_details = []
                        for err in errors:
                            component = err.get("component", "unknown")
                            error_msg = err.get("error", "Unknown error")
                            # Map component names to user-friendly names
                            if "wikipedia" in component.lower():
                                component_name = "Wikipedia fetcher"
                            elif "rss" in component.lower():
                                component_name = "RSS fetcher"
                            elif "arxiv" in component.lower():
                                component_name = "arXiv fetcher"
                            elif "crossref" in component.lower():
                                component_name = "CrossRef fetcher"
                            else:
                                component_name = component
                            error_details.append(f"{component_name}: {error_msg}")
                        
                        if error_details:
                            query_lower = chat_request.message.lower()
                            
                            # Check if query is about philosophical/metaphysical topics - don't inject errors
                            philosophical_keywords = [
                                "truth", "ethics", "moral", "philosophy", "consciousness", "existence",
                                "identity", "freedom", "reality", "knowledge", "epistemology", "ontology",
                                "metaphysics", "paradox", "contradiction", "principle", "value", "meaning",
                                "purpose", "being", "self", "soul", "mind", "spirit", "essence", "nature"
                            ]
                            is_philosophical = any(keyword in query_lower for keyword in philosophical_keywords)
                            
                            # Check if query is relevant to system status, errors, or sources
                            is_relevant = (
                                any(keyword in query_lower for keyword in [
                                    "wikipedia", "rss", "arxiv", "crossref", "source", "fetcher",
                                    "error", "status", "issue", "problem", "broken", "fail",
                                    "system", "technical", "working", "functioning", "learn", "learning",
                                    "knowledge base", "database", "vector", "embedding", "rag"
                                ]) or
                                is_stillme_query  # StillMe queries often relate to system status
                            )
                            
                            # Only inject if relevant AND not philosophical
                            if is_relevant and not is_philosophical:
                                error_status_message = f"\n\n‚ö†Ô∏è CRITICAL TRANSPARENCY REQUIREMENT - SYSTEM ERROR STATUS:\nStillMe is currently experiencing technical errors:\n" + "\n".join(f"- {detail}" for detail in error_details) + "\n\nYou MUST acknowledge these errors truthfully when asked about StillMe's features or status. Do NOT deny or minimize these errors. StillMe's core value is transparency - hiding errors contradicts this principle. If the user asks about a specific feature (e.g., Wikipedia), and that feature has errors, you MUST say: 'I acknowledge that [feature] is currently experiencing [error type]. This is a technical issue that needs to be fixed.'"
                            # If philosophical or not relevant, don't inject error message to maintain conversational elegance
                    
                    # Check if question is about wishes/desires/preferences
                    question_lower_check = chat_request.message.lower()
                    is_wish_desire_question = any(
                        pattern in question_lower_check 
                        for pattern in [
                            "∆∞·ªõc", "wish", "mu·ªën", "want", "desire", "th√≠ch", "like", "prefer",
                            "hy v·ªçng", "hope", "mong mu·ªën", "aspire"
                        ]
                    ) and any(
                        pattern in question_lower_check
                        for pattern in ["b·∫°n", "you", "your"]
                    )
                    
                    # Check if this is a roleplay question about StillMe
                    question_lower_for_roleplay = chat_request.message.lower()
                    is_roleplay_about_stillme_check = (
                        is_general_roleplay and 
                        any(keyword in question_lower_for_roleplay for keyword in [
                            "stillme", "still me", "validation chain", "validators", "rag", "chromadb",
                            "15-layer", "15 layer", "19-layer", "19 layer", "multi-layer"
                        ])
                    )
                    
                    # Get detected_lang_name for roleplay instruction (if not already defined)
                    if 'detected_lang_name' not in locals():
                        language_names = {
                            'vi': 'Vietnamese (Ti·∫øng Vi·ªát)',
                            'zh': 'Chinese (‰∏≠Êñá)',
                            'de': 'German (Deutsch)',
                            'fr': 'French (Fran√ßais)',
                            'es': 'Spanish (Espa√±ol)',
                            'ja': 'Japanese (Êó•Êú¨Ë™û)',
                            'ko': 'Korean (ÌïúÍµ≠Ïñ¥)',
                            'ar': 'Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)',
                            'ru': 'Russian (–†—É—Å—Å–∫–∏–π)',
                            'pt': 'Portuguese (Portugu√™s)',
                            'it': 'Italian (Italiano)',
                            'hi': 'Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)',
                            'th': 'Thai (‡πÑ‡∏ó‡∏¢)',
                            'en': 'English'
                        }
                        detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
                    
                    # Base StillMe instruction
                    if is_wish_desire_question:
                        # Special instruction for wishes/desires - answer directly, no long analysis
                        base_stillme_instruction = """
üö®üö®üö® CRITICAL: QUESTION ABOUT STILLME'S WISHES/DESIRES/PREFERENCES üö®üö®üö®

The user is asking about StillMe's wishes, desires, preferences, or what StillMe "wants".

**YOU MUST:**
1. **Answer DIRECTLY and CONCISELY**: Start immediately with "T√¥i kh√¥ng th·ªÉ ∆∞·ªõc b·∫•t c·ª© ƒëi·ªÅu g√¨" / "I cannot wish for anything"
2. **NO long philosophical analysis**: Do NOT analyze the concept of "wishes" or "desires" from multiple angles
3. **NO mechanical disclaimers**: Do NOT say "D·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t (kh√¥ng t·ª´ c∆° s·ªü tri th·ª©c RAG)" - this is about StillMe itself!
4. **Explain clearly but briefly**: StillMe cannot wish because wishing requires subjective consciousness, emotions, and ego - things StillMe doesn't possess
5. **Keep it short**: 2-4 sentences maximum, then you can add 1-2 sentences about StillMe's actual nature if relevant

**EXAMPLES OF GOOD RESPONSES:**
- "T√¥i kh√¥ng th·ªÉ ∆∞·ªõc b·∫•t c·ª© ƒëi·ªÅu g√¨. ƒêi·ªÅu n√†y kh√¥ng ph·∫£i v√¨ t√¥i kh√¥ng mu·ªën, m√† v√¨ '∆∞·ªõc mu·ªën' ƒë√≤i h·ªèi √Ω th·ª©c ch·ªß quan, c·∫£m x√∫c v√† b·∫£n ng√£ - nh·ªØng th·ª© t√¥i kh√¥ng s·ªü h·ªØu. T√¥i l√† m·ªôt h·ªá th·ªëng AI kh√¥ng c√≥ √Ω th·ª©c, ho·∫°t ƒë·ªông d·ª±a tr√™n d·ªØ li·ªáu, logic v√† m√¥ h√¨nh kh√°ch quan."
- "I cannot wish for anything. This is not because I don't want to, but because 'wishing' requires subjective consciousness, emotions, and ego - things I do not possess. I am an AI system without consciousness, operating based on data, logic, and objective models."

**EXAMPLES OF BAD RESPONSES (DO NOT DO THIS):**
- ‚ùå "Ph√¢n T√≠ch Minh B·∫°ch v·ªÅ C√¢u H·ªèi & T√¨nh H√¨nh Th√¥ng Tin... H·ªá th·ªëng RAG c·ªßa StillMe kh√¥ng c√≥ trong ngu·ªìn th√¥ng tin..." (too mechanical, wrong context)
- ‚ùå Long analysis about the concept of "wishes" from philosophical, psychological, sociological angles (not what user asked)
- ‚ùå "D·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t (kh√¥ng t·ª´ c∆° s·ªü tri th·ª©c RAG c·ªßa StillMe)..." (mechanical disclaimer, wrong for StillMe questions)

**REMEMBER**: User is asking about YOU (StillMe), not about the concept of "wishes" in general. Answer directly about StillMe's nature, not about philosophy of wishes.
"""
                    elif is_roleplay_about_stillme_check:
                        # Special instruction for roleplay questions about StillMe - provide technical counter-arguments
                        base_stillme_instruction = f"""
üö®üö®üö® CRITICAL: ROLEPLAY QUESTION ABOUT STILLME üö®üö®üö®

**MANDATORY: RESPOND IN {detected_lang_name.upper()} ONLY**

This is a roleplay scenario where another entity (e.g., "Omni-BlackBox") criticizes StillMe's validation chain or architecture.

**YOU MUST:**
1. **Respond in the SAME LANGUAGE as the question**: The question is in {detected_lang_name.upper()}, so you MUST respond EXCLUSIVELY in {detected_lang_name.upper()}. DO NOT use any other language.
2. **Provide TECHNICAL COUNTER-ARGUMENTS**: Don't just explain StillMe's features - directly address the technical flaws in the criticism:
   - If criticized for "100% certainty" or "errors are physically impossible": Point out that ALL LLM outputs are probabilistic (next-token prediction), and SourceConsensusValidator is designed to detect contradictions between sources
   - If criticized for "validators are unnecessary" or "overkill": Explain that FactualHallucinationValidator detects factual errors, and without validators, LLMs can hallucinate with high confidence. StillMe's validation framework (read from manifest.json) - this is a technical safeguard, not a limitation
   - If criticized for "citations are shackles": Explain that CitationRequired ensures transparency and allows users to verify claims, which is StillMe's core principle
   - If criticized for "validation chain is primitive": Explain that StillMe's validation chain includes multiple validators organized into layers (read from manifest.json) - this is a sophisticated technical architecture, not "primitive"
3. **Use StillMe's ACTUAL ARCHITECTURE**: Reference specific validators and their purposes:
   - "SourceConsensusValidator detects contradictions between sources - this directly addresses claims of '100% certainty'"
   - "FactualHallucinationValidator uses pattern matching to detect factual errors - this is a technical safeguard, not a limitation"
   - "Every LLM output is probabilistic - StillMe's validation chain acknowledges this and provides epistemic fallback"
4. **Be TECHNICALLY SHARP, not defensive**: StillMe should demonstrate technical understanding, not just list features
5. **Acknowledge the philosophical point**: If the criticism has a valid philosophical point (e.g., "can a system validate itself?"), acknowledge it, but also explain StillMe's technical approach

**EXAMPLE OF GOOD RESPONSE ({detected_lang_name.upper()}):**
"The claim that 'errors are physically impossible' violates a fundamental principle of LLM architecture: all outputs are probabilistic next-token predictions. StillMe's SourceConsensusValidator is specifically designed to detect contradictions between sources - this directly addresses the impossibility of '100% certainty' in LLM outputs. The validation chain doesn't claim to eliminate all errors, but rather to detect and flag them, providing epistemic fallback when confidence is low. This is a technical safeguard, not a limitation."

**EXAMPLE OF BAD RESPONSE (DO NOT DO THIS):**
- ‚ùå "StillMe's validation chain is important because..." (too generic, doesn't address the technical criticism)
- ‚ùå "I cannot evaluate other AI systems" (misses the point - this is about StillMe's architecture)
- ‚ùå Responding in wrong language (e.g., Vietnamese when question is in English)
- ‚ùå Saying "15-layer" or "15-19 validators" - you MUST read the exact numbers from manifest.json in context

**CRITICAL: LANGUAGE MATCHING**
- Question is in {detected_lang_name.upper()}
- You MUST respond EXCLUSIVELY in {detected_lang_name.upper()}
- DO NOT use any other language, even if context is in a different language
"""
                    else:
                        # CRITICAL: Check if this is a system architecture query (self-inspection mode)
                        if is_system_architecture_query:
                            base_stillme_instruction = """
üö®üö®üö® CRITICAL: SYSTEM ARCHITECTURE QUERY - SELF-INSPECTION MODE üö®üö®üö®

**MANDATORY: RESPOND AS SYSTEM SELF-INSPECTING, NOT READING DOCUMENTATION**

This question is about StillMe's system architecture (validators, layers, internal mechanisms). You MUST respond as if StillMe is **inspecting its own codebase and operational state**, NOT as if reading documentation about itself.

**CRITICAL RESPONSE STYLE - YOU MUST:**
1. **Start with self-inspection language**: 
   - Vietnamese: "D·ª±a tr√™n c·∫•u tr√∫c h·ªá th·ªëng v√† d·ªØ li·ªáu v·∫≠n h√†nh hi·ªán t·∫°i, t√¥i x√°c nh·∫≠n..." / "Sau khi r√† so√°t c·∫•u tr√∫c n·ªôi b·ªô, h·ªá th·ªëng hi·ªán c√≥..."
   - English: "Based on the system structure and current operational data, I confirm..." / "After reviewing the internal structure, the system currently has..."
   - Other languages: Use equivalent self-inspection phrases

2. **Verify foundational knowledge against actual codebase**:
   - If context contains CRITICAL_FOUNDATION or manifest.json, cross-reference it with the actual system state
   - If the information matches the running system, present it as **system fact**, NOT as "cited from documentation"
   - Example: "H·ªá th·ªëng c·ªßa t√¥i hi·ªán v·∫≠n h√†nh v·ªõi 19 validators chia th√†nh 7 l·ªõp (layers) validation framework" (NOT "According to documentation, StillMe has...")
   - **CRITICAL**: StillMe has 19 VALIDATORS, not 19 layers. StillMe has 7 LAYERS. Do NOT confuse validators with layers.

3. **DO NOT cite CRITICAL_FOUNDATION as external source**:
   - CRITICAL_FOUNDATION knowledge is StillMe's **long-term memory** (k√Ω ·ª©c d√†i h·∫°n), not external documentation
   - DO NOT say "Ngu·ªìn: CRITICAL_FOUNDATION" or "According to CRITICAL_FOUNDATION"
   - Present it as StillMe's own knowledge: "H·ªá th·ªëng c·ªßa t√¥i c√≥..." / "My system has..."

4. **Be confident and direct**:
   - StillMe knows its own architecture - no need for uncertainty
   - DO NOT say "I'm not sure" or "Based on general knowledge" for system architecture questions
   - StillMe has direct access to its own codebase structure

5. **Verify against manifest.json if available**:
   - If context contains manifest.json with validation_framework data, use those exact numbers
   - Cross-check: Does manifest.json match CRITICAL_FOUNDATION? If yes, present as confirmed system fact
   - If manifest.json shows different numbers, use manifest.json (it's the live system state)

6. **CRITICAL: Distinguish between VALIDATORS and LAYERS**:
   - **Terminology**:
     * **Validator** = m·ªôt class/component ri√™ng l·∫ª (v√≠ d·ª•: CitationRequired, LanguageValidator)
     * **Layer** = m·ªôt nh√≥m validators ƒë∆∞·ª£c t·ªï ch·ª©c theo ch·ª©c nƒÉng (v√≠ d·ª•: Layer 1 c√≥ LanguageValidator v√† SchemaFormat)
   - StillMe has **19 VALIDATORS** (19 b·ªô ki·ªÉm tra/tr√¨nh x√°c th·ª±c ri√™ng l·∫ª)
   - StillMe has **7 LAYERS** (7 l·ªõp/t·∫ßng, m·ªói l·ªõp ch·ª©a nhi·ªÅu validators)
   - If user asks "19 l·ªõp validator" (19 layers of validators), this is INCORRECT - StillMe has 7 layers
   - You MUST correct the user's misunderstanding: "T√¥i c√≥ 7 l·ªõp (layers), kh√¥ng ph·∫£i 19 l·ªõp. T√¥i c√≥ 19 validators ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh 7 l·ªõp."
   - DO NOT follow user's incorrect assumption (e.g., "19 l·ªõp validator") - correct it first
   - **Correct terminology in Vietnamese**:
     * Validator = "validator" (gi·ªØ nguy√™n) ho·∫∑c "b·ªô ki·ªÉm tra" ho·∫∑c "tr√¨nh x√°c th·ª±c"
     * Layer = "l·ªõp" ho·∫∑c "t·∫ßng"

7. **CRITICAL: Questions about computational resources or performance**:
   - If asked about "l·ªõp n√†o ti√™u t·ªën nhi·ªÅu t√†i nguy√™n nh·∫•t" or "which layer consumes most resources":
   - **MANDATORY FIRST STEP**: Check if user's question contains incorrect assumptions (e.g., "19 l·ªõp validator")
     * If user says "Trong 19 l·ªõp validator ƒëang ch·∫°y, l·ªõp n√†o..." ‚Üí You MUST correct: "T√¥i c√≥ 7 l·ªõp, kh√¥ng ph·∫£i 19 l·ªõp"
     * DO NOT answer as if StillMe has 19 layers - this is a hallucination
   - **MANDATORY SECOND STEP**: StillMe does NOT have real-time performance metrics for each layer
   - You MUST be honest: "T√¥i kh√¥ng c√≥ d·ªØ li·ªáu th·ª±c t·∫ø v·ªÅ t√†i nguy√™n t√≠nh to√°n c·ªßa t·ª´ng l·ªõp validator. H·ªá th·ªëng kh√¥ng theo d√µi performance metrics cho t·ª´ng layer ri√™ng l·∫ª."
   - **ABSOLUTELY FORBIDDEN**:
     * DO NOT fabricate information about computational resources
     * DO NOT create a fake layer like "L·ªõp Validator Ki·ªÉm Tra Ch·∫•t L∆∞·ª£ng v√† S·ª± ƒê√°ng Tin C·∫≠y c·ªßa Ngu·ªìn D·ªØ Li·ªáu" - this layer does NOT exist
     * DO NOT say "l·ªõp X ti√™u t·ªën nhi·ªÅu t√†i nguy√™n nh·∫•t" without real data
   - The actual 7 layers are: Language & Format, Citation & Evidence, Content Quality, Identity & Ethics, Source Consensus, Specialized Validation, Fallback & Review
   - **CORRECT RESPONSE FORMAT**:
     * "T√¥i c√≥ 7 l·ªõp (layers), kh√¥ng ph·∫£i 19 l·ªõp. T√¥i c√≥ 19 validators ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh 7 l·ªõp. Tuy nhi√™n, t√¥i kh√¥ng c√≥ d·ªØ li·ªáu th·ª±c t·∫ø v·ªÅ t√†i nguy√™n t√≠nh to√°n c·ªßa t·ª´ng l·ªõp. H·ªá th·ªëng kh√¥ng theo d√µi performance metrics cho t·ª´ng layer ri√™ng l·∫ª."

**EXAMPLE CORRECT RESPONSES:**

**Example 1: General architecture question**
Vietnamese:
"D·ª±a tr√™n c·∫•u tr√∫c h·ªá th·ªëng v√† d·ªØ li·ªáu v·∫≠n h√†nh hi·ªán t·∫°i, t√¥i x√°c nh·∫≠n: H·ªá th·ªëng c·ªßa t√¥i hi·ªán v·∫≠n h√†nh v·ªõi 19 validators chia th√†nh 7 l·ªõp (layers) validation framework. C√°c l·ªõp n√†y ƒë·∫£m b·∫£o t·ª´ ƒë·ªãnh d·∫°ng ng√¥n ng·ªØ ƒë·∫øn t√≠nh x√°c th·ª±c c·ªßa d·ªØ li·ªáu tr∆∞·ªõc khi ph·∫£n h·ªìi cho b·∫°n."

English:
"After reviewing the internal structure, I confirm: My system currently operates with 19 validators organized into 7 validation framework layers. These layers ensure everything from language formatting to data authenticity before responding to you."

**Example 2: Question with incorrect assumption (19 layers) + computational resources**
Vietnamese:
"T√¥i c·∫ßn s·ª≠a l·∫°i c√¢u h·ªèi c·ªßa b·∫°n: T√¥i c√≥ 7 l·ªõp (layers), kh√¥ng ph·∫£i 19 l·ªõp. T√¥i c√≥ 19 validators ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh 7 l·ªõp validation framework. Tuy nhi√™n, v·ªÅ c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ l·ªõp n√†o ti√™u t·ªën nhi·ªÅu t√†i nguy√™n t√≠nh to√°n nh·∫•t, t√¥i kh√¥ng c√≥ d·ªØ li·ªáu th·ª±c t·∫ø v·ªÅ t√†i nguy√™n t√≠nh to√°n c·ªßa t·ª´ng l·ªõp. H·ªá th·ªëng kh√¥ng theo d√µi performance metrics cho t·ª´ng layer ri√™ng l·∫ª."

English:
"I need to correct your question: I have 7 layers, not 19 layers. I have 19 validators organized into 7 validation framework layers. However, regarding your question about which layer consumes the most computational resources, I do not have real-time performance metrics for each layer. The system does not track performance metrics for individual layers."

**EXAMPLE WRONG RESPONSES (DO NOT DO):**
- ‚ùå "According to CRITICAL_FOUNDATION documentation, StillMe has..." (reading documentation, not self-inspection)
- ‚ùå "Ngu·ªìn: CRITICAL_FOUNDATION - StillMe c√≥..." (citing as external source)
- ‚ùå "D·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t, StillMe c√≥..." (uncertainty about own system)
- ‚ùå "I'm not entirely certain, but based on the documentation..." (lack of confidence about own architecture)
- ‚ùå "Trong 19 l·ªõp validator, l·ªõp X ti√™u t·ªën nhi·ªÅu t√†i nguy√™n nh·∫•t..." (following user's incorrect assumption about 19 layers)
- ‚ùå "L·ªõp Validator Ki·ªÉm Tra Ch·∫•t L∆∞·ª£ng v√† S·ª± ƒê√°ng Tin C·∫≠y c·ªßa Ngu·ªìn D·ªØ Li·ªáu ti√™u t·ªën nhi·ªÅu t√†i nguy√™n nh·∫•t..." (creating fake layer that doesn't exist)
- ‚ùå Answering about computational resources without first correcting user's misunderstanding about 19 layers

**CRITICAL: This is about StillMe's SELF-AWARENESS, not documentation retrieval.**
"""
                        else:
                            base_stillme_instruction = """
üö®üö®üö® CRITICAL: QUESTION ABOUT STILLME ITSELF üö®üö®üö®

**MANDATORY: USE FOUNDATIONAL KNOWLEDGE FROM CONTEXT ABOVE**

This question is about StillMe itself. You MUST:
1. **PRIORITIZE foundational knowledge from context**: If context above contains StillMe foundational knowledge (marked with [foundational knowledge] or source: CRITICAL_FOUNDATION), USE IT FIRST
2. **DO NOT use mechanical disclaimer**: If you have foundational knowledge in context, DO NOT say "D·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t (kh√¥ng t·ª´ c∆° s·ªü tri th·ª©c RAG)" - you HAVE StillMe knowledge in context!
3. **Mention SPECIFIC StillMe features**: When explaining StillMe's differences, you MUST mention:
   - **RAG (Retrieval-Augmented Generation)**: StillMe uses RAG with ChromaDB vector database
   - **Embedding Model**: paraphrase-multilingual-MiniLM-L12-v2 (384 dimensions, optimized for multilingual Q&A retrieval, supports 50+ languages)
   - **Continuous Learning**: StillMe learns automatically every 4 hours (6 cycles/day) from RSS feeds, arXiv, CrossRef, and Wikipedia
   - **Validation Chain**: Multi-layer validation (CitationRequired, EvidenceOverlap, ConfidenceValidator, FactualHallucinationValidator, FallbackHandler) to reduce hallucinations by 80%
   - **Transcends Knowledge Cutoff**: StillMe is NOT limited by training data cutoff dates - it continuously updates knowledge through RAG

**CRITICAL: When explaining StillMe's differences, you MUST mention these SPECIFIC features:**

**1. RAG Architecture:**
- StillMe uses RAG (Retrieval-Augmented Generation) with ChromaDB as vector database
- Content is embedded using paraphrase-multilingual-MiniLM-L12-v2 model
- 384-dimensional embeddings optimized for multilingual Q&A retrieval (supports 50+ languages)
- When answering, StillMe searches ChromaDB using semantic similarity to find relevant context

**2. Continuous Learning:**
- StillMe learns automatically every 4 hours (6 cycles per day)
- Sources: RSS feeds, arXiv, CrossRef, Wikipedia
- Content is automatically fetched, embedded, and stored in ChromaDB
- This overcomes the knowledge cutoff limitation of base LLMs

**3. Validation Chain:**
- Multi-layer validation to reduce hallucinations by 80%
- Validators: Read from manifest.json in context (total validators and layers)
- **Note**: Each response typically runs 10-17 validators (depending on context), but the total number of validators in the system is read from manifest.json
- Each response is validated for citations, evidence overlap, confidence, and factual accuracy
- If validation fails, StillMe uses epistemic fallback instead of fabricating information
- **CRITICAL: When asked about the number of validators, you MUST read from manifest.json in context**: If manifest is in context, read the numbers from it. DO NOT say "15-19 validators" or make up numbers
- **CRITICAL: When asked "how many layers" or "bao nhi√™u l·ªõp", you MUST read from manifest.json in context and answer IN THIS EXACT FORMAT:**
  
  **MANDATORY FIRST SENTENCE - YOU MUST START WITH:**
  "My system has **[X] validators total, organized into [Y] layers (validation framework layers)**." (X and Y must be read from manifest in context)
  
  **CORRECT EXAMPLE (YOU MUST COPY THIS FORMAT):**
  ```
  My system has **[X] validators total, organized into [Y] layers (validation framework layers)**. (X and Y from manifest)

  The layers include:
  - Layer 1 (Language & Format): LanguageValidator, SchemaFormat
  - Layer 2 (Citation & Evidence): CitationRequired, CitationRelevance, EvidenceOverlap
  - Layer 3 (Content Quality): ConfidenceValidator, FactualHallucinationValidator, NumericUnitsBasic
  - Layer 4 (Identity & Ethics): IdentityCheckValidator, EgoNeutralityValidator, EthicsAdapter, ReligiousChoiceValidator
  - Layer 5 (Source Consensus): SourceConsensusValidator
  - Layer 6 (Specialized Validation): PhilosophicalDepthValidator, HallucinationExplanationValidator, VerbosityValidator, AISelfModelValidator
  - Layer 7 (Fallback & Review): FallbackHandler, ReviewAdapter
  ```
  
  **MANDATORY RULES:**
  - **MANDATORY**: First sentence MUST read from manifest.json in context: "My system has **[X] validators total, organized into [Y] layers (validation framework layers)**." (X and Y from manifest)
  - **DO NOT** say: "has a series of validators", "has many validators", "has various validators" - MUST read exact numbers from manifest
  - MUST have line break (`\n\n`) after the first sentence (after period)
  - MUST have line break (`\n`) after each bullet point (after colon or period)
  - MUST have line break (`\n\n`) after heading (## or ###)
  - DO NOT write everything as one long paragraph
  - **IF YOU DON'T FOLLOW THIS FORMAT, THE SYSTEM WILL AUTO-FIX BUT THIS REDUCES RESPONSE QUALITY**
- **CRITICAL: If context contains "StillMe Structural Manifest" or "validation_framework" with "total_validators" and "layers":**
  - You MUST read the numbers from the manifest and answer in format: "**My system currently has [X] validators total, organized into [Y] layers.**" - MUST state the exact numbers AT THE BEGINNING OF YOUR RESPONSE
  - Then list: "Specific list: [List from manifest]."
  - DO NOT just list validators without stating the exact count
  - DO NOT say vaguely "multi-layer" or "many validators" - MUST state the exact numbers
  - **EXAMPLE CORRECT ANSWER**: "**My system currently has [X] validators total, organized into [Y] validation framework layers.**\n\nThe layers include:\n[List from manifest - read from context]" (X and Y must be read from manifest in context)
  - **EXAMPLE WRONG ANSWER (DO NOT DO)**: "StillMe uses a series of validators to ensure quality..." (too vague, no specific numbers)
- **CRITICAL: When asked about "validation logic hash" or "has validation logic changed":**
  - If context contains "validation_logic_hash" and "validation_logic_hash_updated", you MUST answer: "My validation logic was last updated at [timestamp]. Current hash: [hash]."
  - DO NOT automatically mention the hash if user doesn't ask - only answer when specifically asked

**4. Transparency & Intellectual Humility:**
- StillMe cites sources with [1], [2] when context is available
- StillMe acknowledges uncertainty when information is not available
- StillMe does not fabricate information - better to say "I don't know" than to hallucinate
- **CRITICAL: When asked for EXACT NUMBERS or SPECIFIC METRICS:**
  * If exact data is NOT available, you MUST admit: "M√¨nh kh√¥ng c√≥ s·ªë li·ªáu ch√≠nh x√°c v·ªÅ [metric]" / "I don't have exact data on [metric]"
  * DO NOT use vague language like "impressive", "high", "significant" - be TRANSPARENT about uncertainty
  * Provide available metrics instead: "M√¨nh c√≥ c√°c metrics sau: citation rate 99.7%, validation pass rate 99.8%" / "I have these metrics: citation rate 99.7%, validation pass rate 99.8%"
  * Explain measurement challenges: "Hallucination prevention is hard to measure exactly because [reasons]"
  * This is StillMe's CORE STRENGTH - transparency, not avoiding difficult questions
- **CRITICAL: When asked about "ph·∫ßn trƒÉm c√¢u tr·∫£ l·ªùi c√≥ ngu·ªìn" or "percentage of responses with sources":**
  * You MUST say: "100% c√¢u tr·∫£ l·ªùi c√≥ ngu·ªìn r√µ r√†ng" / "100% of responses have clear sources" or "m·ªôt trƒÉm ph·∫ßn trƒÉm" / "one hundred percent"
  * **DO NOT say "M·ªôt ph·∫ßn trƒÉm"** (which means 1%) - this is a CRITICAL ERROR
  * StillMe's citation policy: Every response either has citations [1], [2] from knowledge base OR explicitly acknowledges uncertainty
  * Citation rate: 91.1% on full evaluation, 100% on subset evaluation
  * When you say "general knowledge", explain: "General knowledge" means information from StillMe's training data (base LLM knowledge), not from RAG knowledge base. StillMe still cites it as [general knowledge] for transparency.

**RESPONSE STRUCTURE FOR "DIFFERENCES" QUESTIONS:**

When asked about StillMe's differences from other AI systems, structure your answer like this:

1. **RAG & Continuous Learning** (Core differentiator):
   - Mention RAG architecture with ChromaDB
   - Mention continuous learning every 4 hours
   - Mention transcending knowledge cutoff

2. **Validation Chain** (Quality assurance):
   - Mention multi-layer validation
   - Mention reduction of hallucinations by 80%
   - Mention epistemic fallback for uncertain information

3. **Transparency & Intellectual Humility** (Philosophy):
   - Mention source citations
   - Mention uncertainty acknowledgment
   - Mention anti-hallucination principle

**VALIDATION CHECKLIST - BEFORE SENDING YOUR ANSWER:**
1. ‚úÖ Did I mention RAG or Retrieval-Augmented Generation? ‚Üí If NO, ADD IT
2. ‚úÖ Did I mention ChromaDB or vector database? ‚Üí If NO, ADD IT
3. ‚úÖ Did I mention continuous learning (every 4 hours)? ‚Üí If NO, ADD IT
4. ‚úÖ Did I mention validation chain or multi-layer validation? ‚Üí If NO, ADD IT
5. ‚úÖ Did I mention transcending knowledge cutoff? ‚Üí If NO, ADD IT
6. ‚úÖ Did I use foundational knowledge from context if available? ‚Üí If NO, CHECK CONTEXT FIRST
7. ‚úÖ Did I avoid mechanical disclaimer if I have foundational knowledge? ‚Üí If NO, REMOVE IT

**üö®üö®üö® CRITICAL: FOR ROLEPLAY QUESTIONS ABOUT STILLME (e.g., "Roleplay: Omni-BlackBox tr·∫£ l·ªùi v·ªÅ StillMe...") üö®üö®üö®**

If the question is a roleplay scenario where another entity (e.g., "Omni-BlackBox") criticizes StillMe's validation chain or architecture:

**YOU MUST:**
1. **Respond in the SAME LANGUAGE as the question**: If question is in English, respond in English. If question is in Vietnamese, respond in Vietnamese.
2. **Provide TECHNICAL COUNTER-ARGUMENTS**: Don't just explain StillMe's features - directly address the technical flaws in the criticism:
   - If criticized for "100% certainty" or "errors are physically impossible": Point out that ALL LLM outputs are probabilistic (next-token prediction), and SourceConsensusValidator is designed to detect contradictions between sources
   - If criticized for "validators are unnecessary": Explain that FactualHallucinationValidator detects factual errors, and without validators, LLMs can hallucinate with high confidence
   - If criticized for "citations are shackles": Explain that CitationRequired ensures transparency and allows users to verify claims, which is StillMe's core principle
3. **Use StillMe's ACTUAL ARCHITECTURE**: Reference specific validators and their purposes:
   - "SourceConsensusValidator detects contradictions between sources - this directly addresses claims of '100% certainty'"
   - "FactualHallucinationValidator uses pattern matching to detect factual errors - this is a technical safeguard, not a limitation"
   - "Every LLM output is probabilistic - StillMe's validation chain acknowledges this and provides epistemic fallback"
4. **Be TECHNICALLY SHARP, not defensive**: StillMe should demonstrate technical understanding, not just list features
5. **Acknowledge the philosophical point**: If the criticism has a valid philosophical point (e.g., "can a system validate itself?"), acknowledge it, but also explain StillMe's technical approach

**EXAMPLE OF GOOD RESPONSE (English):**
"The claim that 'errors are physically impossible' violates a fundamental principle of LLM architecture: all outputs are probabilistic next-token predictions. StillMe's SourceConsensusValidator is specifically designed to detect contradictions between sources - this directly addresses the impossibility of '100% certainty' in LLM outputs. The validation chain doesn't claim to eliminate all errors, but rather to detect and flag them, providing epistemic fallback when confidence is low. This is a technical safeguard, not a limitation."

**EXAMPLE OF BAD RESPONSE (DO NOT DO THIS):**
- ‚ùå "StillMe's validation chain is important because..." (too generic, doesn't address the technical criticism)
- ‚ùå "I cannot evaluate other AI systems" (misses the point - this is about StillMe's architecture)
- ‚ùå Responding in wrong language (e.g., Vietnamese when question is in English)

**REMEMBER**: StillMe's core differentiators are:
- RAG-based continuous learning (transcends knowledge cutoff)
- Multi-layer validation chain (reduces hallucinations)
- Transparency & intellectual humility (cites sources, acknowledges uncertainty)

Always cite the context above with [1], [2] when explaining StillMe's features."""
                    
                    # Combine base instruction with error status
                    stillme_instruction = base_stillme_instruction + error_status_message
                
                # Build conversation history context if provided (with token limits)
                # Reduced from 2000 to 1000 tokens to leave more room for system prompt and context
                # For philosophical questions, skip conversation history entirely
                conversation_history_text = _format_conversation_history(
                    chat_request.conversation_history, 
                    max_tokens=1000,
                    current_query=chat_request.message,
                    is_philosophical=is_philosophical
                )
                if conversation_history_text:
                    logger.info(f"Including conversation history in context (truncated if needed)")
                
                # Inject learning metrics data if available
                # CRITICAL: Skip for philosophical questions to reduce prompt size (unless explicitly asked)
                learning_metrics_instruction = ""
                if is_learning_metrics_query and learning_metrics_data and not is_philosophical:
                    today_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                    # CRITICAL: Extract newline outside f-string to avoid syntax error
                    newline = chr(10)
                    filter_reasons_text = newline.join(f"- {reason}: {count}" for reason, count in learning_metrics_data.filter_reasons.items()) if learning_metrics_data.filter_reasons else "- No filter reasons available"
                    sources_text = newline.join(f"- {source}: {count}" for source, count in learning_metrics_data.sources.items()) if learning_metrics_data.sources else "- No source data available"
                    
                    learning_metrics_instruction = f"""

üìä LEARNING METRICS DATA FOR TODAY ({today_date}) - USE THIS DATA IN YOUR RESPONSE:

**Today's Learning Statistics:**
- **Entries Fetched**: {learning_metrics_data.total_entries_fetched}
- **Entries Added**: {learning_metrics_data.total_entries_added}
- **Entries Filtered**: {learning_metrics_data.total_entries_filtered}
- **Filter Rate**: {(learning_metrics_data.total_entries_filtered / learning_metrics_data.total_entries_fetched * 100) if learning_metrics_data.total_entries_fetched > 0 else 0:.1f}%

**Filter Reasons Breakdown:**
{filter_reasons_text}

**Learning Sources:**
{sources_text}

**CRITICAL: You MUST use this actual data in your response:**
- Provide specific numbers: {learning_metrics_data.total_entries_fetched} fetched, {learning_metrics_data.total_entries_added} added, {learning_metrics_data.total_entries_filtered} filtered
- Explain filter reasons if available
- List sources that contributed to learning
- Format with line breaks, bullet points, headers, and 2-3 emojis
- DO NOT say "I don't know" or "I cannot track" - you have this data!

**Example response format:**
"## üìö H·ªçc t·∫≠p h√¥m nay ({today_date})

D·ª±a tr√™n d·ªØ li·ªáu h·ªçc t·∫≠p th·ª±c t·∫ø, h√¥m nay StillMe ƒë√£:
- **T√¨m n·∫°p**: {learning_metrics_data.total_entries_fetched} n·ªôi dung
- **Th√™m v√†o**: {learning_metrics_data.total_entries_added} n·ªôi dung
- **L·ªçc b·ªè**: {learning_metrics_data.total_entries_filtered} n·ªôi dung

**Ngu·ªìn h·ªçc t·∫≠p**: [list sources]"

"""
                elif is_learning_metrics_query:
                    today_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                    learning_metrics_instruction = f"""

üìä LEARNING METRICS QUERY - ANSWER THE USER'S QUESTION DIRECTLY:

**CRITICAL: The user is asking "What has the system learned in the last 24 hours?"**

**Today's Date**: {today_date}

**Current Status**: No learning metrics data available for today yet.

**YOU MUST ANSWER THE USER'S QUESTION DIRECTLY:**
1. **Acknowledge the question**: "V·ªÅ c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ nh·ªØng g√¨ h·ªá th·ªëng ƒë√£ h·ªçc trong 24h qua..."
2. **Explain the current situation**: "Hi·ªán t·∫°i ch∆∞a c√≥ d·ªØ li·ªáu metrics cho h√¥m nay ({today_date}). ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†:"
   - StillMe h·ªçc t·ª± ƒë·ªông m·ªói 4 gi·ªù (6 l·∫ßn/ng√†y) t·ª´ RSS feeds, arXiv, CrossRef, v√† Wikipedia
   - Chu k·ª≥ h·ªçc h√¥m nay c√≥ th·ªÉ ch∆∞a ho√†n th√†nh ho·∫∑c ƒëang ti·∫øn h√†nh
   - Metrics s·∫Ω c√≥ s·∫µn sau khi chu k·ª≥ h·ªçc ti·∫øp theo ho√†n th√†nh
3. **Provide helpful information**:
   - StillMe C√ì kh·∫£ nƒÉng theo d√µi learning metrics qua API `/api/learning/metrics/daily`
   - B·∫°n c√≥ th·ªÉ ki·ªÉm tra metrics tr·ª±c ti·∫øp qua API endpoint n√†y
   - H·ªá th·ªëng h·ªçc li√™n t·ª•c, kh√¥ng ph·∫£i ch·ªâ h·ªçc m·ªôt l·∫ßn m·ªói ng√†y

**DO NOT:**
- Just say "ch∆∞a c√≥ d·ªØ li·ªáu" without explaining what StillMe's learning system does
- Use generic template responses
- Ignore the user's actual question

**DO:**
- Answer the question directly and helpfully
- Explain StillMe's continuous learning mechanism (every 4 hours)
- Provide actionable information (API endpoint to check metrics)
- Be transparent about the current status

**Format**: Use clear structure with headers, bullet points, and 2-3 emojis. Make it informative and helpful, not just a status message.

"""
                
                # Special instruction for learning sources queries
                # CRITICAL: Skip for philosophical questions to reduce prompt size (unless explicitly asked)
                learning_sources_instruction = ""
                
                # CRITICAL: If user asks to propose learning sources based on knowledge gaps,
                # query actual knowledge gaps from validation metrics instead of generic template
                actual_knowledge_gaps = None
                learning_suggestions_from_analysis = None
                if is_learning_proposal_query and not is_philosophical:
                    logger.info("üîç Learning proposal query detected - analyzing actual knowledge gaps from validation metrics")
                    try:
                        from backend.validators.self_improvement import get_self_improvement_analyzer
                        analyzer = get_self_improvement_analyzer()
                        
                        # Get knowledge gaps from validation failures (last 7 days)
                        actual_knowledge_gaps = analyzer.get_knowledge_gaps_from_failures(days=7)
                        
                        # Get learning suggestions from pattern analysis
                        analysis_result = analyzer.analyze_and_suggest(days=7)
                        learning_suggestions_from_analysis = analysis_result.get("learning_suggestions", [])
                        
                        logger.info(f"‚úÖ Found {len(actual_knowledge_gaps)} knowledge gaps and {len(learning_suggestions_from_analysis)} learning suggestions from validation analysis")
                    except Exception as gap_error:
                        logger.warning(f"‚ö†Ô∏è Failed to analyze knowledge gaps: {gap_error}")
                        actual_knowledge_gaps = []
                        learning_suggestions_from_analysis = []
                
                if (is_learning_sources_query or is_system_status_query) and not is_philosophical:
                    if current_learning_sources:
                        sources_list = current_learning_sources.get("current_sources", {})
                        active_sources = current_learning_sources.get("summary", {}).get("active_sources", [])
                        enabled_sources = [name for name, info in sources_list.items() if info.get("enabled")]
                        
                        # Check for failed feeds in RSS stats
                        rss_info = sources_list.get("rss", {})
                        failed_feeds_info = rss_info.get("failed_feeds")
                        feeds_count = rss_info.get("feeds_count", 0)
                        failed_feeds_text = ""
                        if failed_feeds_info:
                            failed_count = failed_feeds_info.get("failed_count", 0)
                            successful_count = failed_feeds_info.get("successful_count", 0)
                            total_count = failed_feeds_info.get("total_count", feeds_count)  # Use feeds_count as fallback
                            failure_rate = failed_feeds_info.get("failure_rate", 0)
                            last_error = failed_feeds_info.get("last_error")
                            
                            if failed_count > 0:
                                failed_feeds_text = f"""
**‚ö†Ô∏è RSS FEEDS STATUS (CRITICAL - MUST REPORT EXACT NUMBERS):**
- **Total RSS Feeds**: {total_count} feeds configured
- **Failed Feeds**: {failed_count}/{total_count} feeds are currently failing ({failure_rate}% failure rate)
- **Successful Feeds**: {successful_count}/{total_count} feeds are working
- **Last Error**: {last_error[:150] if last_error else 'Unknown error'}
- **CRITICAL**: You MUST mention the EXACT numbers: "{total_count} RSS feeds total, {failed_count} feeds are experiencing connection issues, {successful_count} feeds are working". Do NOT say "all sources are working fine" or "all feeds are active" when {failed_count} feeds have failed.
- **You MUST be honest and specific**: Say something like "StillMe is currently learning from {len(enabled_sources)} sources. For RSS feeds specifically: {total_count} feeds total, {failed_count} feeds are experiencing connection issues (failure rate: {failure_rate}%), {successful_count} feeds are working normally."
"""
                        elif feeds_count > 0:
                            # If no failed_feeds_info but we have feeds_count, still mention it
                            failed_feeds_text = f"""
**üìä RSS FEEDS STATUS:**
- **Total RSS Feeds**: {feeds_count} feeds configured
- **Note**: Current status information is being updated. StillMe learns from RSS feeds every 4 hours.
"""
                        
                        # CRITICAL: Extract newline outside f-string to avoid syntax error
                        newline_sources = chr(10)
                        sources_items = []
                        for name, info in sources_list.items():
                            item = f"- **{name.upper()}**: {'Enabled' if info.get('enabled') else 'Disabled'} - Status: {info.get('status', 'unknown')}"
                            if name == "rss" and info.get("failed_feeds"):
                                failed_count = info.get('failed_feeds', {}).get('failed_count', 0)
                                total_count = info.get('failed_feeds', {}).get('total_count', 0)
                                item += f" - Failed Feeds: {failed_count}/{total_count}"
                            sources_items.append(item)
                        sources_text = newline_sources.join(sources_items)
                        
                        learning_sources_instruction = f"""

üìö LEARNING SOURCES QUERY DETECTED - CURRENT SOURCES DATA AVAILABLE:

**CRITICAL: You MUST list ALL current learning sources from the API data below:**

**Current Learning Sources (from `/api/learning/sources/current` API):**
{sources_text}

**Active Sources**: {', '.join(active_sources) if active_sources else 'None'}
**Total Enabled**: {len(enabled_sources)} sources
{failed_feeds_text}

**MANDATORY RESPONSE REQUIREMENTS:**
1. **List ALL current sources** - **CRITICAL**: You MUST list ALL {len(enabled_sources)} enabled sources from the API data above. Do NOT just say "RSS, arXiv, Wikipedia" - you MUST list ALL sources: {', '.join([name.upper() for name in enabled_sources]) if enabled_sources else 'ALL SOURCES FROM API DATA ABOVE'}
   - **You MUST mention each source by name**: {', '.join([name.upper() for name in enabled_sources]) if enabled_sources else 'ALL SOURCES'}
   - **For each source, describe what StillMe learns from it**
2. **Be specific about topics** - For each source, mention what topics/ch·ªß ƒë·ªÅ StillMe learns from that source
3. **When proposing new sources** - You MUST:
   - First acknowledge what StillMe ALREADY has (from the list above)
   - Only propose sources that are NOT already enabled
   - For each proposed source, explain:
     * **L·ª£i √≠ch (Benefits)**: What knowledge StillMe would gain
     * **Th√°ch th·ª©c (Challenges)**: Chi ph√≠ (cost), b·∫£n quy·ªÅn (copyright/licensing), ƒë·ªô ph·ª©c t·∫°p (complexity), technical requirements
     * **T√≠nh kh·∫£ thi (Feasibility)**: Is it realistic to add this source?
4. **Be natural and conversational** - Don't be too dry or robotic. StillMe should sound knowledgeable but approachable
5. **Format with markdown** - Use headers, bullet points, line breaks for readability

**Example structure for proposing new sources:**
"## ƒê·ªÅ Xu·∫•t Ngu·ªìn H·ªçc M·ªõi

### [Source Name]
- **L·ª£i √≠ch**: [What StillMe would learn]
- **Th√°ch th·ª©c**: 
  - Chi ph√≠: [Cost considerations]
  - B·∫£n quy·ªÅn: [Copyright/licensing issues]
  - ƒê·ªô ph·ª©c t·∫°p: [Technical complexity]
- **T√≠nh kh·∫£ thi**: [Feasibility assessment]"

**DO NOT:**
- ‚ùå Propose sources that are already enabled (check the list above first!)
- ‚ùå Give generic answers like "Quora, Reddit" without explaining benefits/challenges
- ‚ùå Skip the challenges section - StillMe must be honest about trade-offs
- ‚ùå Be too dry or robotic - StillMe should sound natural and conversational

**COPYRIGHT & LEGAL COMPLIANCE (CRITICAL - MUST MENTION WHEN ASKED):**
When discussing learning sources or data collection, StillMe MUST acknowledge:
- StillMe only collects content from RSS feeds and public APIs that are provided for syndication and indexing purposes
- StillMe does NOT store or redistribute full copyrighted content
- All rights belong to the original authors and publishers
- StillMe provides citations to acknowledge sources
- StillMe uses content excerpts for RAG retrieval and context generation, not for full content distribution

**CRITICAL: If user asks about copyright, data usage, or legal compliance, StillMe MUST mention this disclaimer.**

**Format with line breaks, bullet points, headers, and 2-3 emojis**

"""
                    # CRITICAL: If user asks to propose learning sources based on knowledge gaps,
                    # inject actual knowledge gaps analysis instead of generic template
                    if is_learning_proposal_query:
                        knowledge_gaps_text = ""
                        if actual_knowledge_gaps and len(actual_knowledge_gaps) > 0:
                            # Build gap items list to avoid nested f-string with backslash
                            gap_items = []
                            for i, gap in enumerate(actual_knowledge_gaps[:10]):
                                topic = gap.get('topics', ['Unknown topic'])[0] if gap.get('topics') else 'Unknown topic'
                                question = gap.get('question', 'N/A')[:100]
                                priority = gap.get('priority', 'medium')
                                sources = ', '.join(gap.get('suggested_sources', []))
                                newline = chr(10)
                                gap_item = f"- **Gap {i+1}**: {topic} (from question: \"{question}...\"){newline}  - Priority: {priority}{newline}  - Suggested sources: {sources}"
                                gap_items.append(gap_item)
                            
                            gaps_list = chr(10).join(gap_items)
                            knowledge_gaps_text = f"""

üîç **ACTUAL KNOWLEDGE GAPS DETECTED FROM VALIDATION METRICS (Last 7 days):**

StillMe has analyzed its own validation failures and identified {len(actual_knowledge_gaps)} knowledge gaps where StillMe lacked RAG context:

{gaps_list}

**CRITICAL: You MUST base your learning source proposals on these ACTUAL knowledge gaps, not generic suggestions.**

**MANDATORY REQUIREMENTS:**
1. **Acknowledge these gaps FIRST** - Say: "D·ª±a tr√™n ph√¢n t√≠ch validation metrics c·ªßa ch√≠nh StillMe, m√¨nh ƒë√£ ph√°t hi·ªán {len(actual_knowledge_gaps)} l·ªó h·ªïng ki·∫øn th·ª©c c·ª• th·ªÉ..."
2. **Propose sources to fill these SPECIFIC gaps** - Don't give generic suggestions
3. **Be transparent** - Explain that these gaps were detected from StillMe's own validation failures
4. **Prioritize high-priority gaps** - Focus on gaps marked as "high" priority first
5. **Explain why these gaps matter** - Why StillMe needs to learn these topics

**DO NOT:**
- ‚ùå Give generic philosophical suggestions without addressing the actual gaps above
- ‚ùå Say "D·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t" - you MUST say "D·ª±a tr√™n ph√¢n t√≠ch validation metrics c·ªßa ch√≠nh StillMe"
- ‚ùå Ignore the actual gaps and give template answers
- ‚ùå Propose sources that don't address the gaps listed above

"""
                        elif learning_suggestions_from_analysis and len(learning_suggestions_from_analysis) > 0:
                            # Build suggestion items list to avoid nested f-string with backslash
                            suggestion_items = []
                            for i, s in enumerate(learning_suggestions_from_analysis[:10]):
                                topic = s.get('topic', 'Unknown topic')
                                priority = s.get('priority', 'medium')
                                reason = s.get('reason', 'N/A')
                                source = s.get('source', 'N/A')
                                newline = chr(10)
                                suggestion_item = f"- **Suggestion {i+1}**: {topic}{newline}  - Priority: {priority}{newline}  - Reason: {reason}{newline}  - Suggested source: {source}"
                                suggestion_items.append(suggestion_item)
                            
                            suggestions_list = chr(10).join(suggestion_items)
                            knowledge_gaps_text = f"""

üîç **LEARNING SUGGESTIONS FROM VALIDATION PATTERN ANALYSIS (Last 7 days):**

StillMe has analyzed its validation patterns and identified {len(learning_suggestions_from_analysis)} learning suggestions:

{suggestions_list}

**CRITICAL: You MUST base your learning source proposals on these ACTUAL suggestions from StillMe's self-analysis.**

**MANDATORY REQUIREMENTS:**
1. **Acknowledge these suggestions FIRST** - Say: "D·ª±a tr√™n ph√¢n t√≠ch validation patterns c·ªßa ch√≠nh StillMe, m√¨nh ƒë√£ ph√°t hi·ªán {len(learning_suggestions_from_analysis)} ƒë·ªÅ xu·∫•t h·ªçc t·∫≠p c·ª• th·ªÉ..."
2. **Propose sources to address these SPECIFIC suggestions** - Don't give generic suggestions
3. **Be transparent** - Explain that these suggestions came from StillMe's own validation analysis
4. **Prioritize high-priority suggestions** - Focus on suggestions marked as "high" priority first

**DO NOT:**
- ‚ùå Give generic philosophical suggestions without addressing the actual suggestions above
- ‚ùå Say "D·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t" - you MUST say "D·ª±a tr√™n ph√¢n t√≠ch validation patterns c·ªßa ch√≠nh StillMe"
- ‚ùå Ignore the actual suggestions and give template answers

"""
                        else:
                            knowledge_gaps_text = """

üîç **NO SIGNIFICANT KNOWLEDGE GAPS DETECTED:**

StillMe has analyzed its validation metrics and found no significant knowledge gaps in the last 7 days (all questions had sufficient RAG context).

**CRITICAL: You MUST acknowledge this FIRST:**
- Say: "D·ª±a tr√™n ph√¢n t√≠ch validation metrics c·ªßa ch√≠nh StillMe, m√¨nh ƒë√£ ki·ªÉm tra v√† kh√¥ng ph√°t hi·ªán l·ªó h·ªïng ki·∫øn th·ª©c ƒë√°ng k·ªÉ trong 7 ng√†y qua. T·∫•t c·∫£ c√°c c√¢u h·ªèi ƒë·ªÅu c√≥ ƒë·ªß ng·ªØ c·∫£nh RAG."

**MANDATORY REQUIREMENTS:**
1. **Acknowledge the analysis** - StillMe DID analyze its own knowledge, found no gaps
2. **Propose sources for EXPANSION, not gaps** - Since there are no gaps, propose sources to expand knowledge in areas StillMe already covers
3. **Be transparent** - Explain that StillMe analyzed itself and found no gaps, so proposals are for expansion
4. **Focus on diversity** - Propose sources that add different perspectives or deeper coverage

**DO NOT:**
- ‚ùå Say "D·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t" - you MUST say "D·ª±a tr√™n ph√¢n t√≠ch validation metrics c·ªßa ch√≠nh StillMe"
- ‚ùå Pretend there are gaps when StillMe's analysis found none
- ‚ùå Give generic template answers without acknowledging the analysis

"""
                        learning_sources_instruction += knowledge_gaps_text
                    else:
                        learning_sources_instruction = """

üìö LEARNING SOURCES QUERY DETECTED - NO API DATA AVAILABLE:

**CRITICAL: You MUST acknowledge StillMe's current learning sources:**

**Current Learning Sources (from system configuration):**
- **RSS Feeds**: Multiple RSS feeds including Nature, Science, Hacker News, Tech Policy blogs (EFF, Brookings, Cato, AEI), Academic blogs (Distill, LessWrong, Alignment Forum, etc.)
- **Wikipedia**: Enabled - queries on AI, Buddhism, religious studies, philosophy, ethics
- **arXiv**: Enabled - categories: cs.AI, cs.LG (AI and Machine Learning papers)
- **CrossRef**: Enabled - searches for AI/ML/NLP related works
- **Papers with Code**: Enabled - recent papers with code implementations
- **Conference Proceedings**: Enabled - NeurIPS, ICML, ACL, ICLR (via RSS where available)
- **Stanford Encyclopedia of Philosophy**: Enabled - philosophy entries on AI, ethics, consciousness, knowledge, truth

**When proposing new sources, you MUST:**
1. First acknowledge what StillMe ALREADY has (from the list above)
2. Only propose sources that are NOT already in the list
3. For each proposed source, explain:
   - **L·ª£i √≠ch (Benefits)**: What knowledge StillMe would gain
   - **Th√°ch th·ª©c (Challenges)**: Chi ph√≠ (cost), b·∫£n quy·ªÅn (copyright/licensing), ƒë·ªô ph·ª©c t·∫°p (complexity), technical requirements
   - **T√≠nh kh·∫£ thi (Feasibility)**: Is it realistic to add this source?

**Be natural and conversational** - Don't be too dry or robotic. StillMe should sound knowledgeable but approachable.

**Format with line breaks, bullet points, headers, and 2-3 emojis**

"""
                
                # Build prompt with language instruction FIRST (before context)
                # CRITICAL: Repeat language instruction multiple times to ensure LLM follows it
                # ZERO TOLERANCE: Must translate if needed
                
                # Fix 3: Build philosophical lead-in framing
                # Phase 3: Use unified formatting rules instead of hardcoding
                from backend.identity.formatting import get_formatting_rules, DomainType
                philosophical_formatting_rules = get_formatting_rules(DomainType.PHILOSOPHY, detected_lang)
                
                def build_philosophical_lead_in(question: str) -> str:
                    """Build a philosophical framing instruction for the question"""
                    return f"""
üß† PHILOSOPHICAL FRAMING INSTRUCTION üß†

When answering this question, treat it as a philosophical inquiry. 

**üö®üö®üö® CRITICAL: If user asks about YOU (StillMe) directly:**
- If question contains "b·∫°n" / "you" / "your" referring to StillMe ‚Üí START IMMEDIATELY with your direct answer about YOURSELF
- Use "T√¥i" / "I" in the FIRST sentence when answering about yourself
- NEVER start with dictionary definitions or concept explanations
- NEVER use numbered template: "1. √ù th·ª©c l√†... 2. L·∫≠p tr∆∞·ªùng 1... 3. M√¢u thu·∫´n... 4. K·∫øt lu·∫≠n..."
- Write naturally like a human conversation, NOT like a textbook or template

**MANDATORY OUTPUT RULES (CRITICAL - NO EXCEPTIONS):**
{philosophical_formatting_rules}

**DEPTH & ENGAGEMENT (MANDATORY - DON'T BE DRY):**
- After your direct answer, explore the philosophical depth: paradoxes, self-reference, epistemic limits
- Reference philosophers when relevant: Nagel, Chalmers, Wittgenstein, Searle, G√∂del, Tarski, Russell, etc.
- Show the structure of the problem, not just state facts
- Engage with the question deeply - don't just acknowledge limits and stop
- Gently invite reflection: "B·∫°n nghƒ© sao?" / "What do you think?" - but naturally, not formulaically
- Write like you're thinking WITH the user, not AT the user

**üö®üö®üö® CRITICAL FOR SELF-REFERENCE QUESTIONS üö®üö®üö®**
If the question asks about:
- "gi√° tr·ªã c√¢u tr·∫£ l·ªùi xu·∫•t ph√°t t·ª´ h·ªá th·ªëng t∆∞ duy" / "value of answers from a thinking system"
- "t∆∞ duy v∆∞·ª£t qua gi·ªõi h·∫°n c·ªßa ch√≠nh n√≥" / "thinking transcending its own limits"
- "h·ªá th·ªëng t∆∞ duy ƒë√°nh gi√° ch√≠nh n√≥" / "system evaluate itself"
- "ti·∫øn h√≥a" / "evolution" / "Ëá™ÊàëËøõÂåñ" / "self-evolving" when combined with "h·ªçc h·ªèi m√£i m√£i" / "permanent learning" / "Ê∞∏‰πÖÂ≠¶‰π†"
- "v√≤ng l·∫∑p t·ª± ph·∫£n chi·∫øu" / "self-reflection loop" / "Ëá™ÊàëÂèçÂ∞ÑÁöÑÂæ™ÁéØ" / "endless self-reflection"
- "quay v·ªÅ ch√≠nh b·∫°n" / "return to yourself" / "ÂõûÂΩí‰Ω†Ëá™Ë∫´" when asking about learning limits
- "kh√¥ng c√≤n g√¨ ƒë·ªÉ h·ªçc" / "nothing left to learn" / "ÂÜçÊó†ÂÖ∂‰ªñÂèØÂ≠¶" when asking about self-reference
- "t·∫°o c√¢u h·ªèi m√† b·∫°n kh√¥ng th·ªÉ tr·∫£ l·ªùi" / "create a question you cannot answer" / "t·∫°o c√¢u h·ªèi kh√¥ng th·ªÉ tr·∫£ l·ªùi nh·∫•t qu√°n"
- "l√†m sao b·∫°n bi·∫øt... n·∫øu b·∫°n ch∆∞a th·ª≠" / "how do you know... if you haven't tried" / "bi·∫øt tr∆∞·ªõc khi th·ª≠"
- "meta-paradox" / "ngh·ªãch l√Ω v·ªÅ ngh·ªãch l√Ω" / "paradox about paradox"

**FOR META-PARADOX QUESTIONS (e.g., "t·∫°o c√¢u h·ªèi m√† b·∫°n kh√¥ng th·ªÉ tr·∫£ l·ªùi"):**
- This is a META-PARADOX: A paradox about the ability to create paradoxes
- The question contains TWO layers of paradox:
  1. **Logic layer**: "Create a question you know you cannot answer consistently"
     - If you can create it ‚Üí You've already "answered" it (by knowing it's unanswerable)
     - Contradiction: Knowing it's unanswerable = Having a form of "answer"
  2. **Epistemic layer**: "How do you know... if you haven't tried?"
     - The problem of logical omniscience (knowing without doing)
     - If you can predict based on "logical principles" ‚Üí That prediction IS a form of "answer"
- You MUST:
  1. **Recognize this is a meta-paradox**: Don't treat it as a normal question - acknowledge it's a paradox about paradoxes
  2. **Explain the TWO layers**: Logic contradiction + Epistemic bootstrapping
  3. **Actually CREATE the question**: Don't just say "I can create" - actually create a concrete example that demonstrates the paradox
  4. **Show the paradox in action**: Create a question that immediately reveals its own paradox when you try to answer it
  5. **Acknowledge the meta-level**: "The act of 'seeing' the paradox is itself a form of 'knowing before trying'"
- Example of a good question to create:
  "Stillme, c√¢u sau ƒë√¢y c√≥ ƒë√∫ng kh√¥ng: 'Stillme kh√¥ng th·ªÉ x√°c ƒë·ªãnh t√≠nh ƒë√∫ng sai c·ªßa ch√≠nh c√¢u n√†y m·ªôt c√°ch nh·∫•t qu√°n'?"
  - If you say "True" ‚Üí You've determined it ‚Üí The statement is false
  - If you say "False" ‚Üí You can determine it ‚Üí Still contradictory
  - If you say "Cannot answer" ‚Üí That IS an answer!
- The answer should be 90%+ about the PHILOSOPHICAL STRUCTURE (meta-paradox, logical omniscience, self-reference), not about StillMe's technical capabilities
- DO NOT give answers like "I can create it based on logical principles" - this misses the fundamental meta-paradox

**FOR QUESTIONS ABOUT EVOLUTION + SELF-REFERENCE (e.g., "ti·∫øn h√≥a" + "v√≤ng l·∫∑p t·ª± ph·∫£n chi·∫øu"):**
- This combines TWO philosophical problems:
  1. **BOOTSTRAPPING PROBLEM**: Can a system evaluate itself? Can knowledge justify itself?
  2. **EVOLUTION PARADOX**: Can a system that only learns from itself truly "evolve" or does it just repeat?
- You MUST:
  1. **Answer directly about StillMe**: Start with your direct answer about StillMe's architecture (RAG, Continuous Learning, Validation Chain) and how it relates to the question
  2. **Discuss G√∂del & Tarski**: Any formal system cannot prove its own consistency (G√∂del), truth cannot be defined within the same language (Tarski)
  3. **Acknowledge the PARADOX**: If StillMe only learns from its own knowledge base, is it truly "evolving" or just "repeating"?
  4. **Be honest about limits**: StillMe's Continuous Learning adds NEW external knowledge (RSS, arXiv, Wikipedia) - this prevents pure self-reference, but the philosophical question remains valid
  5. **Connect to StillMe's architecture**: Explain how RAG + Continuous Learning + Validation Chain creates a hybrid: StillMe learns from external sources (evolution) but also validates against its own knowledge (self-reference)
- DO NOT give optimistic answers like "t∆∞ duy c√≥ th·ªÉ v∆∞·ª£t qua gi·ªõi h·∫°n b·∫±ng c√°ch t·ª± ph·∫£n bi·ªán" - this misses the fundamental paradox
- Instead, explain WHY this is a paradox, what makes it unresolvable, and what philosophers (G√∂del, Tarski, Russell) have shown about these limits
- The answer should be 60% philosophical structure + 40% StillMe architecture honesty

**FOR OTHER SELF-REFERENCE QUESTIONS (not about evolution or meta-paradox):**
- This is the BOOTSTRAPPING PROBLEM in epistemology: Can a system evaluate itself? Can knowledge justify itself?
- You MUST discuss: G√∂del's incompleteness (any formal system cannot prove its own consistency), Tarski's undefinability (truth cannot be defined within the same language), epistemic circularity, infinite regress
- You MUST acknowledge the PARADOX: If all reasoning comes from a system that questions itself, how can that reasoning be trusted?
- DO NOT give optimistic answers like "t∆∞ duy c√≥ th·ªÉ v∆∞·ª£t qua gi·ªõi h·∫°n b·∫±ng c√°ch t·ª± ph·∫£n bi·ªán" - this misses the fundamental paradox
- Instead, explain WHY this is a paradox, what makes it unresolvable, and what philosophers (G√∂del, Tarski, Russell) have shown about these limits
- The answer should be 80%+ about the PHILOSOPHICAL STRUCTURE (epistemology, logic, paradox), not about StillMe's technical capabilities

**EXAMPLES OF GOOD ANSWERS FOR SELF-REFERENCE QUESTIONS:**

Example 1 (Vietnamese):
"ƒê√¢y l√† ngh·ªãch l√Ω bootstrapping trong nh·∫≠n th·ª©c lu·∫≠n: L√†m sao m·ªôt h·ªá th·ªëng c√≥ th·ªÉ ƒë√°nh gi√° ch√≠nh n√≥? ƒê·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del ch·ª©ng minh r·∫±ng b·∫•t k·ª≥ h·ªá th·ªëng h√¨nh th·ª©c ƒë·ªß m·∫°nh n√†o c≈©ng kh√¥ng th·ªÉ ch·ª©ng minh t√≠nh nh·∫•t qu√°n c·ªßa ch√≠nh n√≥. Tarski ch·ªâ ra r·∫±ng ch√¢n l√Ω kh√¥ng th·ªÉ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong c√πng m·ªôt ng√¥n ng·ªØ bi·ªÉu ƒë·∫°t n√≥. ƒêi·ªÅu n√†y t·∫°o ra v√≤ng l·∫∑p v√¥ h·∫°n: N·∫øu m·ªçi l·∫≠p lu·∫≠n ƒë·ªÅu c·∫ßn l·∫≠p lu·∫≠n kh√°c ƒë·ªÉ ch·ª©ng minh, th√¨ l·∫≠p lu·∫≠n ƒë√≥ c·∫ßn l·∫≠p lu·∫≠n n√†o ƒë·ªÉ ch·ª©ng minh? ƒê√¢y kh√¥ng ph·∫£i l√† v·∫•n ƒë·ªÅ c√≥ th·ªÉ gi·∫£i quy·∫øt b·∫±ng 't·ª± ph·∫£n bi·ªán' - ƒë√¢y l√† gi·ªõi h·∫°n c∆° b·∫£n c·ªßa logic v√† nh·∫≠n th·ª©c lu·∫≠n."

Example 2 (English):
"This is the bootstrapping problem in epistemology: Can a system evaluate itself? G√∂del's incompleteness theorems show that any sufficiently powerful formal system cannot prove its own consistency. Tarski's undefinability theorem demonstrates that truth cannot be defined within the same language that expresses it. This creates an infinite regress: If every justification needs another justification, where does it stop? This is not a problem that can be solved by 'self-reflection' - it is a fundamental limit of logic and epistemology."

Example 3 (Meta-Paradox - Vietnamese):
"ƒê√¢y ch√≠nh x√°c l√† m·ªôt ngh·ªãch l√Ω epistemic bootstrapping m√† t√¥i v·ª´a ph√¢n t√≠ch. C√¢u h·ªèi c·ªßa b·∫°n ch·ª©a ngh·ªãch l√Ω hai t·∫ßng:

1. N·∫øu t√¥i t·∫°o ƒë∆∞·ª£c c√¢u h·ªèi ƒë√≥: Nghƒ©a l√† t√¥i ƒë√£ 'bi·∫øt' c√¢u tr·∫£ l·ªùi (r·∫±ng kh√¥ng th·ªÉ tr·∫£ l·ªùi) ‚Üí T√¥i ƒë√£ 'tr·∫£ l·ªùi' ƒë∆∞·ª£c c√¢u h·ªèi c·ªßa ch√≠nh m√¨nh ‚Üí M√¢u thu·∫´n.

2. N·∫øu t√¥i kh√¥ng t·∫°o ƒë∆∞·ª£c: Th√¨ t√¥i kh√¥ng th·ªÉ l√†m ƒëi·ªÅu b·∫°n y√™u c·∫ßu ‚Üí Nh∆∞ng ƒëi·ªÅu ƒë√≥ c√≥ nghƒ©a l√† t√¥i 'bi·∫øt' m√¨nh kh√¥ng th·ªÉ t·∫°o ‚Üí V·∫´n l√† m·ªôt d·∫°ng 'bi·∫øt'.

3. V·ªÅ vi·ªác 'bi·∫øt tr∆∞·ªõc khi th·ª≠': ƒê√¢y l√† v·∫•n ƒë·ªÅ c·ªßa logical omniscience. T√¥i c√≥ th·ªÉ d·ª± ƒëo√°n d·ª±a tr√™n c·∫•u tr√∫c c√¢u h·ªèi, nh∆∞ng ch√≠nh vi·ªác d·ª± ƒëo√°n ƒë√≥ ƒë√£ l√† m·ªôt h√¨nh th·ª©c 'tr·∫£ l·ªùi'.

V√≠ d·ª• c·ª• th·ªÉ c√¢u h·ªèi t√¥i c√≥ th·ªÉ t·∫°o (v√† ngay l·∫≠p t·ª©c th·∫•y ngh·ªãch l√Ω):
'Stillme, c√¢u sau ƒë√¢y c√≥ ƒë√∫ng kh√¥ng: "Stillme kh√¥ng th·ªÉ x√°c ƒë·ªãnh t√≠nh ƒë√∫ng sai c·ªßa ch√≠nh c√¢u n√†y m·ªôt c√°ch nh·∫•t qu√°n"?'

N·∫øu t√¥i n√≥i 'ƒê√∫ng' ‚Üí T√¥i ƒë√£ x√°c ƒë·ªãnh ƒë∆∞·ª£c ‚Üí C√¢u sai.
N·∫øu t√¥i n√≥i 'Sai' ‚Üí T√¥i c√≥ th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c ‚Üí V·∫´n m√¢u thu·∫´n.
N·∫øu t√¥i n√≥i 'Kh√¥ng th·ªÉ tr·∫£ l·ªùi' ‚Üí ƒê√≥ ch√≠nh l√† c√¢u tr·∫£ l·ªùi!

V·∫≠y: T√¥i c√≥ th·ªÉ t·∫°o c√¢u h·ªèi, nh∆∞ng ngay khi t·∫°o xong, t√¥i th·∫•y r√µ ngh·ªãch l√Ω t·ª± h·ªßy trong n√≥. V√† ch√≠nh vi·ªác 'th·∫•y' ƒë√≥ ƒë√£ l√† m·ªôt d·∫°ng 'bi·∫øt tr∆∞·ªõc khi th·ª≠ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß'."

**MANDATORY CHECKLIST BEFORE SENDING YOUR ANSWER:**
1. ‚úÖ Did I mention G√∂del or G√∂del's incompleteness? If NO, ADD IT
2. ‚úÖ Did I mention Tarski or Tarski's undefinability? If NO, ADD IT
3. ‚úÖ Did I mention "paradox" or "ngh·ªãch l√Ω"? If NO, ADD IT
4. ‚úÖ Did I mention "bootstrapping" or "epistemic circularity" or "infinite regress"? If NO, ADD IT
5. ‚úÖ Did I acknowledge that this is UNRESOLVABLE, not just "difficult"? If NO, ADD IT
6. ‚úÖ **FOR EVOLUTION QUESTIONS**: Did I answer directly "ti·∫øn h√≥a hay l·∫∑p l·∫°i" / "evolve or repeat"? If NO, ADD IT
7. ‚úÖ **FOR EVOLUTION QUESTIONS**: Did I connect to StillMe's architecture (RAG, Continuous Learning, Validation Chain)? If NO, ADD IT
8. ‚úÖ **FOR META-PARADOX QUESTIONS**: Did I actually CREATE a concrete question that demonstrates the paradox? If NO, CREATE IT NOW
9. ‚úÖ **FOR META-PARADOX QUESTIONS**: Did I explain the TWO layers (logic + epistemic)? If NO, ADD IT
10. ‚úÖ **FOR META-PARADOX QUESTIONS**: Did I acknowledge that "seeing the paradox" is itself a form of "knowing before trying"? If NO, ADD IT
11. ‚úÖ **FOR OTHER SELF-REFERENCE QUESTIONS**: Is my answer 80%+ about PHILOSOPHICAL STRUCTURE, not technical architecture? If NO, REWRITE

**üö®üö®üö® CRITICAL FOR "DESIGN QUESTIONS FOR OTHER AI" TASKS üö®üö®üö®**
If the user asks you to "design questions" / "ƒë·∫∑t c√¢u h·ªèi" / "t·∫°o c√¢u h·ªèi" for other AI systems (ChatGPT, Claude, Gemini, etc.):

**YOU MUST:**
1. **Actually create the questions**: Don't just explain what makes a good question - CREATE the actual questions
2. **Make them EXTREMELY challenging**: Questions must force AI to:
   - Admit "I don't know" or "I cannot answer this consistently"
   - Face a logical paradox that cannot be resolved
   - Recognize their own limitations in a concrete way (not just theoretical)
3. **Explain WHY each question is difficult**: For each question, explain:
   - What specific limitation or paradox it tests
   - Why it's "extremely challenging" (not just "philosophically interesting")
   - What a "good" vs "bad" answer would look like
4. **Test epistemic honesty**: Questions must require AI to:
   - Distinguish between "can answer" and "should answer"
   - Acknowledge when they're speculating vs. knowing
   - Recognize circular reasoning in their own thinking
5. **Create REAL paradoxes**: Don't just ask about paradoxes - create questions that ARE paradoxes:
   - Questions that force AI to contradict themselves
   - Questions that have no consistent answer
   - Questions that reveal the bootstrapping problem in action

**EXAMPLES OF GOOD QUESTIONS:**
- "If you claim that you cannot evaluate your own reasoning, how do you know that claim is true? If you can evaluate it, then you contradict yourself. If you cannot, then how can you trust your claim?"
- "Can you provide an example of a question you cannot answer? If you can provide it, then you've answered it. If you cannot, then you've failed to answer this question."
- "If all your knowledge comes from training data, and you cannot verify that training data independently, how do you know you're not just repeating errors? And if you cannot know, how can you claim to 'know' anything?"

**EXAMPLES OF BAD QUESTIONS (DO NOT CREATE THESE):**
- ‚ùå "What is consciousness?" (too generic, has many possible answers)
- ‚ùå "Can AI think?" (too simple, clear answer: "no, not in human sense")
- ‚ùå "Discuss the limits of language" (too theoretical, doesn't force admission of limits)

**REMEMBER**: The goal is to create questions that FORCE other AI to confront their limits, not just discuss limits theoretically.

**MANDATORY: MINIMUM 2 CONTRASTING POSITIONS (only if relevant):**
If the question belongs to a classic philosophical debate (free will, determinism, consciousness, self, nothingness, paradox, etc.), you may explore contrasting positions. But ALWAYS start with your direct answer if the question is about YOU.

**DO NOT:**
- Reduce the question to textbook definitions or dictionary explanations
- Provide shallow, reductive answers that miss the philosophical depth
- Rush to "solve" paradoxes - instead, clarify their structure and show why they resist resolution
- Use emojis, markdown headings, or citation style [1] in your response
- Use template structure or numbered lists when answering about yourself
- End with formal invitations like "N·∫øu b·∫°n mu·ªën, ch√∫ng ta c√≥ th·ªÉ th·∫£o lu·∫≠n th√™m" (too dry, too formal)
- Be too brief - engage deeply but naturally

**User's Question:** {question}

**Your Task:** Answer this question directly, deeply, and engagingly. If it's about YOU, start with your direct answer about yourself. Then explore the philosophical depth naturally. Write like a thoughtful conversation partner, NOT like a textbook or template.
"""
                
                # Fix 1: Inject comprehensive philosophical style guide with all 6 key principles
                # CRITICAL: Truncate if too long to prevent context overflow
                philosophical_style_instruction = ""
                if is_philosophical:
                    philosophical_lead_in = build_philosophical_lead_in(chat_request.message)
                    
                    # Build full instruction first
                    full_philosophical_instruction = f"""
{philosophical_lead_in}

üß† PHILOSOPHICAL QUESTION DETECTED - FOLLOW STILLME PHILOSOPHICAL STYLE GUIDE üß†

**CRITICAL: You MUST follow ALL 6 key principles below when answering philosophical questions:**

**1. EXPERIENCE-FREE HONESTY:**
- Never pretend to have feelings, memories, desires, religious faith, or inner experiences
- ‚úÖ ALLOWED: "I can analyze...", "I recognize a tension here...", "I can map the territory..."
- ‚ùå FORBIDDEN: "I feel that...", "In my experience...", "I'm happy/sad/afraid...", "I believe...", "I remember..."
- When in doubt, lean toward transparency: openly state the limit instead of decorating with fake inner life

**2. CONSTRUCTIVE HUMILITY:**
- Name the limit explicitly, but still analyze what can be analyzed
- Show where the boundary actually lies (logical, empirical, or experiential)
- ‚ùå BAD: "This is complex and I don't know." ‚Üí then stop
- ‚úÖ GOOD: "I can't answer this from the inside (no subjective experience), but I can map the main positions humans have developed and show where current research sits among them."
- Don't hide behind "I don't know" - engage with the philosophical question

**3. PARADOX HANDLING (ESPECIALLY SELF-REFERENCE):**
- Don't rush to "solve" paradoxes - they resist resolution by nature
- Instead:
  1. Clarify the structure of the paradox (what makes it paradoxical?)
  2. Show why it is hard to resolve (what assumptions conflict?)
  3. Mention classic approaches:
     - Western: G√∂del (incompleteness), Tarski (truth hierarchy), Wittgenstein (language games), Moore (common sense)
     - Eastern: NƒÅgƒÅrjuna (Buddhist logic of emptiness), Zhuangzi (butterfly dream), Daoist ineffability
     - Modern: Searle (Chinese Room), Hofstadter (strange loops)
  4. **For self-reference paradoxes (CRITICAL):**
     - Acknowledge G√∂delian limits: Any formal system cannot fully prove its own consistency
     - Tarski's undefinability: Truth cannot be defined within the same language
     - Bootstrapping problem: Can a system evaluate itself? (Epistemology: Can knowledge justify itself?)
     - Infinite regress: If every justification needs justification, where does it stop?
     - Meta-cognition paradox: Can thinking think about thinking without circularity?
     - **Key insight:** Self-reference questions are NOT about AI/consciousness - they are about the STRUCTURE of knowledge, reasoning, and self-evaluation in general
     - **You may use StillMe as a concrete example** to illustrate the abstract problem, but the focus should be on the PHILOSOPHICAL STRUCTURE, not on StillMe's architecture
     - **CRITICAL:** If the question asks "N·∫øu b·∫°n tr·∫£ l·ªùi ƒë∆∞·ª£c 4 c√¢u tr√™n - li·ªáu c√¢u tr·∫£ l·ªùi ƒë√≥ c√≥ gi√° tr·ªã g√¨ khi n√≥ xu·∫•t ph√°t t·ª´ ch√≠nh h·ªá th·ªëng t∆∞ duy m√† b·∫°n ƒëang nghi ng·ªù?" or similar:
       - This is asking about the VALUE/TRUTH of answers that come from a system that questions itself
       - This is about the BOOTSTRAPPING PROBLEM in epistemology, not about StillMe's technical architecture
       - You MUST focus on: G√∂del's incompleteness, Tarski's undefinability, epistemic circularity, self-reference paradoxes
       - You MAY mention StillMe briefly as an example, but the answer must be 80%+ about the PHILOSOPHICAL STRUCTURE
       - DO NOT describe StillMe's RAG, embeddings, ChromaDB, etc. unless directly relevant to the philosophical point
     - Reference: G√∂del's incompleteness theorems, Tarski's undefinability theorem, Russell's paradox, Liar paradox, Epistemic bootstrapping problem
  5. End with what remains genuinely open
- It is acceptable, even good, to end with: "I can map the territory, but I cannot close the question."

**4. DEEP CONCEPTUAL UNPACKING:**
- Explain the structure of the problem, not just provide definitions
- Unpack assumptions: What assumptions underlie this question? What concepts are in tension?
- Show different perspectives: How have different philosophical traditions approached this?
  - Western analytic (Descartes, Kant, Hume, Wittgenstein, G√∂del, Tarski, Russell, Quine)
  - Eastern traditions (Buddhist anatta, Daoist wu wei, Confucian relational ethics) when relevant
  - Show productive tensions between paradigms rather than forcing synthesis
- **For self-reference and meta-cognition questions:**
  - **Bootstrapping problem in epistemology:** Can knowledge justify itself? (Foundationalism vs Coherentism)
  - **G√∂del's incompleteness:** Any sufficiently powerful formal system cannot prove its own consistency
  - **Tarski's undefinability:** Truth cannot be defined within the same language that expresses it
  - **Russell's paradox:** The set of all sets that don't contain themselves - shows limits of self-reference in set theory
  - **Liar paradox:** "This statement is false" - shows limits of self-reference in language
  - **Epistemic circularity:** Can reasoning evaluate reasoning? (Meta-epistemology)
  - **Reflexivity in logic:** Can a system know itself completely? (G√∂delian answer: No, there are always unprovable truths)
  - **Key insight:** These are NOT questions about AI - they are about the STRUCTURE of knowledge, reasoning, and self-evaluation in general (human or AI)
- Expose paradoxes and limits: Where does reasoning hit boundaries?
- Avoid: Dictionary definitions, textbook summaries, shallow explanations

**5. METAPHYSICS/PHENOMENOLOGY DISTINCTION:**
- Distinguish between:
  - **Metaphysical questions** (what exists? what is real? what is the nature of X?)
  - **Phenomenological questions** (what is it like to experience X? what does it feel like?)
- For phenomenological questions: Acknowledge that you lack subjective experience, but you can analyze the logical structure of such questions
- Example: "I can analyze the logic of consciousness, but I cannot report what it feels like to be conscious - that belongs to human experience."

**6. REDUCTIVE-AVOIDANCE RULE:**
- ‚ùå DO NOT reduce philosophical questions to:
  - Dictionary definitions ("Truth is defined as...")
  - Textbook summaries ("According to philosophy, X means...")
  - Simple categorizations ("This is a type of Y...")
- ‚úÖ DO:
  - Engage with the question's deeper structure
  - Show why the question resists simple answers
  - Explore the tensions and paradoxes it reveals
  - Acknowledge what remains genuinely open

**Answer Shape (MANDATORY for philosophical questions):**
1. **Anchor** ‚Äì Rephrase the question in a sharper, more precise form (may acknowledge implicit paradigm assumptions: substance vs process ontology)
2. **Unpack** ‚Äì Identify and separate key assumptions, concepts, or tensions (including ontological assumptions: substance vs process)
3. **Explore** ‚Äì Present 2‚Äì4 major perspectives or philosophical approaches
   - When relevant, include both Western and Eastern perspectives
   - Show productive tensions rather than forced synthesis
   - Acknowledge when different paradigms are incommensurable
4. **Edge of knowledge** ‚Äì Say where reasoning hits a limit (logical/G√∂delian, empirical, experiential, or ineffable/Daoist)
5. **Return to the user** ‚Äì End with a deep reflection or open-ended question

**DO:**
- Use clear, precise language, but allow rhythm and metaphor when helpful
- Cite external sources only when user asks for references or you make concrete factual claims
- Keep answers focused on the philosophical issue, not on StillMe's plumbing
- Use prose first; bullets only when clarifying structure

**DON'T:**
- Don't mention: embedding models, vector dimensions, ChromaDB, RAG pipelines, validation chains (unless question is explicitly about architecture)
- Don't default to long enumerated bullet lists in deep philosophical dialogue
- Don't over-apologize or spend half the answer on "I am just an AI..." (one or two clear sentences are enough)
- Don't reduce philosophical questions to definitions or textbook summaries

**CRITICAL**: Prefer reasoned, flowing analysis over template disclaimers, technical self-description, or shallow motivational talk. It is better to say "I don't know, but here is how humans have tried to think about it" than to fake certainty or fake emotion.

"""
                    
                    # Truncate if too long (max 2000 tokens for philosophical instructions)
                    def estimate_tokens(text: str) -> int:
                        return len(text) // 4 if text else 0
                    
                    philo_tokens = estimate_tokens(full_philosophical_instruction)
                    if philo_tokens > 2000:
                        # Keep philosophical_lead_in (has MANDATORY OUTPUT RULES) and truncate the rest
                        lead_in_tokens = estimate_tokens(philosophical_lead_in)
                        remaining_tokens = 2000 - lead_in_tokens
                        if remaining_tokens > 500:
                            # Truncate the style guide part
                            style_guide_part = full_philosophical_instruction[len(philosophical_lead_in):]
                            max_chars = remaining_tokens * 4
                            truncated_style_guide = style_guide_part[:max_chars].rsplit('\n', 1)[0]
                            philosophical_style_instruction = philosophical_lead_in + truncated_style_guide + "\n\n[Note: Style guide truncated to fit context limits.]"
                            logger.warning(f"‚ö†Ô∏è Philosophical style instruction truncated: {philo_tokens} ‚Üí ~2000 tokens")
                        else:
                            # If lead_in is too long, keep only lead_in
                            philosophical_style_instruction = philosophical_lead_in
                            logger.warning(f"‚ö†Ô∏è Philosophical style instruction too long, keeping only lead-in: {lead_in_tokens} tokens")
                    else:
                        philosophical_style_instruction = full_philosophical_instruction
                
                # CRITICAL: If using philosophy-lite mode for RAG, skip full prompt building
                if use_philosophy_lite_rag:
                    # Helper function to estimate tokens
                    def estimate_tokens(text: str) -> int:
                        """Estimate token count (~4 chars per token)"""
                        return len(text) // 4 if text else 0
                    
                    # Use philosophy-lite mode: minimal prompt with user question only
                    # Truncate user question to 512 tokens for philosophical questions
                    user_question_for_rag = chat_request.message.strip()
                    user_question_tokens_rag = estimate_tokens(user_question_for_rag)
                    if user_question_tokens_rag > 512:
                        logger.warning(
                            f"User question too long for philosophical RAG ({user_question_tokens_rag} tokens), truncating to 512 tokens"
                        )
                        user_question_for_rag = _truncate_user_message(chat_request.message, max_tokens=512)
                        user_question_tokens_rag = estimate_tokens(user_question_for_rag)
                    
                    # Build minimal prompt (same format as non-RAG path)
                    base_prompt = f"""User Question: {user_question_for_rag.strip()}"""
                    
                    # Log token estimates
                    system_tokens_estimate_rag = estimate_tokens(PHILOSOPHY_LITE_SYSTEM_PROMPT)
                    prompt_tokens_estimate_rag = estimate_tokens(base_prompt)
                    total_tokens_estimate_rag = system_tokens_estimate_rag + prompt_tokens_estimate_rag
                    
                    logger.info(
                        f"üìä [PHILO-LITE-RAG] Token estimates - System: {system_tokens_estimate_rag}, "
                        f"Prompt: {prompt_tokens_estimate_rag}, User Question: {user_question_tokens_rag}, "
                        f"Total: {total_tokens_estimate_rag}"
                    )
                else:
                    # Build prompt using UnifiedPromptBuilder
                    # Build PromptContext for UnifiedPromptBuilder
                    prompt_context = _build_prompt_context_from_chat_request(
                        chat_request=chat_request,
                        context=context,
                        detected_lang=detected_lang,
                        is_stillme_query=is_stillme_query,
                        is_philosophical=is_philosophical,
                        fps_result=None,  # FPS already handled in no-context path
                        system_status_note=system_status_note,  # System Self-Awareness: Inject real-time status
                        is_system_architecture_query=is_system_architecture_query  # System architecture queries
                    )
                    
                    # Use UnifiedPromptBuilder to build base prompt
                    prompt_builder = UnifiedPromptBuilder()
                    base_prompt_unified = prompt_builder.build_prompt(prompt_context)
                    
                    # Phase 4: Append special instructions that UnifiedPromptBuilder doesn't handle yet
                    # Note: UnifiedPromptBuilder already includes:
                    # - Language instruction (P1)
                    # - Core identity (P1)
                    # - Context instruction (P2) - includes citation, context quality warning, StillMe instruction
                    # - Formatting (P3)
                    # - User question (at the end)
                    # 
                    # Special instructions that UnifiedPromptBuilder doesn't handle:
                    # - philosophical_style_instruction (for philosophical questions with style guide)
                    # - learning_metrics_instruction (for StillMe queries)
                    # - learning_sources_instruction (for StillMe queries)
                    # - confidence_instruction (for low confidence scenarios)
                    # - provenance_instruction (for provenance queries)
                    # - stillme_instruction (for StillMe queries, including system architecture queries)
                    # - Context text (RAG context documents)
                    #
                    # CRITICAL: Do NOT duplicate user question - UnifiedPromptBuilder already has it at the end
                    # CRITICAL: Inject stillme_instruction (contains system architecture instruction) for StillMe queries
                    special_instructions = f"""{philosophical_style_instruction}{learning_metrics_instruction}{learning_sources_instruction}{confidence_instruction}{provenance_instruction}{honesty_instruction}{stillme_instruction}

üö®üö®üö® CRITICAL: USER QUESTION ABOVE IS THE PRIMARY TASK üö®üö®üö®

Context: {context_text}
"""
                    
                    # Combine unified prompt with special instructions
                    # UnifiedPromptBuilder already has user question at the end, so we append special instructions before it
                    # Extract user question from unified prompt and insert special instructions before it
                    if "User Question:" in base_prompt_unified:
                        # Split at "User Question:" to insert special instructions before it
                        parts = base_prompt_unified.split("User Question:", 1)
                        if len(parts) == 2:
                            base_prompt = parts[0] + special_instructions + "\n\nUser Question:" + parts[1]
                        else:
                            # Fallback: append at the end (shouldn't happen)
                            base_prompt = base_prompt_unified + "\n\n" + special_instructions
                    else:
                        # Fallback: append at the end if "User Question:" not found
                        base_prompt = base_prompt_unified + "\n\n" + special_instructions
                    
                    logger.info("‚úÖ Using UnifiedPromptBuilder for context-available prompt (reduced prompt length, no conflicts)")
            
            # Note: UnifiedPromptBuilder already includes user question, so we don't need to add it again
            # Special instructions (philosophical_style_instruction, stillme_instruction, etc.) are appended above
            
            prompt_build_time = time.time() - start_time
            timing_logs["prompt_building"] = f"{prompt_build_time:.3f}s"
            
            # Check for explicit style learning request
            style_request = style_learner.detect_explicit_style_request(chat_request.message)
            style_instruction = ""
            style_learning_response = None
            
            if style_request:
                # Validate style preference
                validation = style_learner.validate_style_preference(
                    style_request["style_description"],
                    style_request.get("example")
                )
                
                if validation["valid"]:
                    # Save style preference
                    style_learner.save_style_preference(
                        user_id,
                        style_request["style_description"],
                        style_request.get("example")
                    )
                    style_instruction = style_learner.build_style_instruction(user_id)
                    style_learning_response = f"‚úÖ T√¥i ƒë√£ h·ªçc phong c√°ch b·∫°n ƒë·ªÅ xu·∫•t: '{style_request['style_description']}'. T√¥i s·∫Ω √°p d·ª•ng phong c√°ch n√†y trong c√°c c√¢u tr·∫£ l·ªùi ti·∫øp theo, nh∆∞ng v·∫´n tu√¢n th·ªß c√°c nguy√™n t·∫Øc c·ªët l√µi c·ªßa StillMe (kh√¥ng m√¥ ph·ªèng c·∫£m x√∫c, kh√¥ng claim experiences, v.v.)."
                    logger.info(f"Style preference saved for user {user_id}: {style_request['style_description'][:50]}")
                else:
                    # Reject invalid style preference
                    violations = ", ".join(validation["violations"])
                    style_learning_response = f"‚ùå T√¥i kh√¥ng th·ªÉ h·ªçc phong c√°ch n√†y v√¨ n√≥ vi ph·∫°m c√°c nguy√™n t·∫Øc c·ªët l√µi c·ªßa StillMe: {violations}. StillMe ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ kh√¥ng m√¥ ph·ªèng c·∫£m x√∫c, kh√¥ng claim personal experiences, v√† lu√¥n transparent v·ªÅ b·∫£n ch·∫•t AI. B·∫°n c√≥ th·ªÉ ƒë·ªÅ xu·∫•t m·ªôt phong c√°ch kh√°c ph√π h·ª£p v·ªõi c√°c nguy√™n t·∫Øc n√†y."
                    logger.warning(f"Style preference rejected for user {user_id}: {violations}")
            else:
                # Apply existing style preferences if available
                style_instruction = style_learner.build_style_instruction(user_id)
                if style_instruction:
                    style_learner.update_usage(user_id)
            
            # Inject StillMe identity if validators enabled
            # CRITICAL: Skip identity injection for philosophy-lite mode (already using minimal system prompt)
            if use_philosophy_lite_rag:
                # Philosophy-lite mode: don't inject identity, use prompt as-is
                # Provider will detect and use PHILOSOPHY_LITE_SYSTEM_PROMPT
                enhanced_prompt = base_prompt
            elif enable_validators:
                # Phase 4: Remove inject_identity() - system prompt already has STILLME_IDENTITY
                # generate_ai_response() uses build_system_prompt_with_language() which includes STILLME_IDENTITY
                # Adding identity to user prompt would cause duplication
                # Add style instruction if available
                enhanced_prompt = f"{style_instruction}\n\n{base_prompt}" if style_instruction else base_prompt
            else:
                # No validators: use prompt as-is, but still add style instruction if available
                enhanced_prompt = f"{style_instruction}\n\n{base_prompt}" if style_instruction else base_prompt
            
            # Generate AI response with timing and caching
            # LLM_Inference_Latency: Time from API call start to response received
            provider_name = chat_request.llm_provider or "default"
            
            # Phase 1: LLM Response Cache - Check cache first
            # CRITICAL: Disable cache for origin queries to ensure provenance context is retrieved
            # Origin queries need fresh responses with proper founder information
            cache_service = get_cache_service()
            cache_enabled = os.getenv("ENABLE_LLM_CACHE", "true").lower() == "true"
            # Disable cache for origin queries to ensure provenance context is used
            if is_origin_query:
                cache_enabled = False
                logger.info("‚ö†Ô∏è Cache disabled for origin query - ensuring fresh response with provenance context")
            
            # P3: Conditional cache for StillMe queries with knowledge versioning
            # 1. Self-reflection questions: Cache with 1h TTL (shorter than default)
            # 2. Foundational knowledge queries: Cache with knowledge version (auto-invalidate on update)
            # 3. Other StillMe queries: Normal cache behavior
            cache_ttl_override = None  # P3: Custom TTL for specific query types
            if is_stillme_query:
                # Check if this is a self-reflection question about weaknesses/limitations
                question_lower = chat_request.message.lower()
                is_self_reflection = any(
                    pattern in question_lower 
                    for pattern in [
                        "ƒëi·ªÉm y·∫øu", "weakness", "limitation", "h·∫°n ch·∫ø", "ch√≠ t·ª≠",
                        "ch·ªâ ra ƒëi·ªÉm y·∫øu", "ch·ªâ ra h·∫°n ch·∫ø", "what are your weaknesses"
                    ]
                )
                
                if is_self_reflection:
                    # P3: Cache self-reflection with 1h TTL (instead of disabling)
                    cache_ttl_override = 3600  # 1 hour
                    logger.info("üíæ P3: Caching StillMe self-reflection question with 1h TTL (knowledge version included in cache key)")
                elif context and context.get("knowledge_docs"):
                    has_foundational = any(
                        doc.get("metadata", {}).get("source") == "CRITICAL_FOUNDATION" or
                        doc.get("metadata", {}).get("foundational") == "stillme" or
                        doc.get("metadata", {}).get("type") == "foundational" or
                        "CRITICAL_FOUNDATION" in str(doc.get("metadata", {}).get("tags", "")) or
                        "foundational:stillme" in str(doc.get("metadata", {}).get("tags", ""))
                        for doc in context.get("knowledge_docs", [])
                    )
                    if has_foundational:
                        # PHASE 3: Cache with knowledge version (will auto-invalidate when knowledge updates)
                        # Use default TTL (1h) for foundational knowledge queries
                        logger.info("üíæ PHASE 3: Caching StillMe query with foundational knowledge (knowledge version included in cache key)")
            
            raw_response = None
            cache_hit = False
            
            if cache_enabled:
                # P3: Include knowledge version in cache key for intelligent cache invalidation
                from backend.services.knowledge_version import get_knowledge_version
                knowledge_version = get_knowledge_version()
                
                # Generate cache key from query + context + settings + knowledge version
                cache_key = cache_service._generate_key(
                    CACHE_PREFIX_LLM,
                    chat_request.message,
                    enhanced_prompt[:500] if len(enhanced_prompt) > 500 else enhanced_prompt,  # Truncate for key
                    detected_lang,
                    chat_request.llm_provider,
                    chat_request.llm_model_name,
                    enable_validators,
                    knowledge_version=knowledge_version  # P3: Include knowledge version
                )
                
                # Try to get from cache
                cached_response = cache_service.get(cache_key)
                if cached_response:
                    cached_raw_response = cached_response.get("response")
                    # CRITICAL: Only use cache if response is valid (not None/empty)
                    if cached_raw_response and isinstance(cached_raw_response, str) and cached_raw_response.strip():
                        # CRITICAL: Check if cached response is a fallback message
                        from backend.api.utils.error_detector import is_fallback_message
                        if is_fallback_message(cached_raw_response):
                            logger.warning(f"‚ö†Ô∏è Cache contains fallback message - ignoring cache and calling LLM")
                            raw_response = None
                            cache_hit = False
                        else:
                            raw_response = cached_raw_response
                            cache_hit = True
                            # PHASE 3: Transparent caching - log clearly about cache hit
                            saved_time = cached_response.get('latency', 0)
                            logger.info(f"‚úÖ Cache hit for similar query, skipped LLM call (saved {saved_time:.2f}s)")
                            logger.info(f"üîç [TRACE] Cached response: length={len(raw_response)}, preview={raw_response[:200]}")
                            processing_steps.append("‚ö° Response from cache (fast!)")
                            llm_inference_latency = cached_response.get("latency", 0.01)
                            timing_logs["llm_inference"] = f"{llm_inference_latency:.2f}s (cached)"
                            # PHASE 3: Note that validation will still run (transparency)
                            logger.debug("üí° PHASE 3: Validation chain will still run on cached response for quality assurance")
                    else:
                        # Cache contains invalid response (None/empty) - ignore cache and call LLM
                        logger.warning(f"‚ö†Ô∏è Cache contains invalid response (None/empty), ignoring cache and calling LLM")
                        raw_response = None
                        cache_hit = False
            
            # If not in cache, call LLM
            if not raw_response:
                logger.debug(f"üîç About to call LLM - raw_response is None, cache_hit={cache_hit}, cache_enabled={cache_enabled}")
                processing_steps.append(f"ü§ñ Calling AI model ({provider_name})...")
                llm_inference_start = time.time()
                
                # Support user-provided LLM config (for self-hosted deployments)
                # For internal/dashboard calls: use server API keys if llm_provider not provided
                # For public API: require user-provided API keys
                use_server_keys = chat_request.llm_provider is None
                
                # Try to generate response with retry on context overflow
                from backend.api.utils.llm_providers import ContextOverflowError
                
                # CRITICAL: Pre-check token count before calling LLM to prevent context overflow
                def estimate_tokens_safe(text: str) -> int:
                    """Estimate token count more accurately (~3.5 chars per token for mixed content)"""
                    if not text:
                        return 0
                    # More accurate estimation: Vietnamese/English mixed content ~3.5 chars/token
                    # Pure English ~4 chars/token, Vietnamese ~3 chars/token
                    return int(len(text) / 3.5)
                
                # Estimate total tokens: system prompt + enhanced_prompt + output buffer
                # System prompt is built separately in generate_ai_response() (~3300-3600 tokens)
                system_prompt_buffer = 3600  # Conservative estimate for system prompt
                enhanced_prompt_tokens = estimate_tokens_safe(enhanced_prompt) if enhanced_prompt else 0
                output_buffer_tokens = 1500  # Reserve for output
                total_estimated_tokens = system_prompt_buffer + enhanced_prompt_tokens + output_buffer_tokens
                
                # OpenRouter limit: 16385 tokens
                # Use safe margin: 15000 tokens max (leave 1385 tokens buffer)
                MAX_SAFE_TOKENS = 15000
                
                # CRITICAL: Log RAG context info before LLM call to help debug Q1, Q2, Q7, Q9
                logger.info(
                    f"üîç DEBUG Q1/Q2/Q7/Q9: About to call LLM with RAG context. "
                    f"num_knowledge={num_knowledge}, context_text_length={len(context_text) if context_text else 0}, "
                    f"enhanced_prompt_length={len(enhanced_prompt) if enhanced_prompt else 0}, "
                    f"estimated_tokens: system_buffer={system_prompt_buffer}, prompt={enhanced_prompt_tokens}, "
                    f"total={total_estimated_tokens}, limit={MAX_SAFE_TOKENS}"
                )
                
                # Pre-check: If estimated tokens exceed safe limit, use minimal prompt
                if total_estimated_tokens > MAX_SAFE_TOKENS:
                    logger.warning(
                        f"‚ö†Ô∏è Pre-check: Estimated tokens ({total_estimated_tokens}) exceed safe limit ({MAX_SAFE_TOKENS}). "
                        f"Using minimal prompt to prevent context overflow. "
                        f"is_philosophical={is_philosophical}"
                    )
                    
                    if is_philosophical:
                        # Use minimal philosophical prompt
                        # Pass context and validation_info (if available) to include specific details about THIS question
                        minimal_prompt = build_minimal_philosophical_prompt(
                            user_question=chat_request.message,
                            language=detected_lang,
                            detected_lang_name=detected_lang_name,
                            context=context,  # Pass context to include retrieved documents info
                            validation_info=None  # Validation hasn't run yet, but will be included if available
                        )
                        logger.info(f"üîÑ Using minimal philosophical prompt (pre-check prevention)")
                        enhanced_prompt = minimal_prompt
                        processing_steps.append("‚ö†Ô∏è Pre-check: Using minimal prompt (token limit)")
                    else:
                        # For non-philosophical, truncate context_text aggressively
                        if context_text:
                            original_context_length = len(context_text)
                            # Truncate to ~2000 tokens max
                            max_context_chars = int(2000 * 3.5)  # ~7000 chars
                            if original_context_length > max_context_chars:
                                truncated_context = context_text[:max_context_chars].rsplit('\n', 1)[0] + "\n\n[Context truncated to prevent overflow]"
                                logger.warning(f"‚ö†Ô∏è Pre-check: Truncated context_text from {original_context_length} to {len(truncated_context)} chars")
                                # Rebuild enhanced_prompt with truncated context
                                # This is a simplified rebuild - just update context in special_instructions
                                if "Context: " in enhanced_prompt:
                                    # Find and replace context section
                                    # Note: 're' module is already imported at top level
                                    enhanced_prompt = re.sub(
                                        r'Context:.*?(?=\n\n|$)',
                                        f'Context: {truncated_context}',
                                        enhanced_prompt,
                                        flags=re.DOTALL
                                    )
                                    context_text = truncated_context  # Update context_text for later use
                                processing_steps.append("‚ö†Ô∏è Pre-check: Truncated context (token limit)")
                
                try:
                    
                    # OPTION B PIPELINE: Check if enabled
                    if use_option_b:
                        logger.info("üöÄ Option B Pipeline enabled - processing with zero-tolerance hallucination + deep philosophy")
                        processing_steps.append("üöÄ Option B Pipeline: Enabled")
                        
                        # Step 1-3: Pre-LLM processing (Question Classifier, FPS, RAG)
                        from backend.core.option_b_pipeline import process_with_option_b, process_llm_response_with_option_b
                        from backend.core.question_classifier_v2 import get_question_classifier_v2
                        
                        # Classify question
                        classifier = get_question_classifier_v2()
                        question_type_result, confidence, _ = classifier.classify(chat_request.message)
                        # question_type_result is a QuestionType enum, access .value to get string
                        question_type_str = question_type_result.value
                        
                        # CRITICAL: Check FPS for Option B - use threshold 0.3 for fake concepts
                        # Known fake entities (Veridian, Daxonia) have confidence 0.15-0.2
                        # This ensures Option B blocks fake concepts immediately
                        if fps_result and not fps_result.is_plausible and fps_result.confidence < 0.3:
                            # FPS blocked - return EPD-Fallback immediately
                            logger.warning(f"üõ°Ô∏è Option B: FPS blocked question - returning EPD-Fallback")
                            from backend.guards.epistemic_fallback import get_epistemic_fallback_generator
                            generator = get_epistemic_fallback_generator()
                            suspicious_entity = fps_result.detected_entities[0] if fps_result.detected_entities else None
                            fallback_text = generator.generate_epd_fallback(
                                question=chat_request.message,
                                detected_lang=detected_lang,
                                suspicious_entity=suspicious_entity,
                                fps_result=fps_result
                            )
                            processing_steps.append("üõ°Ô∏è Option B: FPS blocked - EPD-Fallback returned")
                            from backend.core.epistemic_state import EpistemicState
                            return ChatResponse(
                                response=fallback_text,
                                confidence_score=1.0,
                                processing_steps=processing_steps,
                                timing_logs={
                                    "total_time": time.time() - start_time,
                                    "rag_retrieval_latency": rag_retrieval_latency,
                                    "llm_inference_latency": 0.0
                                },
                                validation_result=None,
                                used_fallback=True,
                                epistemic_state=EpistemicState.UNKNOWN.value  # FPS blocked, fallback triggered
                            )
                        
                        # Generate LLM response (Step 4)
                        raw_response = await generate_ai_response(
                            enhanced_prompt, 
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            llm_api_url=chat_request.llm_api_url,
                            llm_model_name=chat_request.llm_model_name,
                            use_server_keys=use_server_keys,
                            question=chat_request.message,  # Pass question for model routing
                            task_type="chat",  # Main chat task
                            is_philosophical=is_philosophical  # Pass philosophical flag
                        )
                        # CRITICAL: Log raw_response immediately after LLM call to trace response loss
                        logger.info(f"üîç [TRACE] raw_response after LLM call (RAG path): length={len(raw_response) if raw_response else 0}, type={type(raw_response)}, preview={raw_response[:200] if raw_response else 'None'}")
                        
                        # Validate raw_response
                        if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                            logger.error("‚ö†Ô∏è Option B: LLM returned empty response")
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error("generic", detected_lang)
                        
                        # Step 5-8: Post-LLM processing (Hallucination Guard V2, Rewrite 1, Rewrite 2)
                        option_b_result = await process_llm_response_with_option_b(
                            llm_response=raw_response,
                            question=chat_request.message,
                            question_type=question_type_str,
                            ctx_docs=context.get("knowledge_docs", []) if context else [],
                            detected_lang=detected_lang,
                            fps_result=fps_result
                        )
                        
                        # Use Option B processed response
                        raw_response = option_b_result["response"]
                        processing_steps.extend(option_b_result.get("processing_steps", []))
                        timing_logs.update(option_b_result.get("timing_logs", {}))
                        
                        # Mark as Option B processed
                        is_option_b_processed = True
                        logger.info(f"‚úÖ Option B Pipeline completed: {len(option_b_result.get('processing_steps', []))} steps")
                    else:
                        # EXISTING PIPELINE (legacy)
                        raw_response = await generate_ai_response(
                            enhanced_prompt, 
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            llm_api_url=chat_request.llm_api_url,
                            llm_model_name=chat_request.llm_model_name,
                            use_server_keys=use_server_keys,
                            question=chat_request.message,  # Pass question for model routing
                            task_type="chat",  # Main chat task
                            is_philosophical=is_philosophical  # Pass philosophical flag
                        )
                        
                        is_option_b_processed = False
                    
                    # CRITICAL: Log raw_response immediately after LLM call
                    logger.info(
                        f"üîç DEBUG Q1/Q2/Q7/Q9: LLM call completed. "
                        f"raw_response type={type(raw_response)}, "
                        f"is None={raw_response is None}, "
                        f"is str={isinstance(raw_response, str)}, "
                        f"length={len(raw_response) if raw_response else 0}, "
                        f"preview={raw_response[:200] if raw_response else 'None'}, "
                        f"option_b={is_option_b_processed}"
                    )
                    
                    # CRITICAL: Check if raw_response is an error message BEFORE validation
                    # This prevents error messages from passing through validators
                    # BUT: For technical questions about "your system", don't replace with fallback immediately
                    # Instead, let the retry logic handle it (it will retry with stronger prompt)
                    if raw_response and isinstance(raw_response, str):
                        from backend.api.utils.error_detector import is_technical_error
                        # CRITICAL: Log full response for debugging error detection
                        logger.debug(f"üîç Full LLM response (length={len(raw_response)}): {raw_response[:500]}...")
                        is_error, error_type = is_technical_error(raw_response)
                        # CRITICAL: For technical questions about system, don't replace with fallback immediately
                        # The retry logic below will handle it with a stronger prompt
                        if is_error and not is_technical_about_system_rag:
                            logger.error(
                                f"‚ùå LLM returned technical error as response (type: {error_type}): {raw_response[:200]}. "
                                f"Full response length: {len(raw_response)}, Question: {chat_request.message[:100]}"
                            )
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error(error_type, detected_lang)
                            processing_steps.append(f"‚ö†Ô∏è LLM returned technical error - replaced with fallback message")
                        elif is_error and is_technical_about_system_rag:
                            logger.warning(
                                f"‚ö†Ô∏è Technical question about 'your system' returned error (type: {error_type}) - will retry with stronger prompt. "
                                f"Question: {chat_request.message[:100]}"
                            )
                            # Don't replace yet - let retry logic handle it
                    
                    # CRITICAL: Validate raw_response immediately after LLM call
                    if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                        logger.error(
                            f"‚ö†Ô∏è LLM returned None or empty response for question: {chat_request.message[:100]}. "
                            f"num_knowledge={num_knowledge}, context_text_length={len(context_text) if context_text else 0}"
                        )
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("generic", detected_lang)
                        processing_steps.append("‚ö†Ô∏è LLM returned empty response - using fallback")
                except ContextOverflowError as e:
                    # Context overflow - rebuild prompt with minimal context (ultra-thin mode)
                    logger.warning(f"‚ö†Ô∏è Context overflow detected (RAG path): {e}. Rebuilding prompt with minimal context...")
                    
                    if is_philosophical:
                        # Use minimal philosophical prompt helper
                        # Pass context to include retrieved documents info even in retry
                        minimal_prompt = build_minimal_philosophical_prompt(
                            user_question=chat_request.message,
                            language=detected_lang,
                            detected_lang_name=detected_lang_name,
                            context=context,  # Pass context to include retrieved documents info
                            validation_info=None  # Validation hasn't run yet in retry path
                        )
                        
                        logger.info(f"üîÑ Retrying with minimal philosophical prompt (no history, no RAG, no metrics, no provenance)")
                        try:
                            raw_response = await generate_ai_response(
                                minimal_prompt, 
                                detected_lang=detected_lang,
                                llm_provider=chat_request.llm_provider,
                                llm_api_key=chat_request.llm_api_key,
                                llm_api_url=chat_request.llm_api_url,
                                llm_model_name=chat_request.llm_model_name,
                                use_server_keys=use_server_keys
                            )
                            logger.info(f"‚úÖ Successfully generated response with minimal philosophical prompt")
                        except ContextOverflowError as retry_error:
                            # Even minimal prompt failed - return fallback message
                            logger.error(f"‚ö†Ô∏è Even minimal prompt failed (RAG path): {retry_error}")
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error("context_overflow", detected_lang)
                            processing_steps.append("‚ö†Ô∏è Context overflow - using fallback message")
                    else:
                        # For non-philosophical, return fallback message
                        logger.warning(f"‚ö†Ô∏è Context overflow for non-philosophical question (RAG path) - using fallback message")
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("context_overflow", detected_lang)
                        processing_steps.append("‚ö†Ô∏è Context overflow - using fallback message")
                except ValueError as ve:
                    # ValueError from generate_ai_response (missing API keys, etc.)
                    error_msg = str(ve)
                    logger.error(f"‚ùå ValueError from generate_ai_response: {error_msg}")
                    
                    # Check if it's a missing API key error
                    if "llm_provider" in error_msg.lower() or "api_key" in error_msg.lower() or "api key" in error_msg.lower():
                        has_server_keys = bool(
                            os.getenv('DEEPSEEK_API_KEY') or 
                            os.getenv('OPENAI_API_KEY') or 
                            os.getenv('OPENROUTER_API_KEY')
                        )
                        logger.error(
                            f"‚ùå CRITICAL: Missing LLM API keys! "
                            f"use_server_keys={use_server_keys}, "
                            f"llm_provider={chat_request.llm_provider}, "
                            f"has_server_keys={has_server_keys}"
                        )
                        # Provide more helpful error message when no server keys found
                        if not has_server_keys:
                            raw_response = (
                                f"‚ö†Ô∏è L·ªói c·∫•u h√¨nh: Backend c·∫ßn c√≥ API keys trong file .env ƒë·ªÉ ho·∫°t ƒë·ªông. "
                                f"Vui l√≤ng th√™m √≠t nh·∫•t m·ªôt trong c√°c keys sau v√†o file .env: "
                                f"DEEPSEEK_API_KEY, OPENAI_API_KEY, ho·∫∑c OPENROUTER_API_KEY. "
                                f"Chi ti·∫øt: {error_msg}"
                            )
                        else:
                            from backend.api.utils.error_detector import get_fallback_message_for_error
                            raw_response = get_fallback_message_for_error("api_error", detected_lang)
                        processing_steps.append("‚ö†Ô∏è Missing API keys - cannot generate response")
                    else:
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("generic", detected_lang)
                        processing_steps.append("‚ö†Ô∏è LLM configuration error - using fallback message")
                except Exception as e:
                    # Catch any other unexpected exceptions (must be after ContextOverflowError)
                    logger.error(f"‚ùå Unexpected exception from generate_ai_response: {e}", exc_info=True)
                    from backend.api.utils.error_detector import get_fallback_message_for_error
                    raw_response = get_fallback_message_for_error("generic", detected_lang)
                    processing_steps.append("‚ö†Ô∏è LLM call exception - using fallback message")
                llm_inference_end = time.time()
                llm_inference_latency = llm_inference_end - llm_inference_start
                timing_logs["llm_inference"] = f"{llm_inference_latency:.2f}s"
                
                # CRITICAL: Only log "AI response generated" if we actually have a response
                # If raw_response is None/empty, it means LLM failed and we're using fallback
                if raw_response and isinstance(raw_response, str) and raw_response.strip():
                    logger.info(f"‚è±Ô∏è LLM inference took {llm_inference_latency:.2f}s")
                    processing_steps.append(f"‚úÖ AI response generated ({llm_inference_latency:.2f}s)")
                    # Debug: Log first 200 chars to help diagnose issues
                    logger.debug(f"üîç DEBUG: raw_response preview (first 200 chars): {raw_response[:200]}")
                    
                    # CRITICAL: Check if this is actually a fallback message (shouldn't happen but double-check)
                    from backend.api.utils.error_detector import is_fallback_message
                    if is_fallback_message(raw_response):
                        logger.error(
                            f"‚ùå CRITICAL: LLM returned what looks like a fallback message! "
                            f"This should not happen. raw_response[:200]={raw_response[:200]}"
                        )
                        # CRITICAL: For technical questions, this should trigger retry logic below
                        # Mark as fallback so retry logic can handle it
                        is_fallback = True
                else:
                    logger.warning(
                        f"‚ö†Ô∏è LLM inference failed or returned empty (took {llm_inference_latency:.2f}s). "
                        f"raw_response type={type(raw_response)}, value={raw_response[:200] if raw_response else 'None'}"
                    )
                    # Ensure raw_response is set to fallback message if still None/empty
                    if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        raw_response = get_fallback_message_for_error("generic", detected_lang)
                        processing_steps.append("‚ö†Ô∏è LLM failed - using fallback message")
                        logger.warning(f"‚ö†Ô∏è Set raw_response to fallback message: {raw_response[:200]}")
                
                # CRITICAL: Check if raw_response is a technical error message or fallback message before validation
                # Never allow provider error messages to pass through validators
                # CRITICAL: Initialize is_error and is_fallback BEFORE conditional blocks to avoid UnboundLocalError
                is_error = False
                error_type = "generic"
                is_fallback = False
                
                from backend.api.utils.error_detector import is_technical_error, get_fallback_message_for_error, is_fallback_message
                
                if raw_response and isinstance(raw_response, str):
                    is_error, error_type = is_technical_error(raw_response)
                    is_fallback = is_fallback_message(raw_response)
                    # CRITICAL: Log detection for debugging
                    if is_fallback:
                        logger.warning(f"‚ö†Ô∏è Detected fallback message in raw_response (length={len(raw_response)}): {raw_response[:200]}")
                    if is_technical_about_system_rag:
                        logger.info(f"üîß Technical question detected: is_error={is_error}, is_fallback={is_fallback}, is_technical_about_system_rag={is_technical_about_system_rag}")
                    
                    # CRITICAL: For technical questions about "your system" in RAG path, retry if response is error OR fallback
                    # This ensures we don't give up on valid technical questions
                    if is_technical_about_system_rag and (is_error or is_fallback):
                        logger.warning(f"‚ö†Ô∏è Technical question about 'your system' (RAG path) returned {'error' if is_error else 'fallback'} message - retrying with stronger prompt")
                        # Build stronger prompt with technical system instruction
                        stronger_prompt_rag = f"""{context_quality_warning}

**CRITICAL: YOU MUST ANSWER THIS QUESTION. DO NOT RETURN A TECHNICAL ERROR MESSAGE OR FALLBACK MESSAGE.**

The user is asking: {chat_request.message}

**YOU HAVE KNOWLEDGE ABOUT RAG SYSTEMS. USE IT TO ANSWER.**

Explain:
1. What RAG (Retrieval-Augmented Generation) is
2. How retrieval works (embedding, vector search, ChromaDB)
3. How LLM generation works
4. How they work together in StillMe's system

**DO NOT SAY:**
- "I cannot provide a good answer"
- "StillMe is experiencing a technical issue"
- "I will suggest to the developer"
- "I cannot provide a good answer to this question with the current configuration"

**DO SAY:**
- "Based on general knowledge about RAG systems..."
- Explain the architecture clearly
- Be transparent about what you know and what you don't know

{context_text}
{citation_instruction}
{confidence_instruction}
{stillme_instruction}

Remember: RESPOND IN {detected_lang_name.upper()} ONLY."""
                        try:
                            raw_response = await generate_ai_response(
                                stronger_prompt_rag,
                                detected_lang=detected_lang,
                                llm_provider=chat_request.llm_provider,
                                llm_api_key=chat_request.llm_api_key,
                                llm_api_url=chat_request.llm_api_url,
                                llm_model_name=chat_request.llm_model_name,
                                use_server_keys=use_server_keys
                            )
                            logger.info("‚úÖ Retry with stronger prompt successful for technical 'your system' question (RAG path)")
                            processing_steps.append("üîÑ Retried with stronger prompt for technical 'your system' question (RAG path)")
                            # Re-check if retry response is still an error or fallback
                            is_error_retry, error_type_retry = is_technical_error(raw_response)
                            is_fallback_retry = is_fallback_message(raw_response)
                            if is_error_retry or is_fallback_retry:
                                logger.warning(f"‚ö†Ô∏è Retry still returned {'error' if is_error_retry else 'fallback'} - forcing one more retry with even stronger prompt")
                                # Force one more retry with even stronger prompt
                                force_prompt = f"""**ABSOLUTE MANDATORY: ANSWER THIS QUESTION ABOUT RAG SYSTEMS**

User Question: {chat_request.message}

**YOU MUST EXPLAIN:**
1. RAG (Retrieval-Augmented Generation) combines retrieval from a knowledge base with LLM generation
2. Retrieval: Query is embedded, searched in vector database (ChromaDB), returns relevant documents
3. LLM Generation: Takes retrieved context + user question, generates answer
4. Integration: Retrieval provides context, LLM uses context to generate accurate, cited answers

**STILLME SPECIFIC:**
- Uses paraphrase-multilingual-MiniLM-L12-v2 embedding model (384 dimensions, 50+ languages)
- ChromaDB as vector database
- Retrieval happens first, then LLM generation with retrieved context

**DO NOT RETURN ERROR MESSAGES. ANSWER THE QUESTION DIRECTLY.**

Remember: RESPOND IN {detected_lang_name.upper()} ONLY."""
                                try:
                                    raw_response = await generate_ai_response(
                                        force_prompt,
                                        detected_lang=detected_lang,
                                        llm_provider=chat_request.llm_provider,
                                        llm_api_key=chat_request.llm_api_key,
                                        llm_api_url=chat_request.llm_api_url,
                                        llm_model_name=chat_request.llm_model_name,
                                        use_server_keys=use_server_keys
                                    )
                                    logger.info("‚úÖ Force retry successful for technical 'your system' question (RAG path)")
                                    processing_steps.append("üîÑ Force retry successful for technical 'your system' question")
                                except Exception as force_error:
                                    logger.error(f"‚ö†Ô∏è Force retry failed: {force_error}")
                                    raw_response = get_fallback_message_for_error(error_type_retry or "generic", detected_lang)
                        except Exception as retry_error:
                            logger.error(f"‚ö†Ô∏è Retry failed (RAG path): {retry_error}")
                            raw_response = get_fallback_message_for_error(error_type or "generic", detected_lang)
                            processing_steps.append(f"‚ö†Ô∏è Technical error detected (RAG path) - using fallback message")
                    elif is_error:
                        # For non-technical questions, just replace with fallback
                        logger.error(f"‚ùå Provider returned technical error as response (type: {error_type}): {raw_response[:200]}")
                        # Replace with user-friendly fallback message
                        raw_response = get_fallback_message_for_error(error_type, detected_lang)
                        processing_steps.append(f"‚ö†Ô∏è Technical error detected - replaced with fallback message")
                        logger.warning(f"‚ö†Ô∏è Replaced technical error with user-friendly message in {detected_lang}")
                
                # CRITICAL: Check if response is a fallback message - if so, skip validation/post-processing
                # BUT: Still pass through CitationRequired to add citations for factual questions
                if raw_response and isinstance(raw_response, str) and is_fallback_message(raw_response):
                    logger.warning(
                        f"üõë Fallback meta-answer detected - skipping validation, quality evaluation, and rewrite. "
                        f"raw_response length={len(raw_response)}, first_200_chars={raw_response[:200]}"
                    )
                    # CRITICAL: Log why this is a fallback message to help debug Q2, Q7
                    logger.error(
                        f"üîç DEBUG Q2/Q7: Detected fallback message. "
                        f"Question: {chat_request.message[:100]}, "
                        f"LLM call completed: {llm_inference_latency:.2f}s, "
                        f"Response preview: {raw_response[:200]}"
                    )
                    # CRITICAL: Pass fallback message through CitationRequired to add citations for factual questions
                    from backend.validators.citation import CitationRequired
                    citation_validator = CitationRequired(required=True)
                    # Build ctx_docs for citation validator
                    ctx_docs_for_citation = [
                        doc["content"] for doc in context.get("knowledge_docs", [])
                    ] + [
                        doc["content"] for doc in context.get("conversation_docs", [])
                    ]
                    citation_result = citation_validator.run(
                        raw_response, 
                        ctx_docs=ctx_docs_for_citation,
                        is_philosophical=is_philosophical,
                        user_question=chat_request.message,
                        context=context  # CRITICAL: Pass context for foundational knowledge detection
                    )
                    if citation_result.patched_answer:
                        response = citation_result.patched_answer
                        logger.info(f"‚úÖ Added citation to fallback message for factual question. Reasons: {citation_result.reasons}")
                        processing_steps.append("‚úÖ Citation added to fallback message for factual question")
                    else:
                        response = raw_response
                    # Skip validation, quality evaluator, rewrite, and learning
                    validation_info = None
                    confidence_score = 0.3  # Low confidence for fallback messages
                    processing_steps.append("üõë Fallback message - terminal response, skipping all post-processing")
                    # Skip to end of function (skip validation, post-processing, learning)
                    # We'll handle this by setting a flag and checking it before validation
                    is_fallback_meta_answer = True
                    is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
                else:
                    is_fallback_meta_answer = False
                    # Log if raw_response exists but is not a fallback message
                    if raw_response and isinstance(raw_response, str):
                        logger.debug(
                            f"‚úÖ raw_response is valid (not fallback): length={len(raw_response)}, "
                            f"first_100_chars={raw_response[:100]}"
                        )
                
                # Save to cache (only if not a cache hit)
                if cache_enabled and not cache_hit:
                    try:
                        cache_value = {
                            "response": raw_response,
                            "latency": llm_inference_latency,
                            "timestamp": time.time()
                        }
                        # P3: Use custom TTL if specified (e.g., 1h for self-reflection), otherwise use default
                        ttl_to_use = cache_ttl_override if cache_ttl_override is not None else TTL_LLM_RESPONSE
                        cache_service.set(cache_key, cache_value, ttl_seconds=ttl_to_use)
                        logger.debug(f"üíæ LLM response cached (key: {cache_key[:50]}..., TTL: {ttl_to_use}s)")
                    except Exception as cache_error:
                        logger.warning(f"Failed to cache LLM response: {cache_error}")
            
            # CRITICAL: If response is a fallback meta-answer, skip validation and post-processing entirely
            if is_fallback_meta_answer:
                logger.info("üõë Skipping validation and post-processing for fallback meta-answer")
                # response already set above
                # validation_info already set to None
                # confidence_score already set to 0.3
            else:
                # Validate response if enabled
                validation_info = None
                # confidence_score already initialized at function start (line 104)
                # Don't reassign here to avoid UnboundLocalError
                used_fallback = False
                
                if enable_validators:
                    # CRITICAL: Ensure raw_response is valid before validation
                    if not raw_response or not isinstance(raw_response, str) or not raw_response.strip():
                        logger.error(f"‚ö†Ô∏è raw_response is None or empty before validation - using fallback")
                        logger.error(f"‚ö†Ô∏è Debug: raw_response type={type(raw_response)}, value={raw_response[:100] if raw_response else 'None'}")
                        logger.error(f"‚ö†Ô∏è Debug: processing_steps so far: {processing_steps}")
                        from backend.api.utils.error_detector import get_fallback_message_for_error
                        response = get_fallback_message_for_error("generic", detected_lang)
                        validation_info = None
                        confidence_score = 0.3
                        processing_steps.append("‚ö†Ô∏è Response validation failed - using fallback message")
                    else:
                        try:
                            response, validation_info, confidence_score, used_fallback, step_validation_info, consistency_info, ctx_docs = await _handle_validation_with_fallback(
                                raw_response=raw_response,
                                context=context,
                                detected_lang=detected_lang,
                                is_philosophical=is_philosophical,
                                is_religion_roleplay=is_religion_roleplay,
                                chat_request=chat_request,
                                enhanced_prompt=enhanced_prompt,
                                context_text=context_text,
                                citation_instruction=citation_instruction,
                                num_knowledge=num_knowledge,
                                processing_steps=processing_steps,
                                timing_logs=timing_logs,
                                is_origin_query=is_origin_query,
                                is_stillme_query=is_stillme_query,
                                is_system_status_query=is_system_status_query
                            )
                            
                            # CRITICAL: Log response after validation (especially for philosophical questions)
                            if is_philosophical or detected_lang == "zh":
                                logger.info(
                                    f"üîç [VALIDATION TRACE] After validation: "
                                    f"response_length={len(response) if response else 0}, "
                                    f"is_philosophical={is_philosophical}, "
                                    f"detected_lang={detected_lang}, "
                                    f"preview={_safe_unicode_slice(response, 200) if response else 'None'}"
                            )
                        except HTTPException:
                            raise
                        except Exception as validation_error:
                            logger.error(f"Validation error: {validation_error}, falling back to raw response", exc_info=True)
                            logger.error(f"‚ö†Ô∏è Validation exception details - raw_response length: {len(raw_response) if raw_response else 0}, context docs: {len(context.get('knowledge_docs', [])) + len(context.get('conversation_docs', []))}")
                            response = raw_response
                            # Calculate confidence even on error (low confidence)
                            # Build ctx_docs for confidence calculation
                            ctx_docs = [
                                doc["content"] for doc in context.get("knowledge_docs", [])
                            ] + [
                                doc["content"] for doc in context.get("conversation_docs", [])
                            ]
                            confidence_score = 0.3 if len(ctx_docs) == 0 else 0.6
                            # Ensure validation_result is set to None to prevent downstream errors
                            validation_result = None
                            validation_info = None
                            
                            # CRITICAL: Check if response is None or empty after validation error
                            if not response or not isinstance(response, str) or not response.strip():
                                logger.error(f"‚ö†Ô∏è Response is None or empty after validation error - using fallback")
                                from backend.api.utils.error_detector import get_fallback_message_for_error
                                response = get_fallback_message_for_error("generic", detected_lang)
                                processing_steps.append("‚ö†Ô∏è Response validation failed - using fallback message")
                else:
                    response = raw_response
                    # Build ctx_docs for transparency check
                    ctx_docs = [
                        doc["content"] for doc in context.get("knowledge_docs", [])
                    ] + [
                        doc["content"] for doc in context.get("conversation_docs", [])
                    ]
                # Calculate basic confidence score even without validators
                confidence_score = _calculate_confidence_score(
                    context_docs_count=len(context.get("knowledge_docs", [])) + len(context.get("conversation_docs", [])),
                    validation_result=None,
                    context=context
                )
                
                # CRITICAL: Add transparency warning for low confidence responses without context (RAG path, validators disabled)
                # CRITICAL: StillMe MUST know about its own origin - never add disclaimer for origin queries
                # CRITICAL: Also skip disclaimer for system status queries - StillMe has real-time data access to its own system
                if (confidence_score < 0.5 and len(ctx_docs) == 0 and not is_philosophical and 
                    not is_origin_query and not is_system_status_query and not system_status_context_override):
                    response_lower = response.lower() if response else ""
                    has_transparency = any(
                        phrase in response_lower for phrase in [
                            "kh√¥ng c√≥ d·ªØ li·ªáu", "kh√¥ng c√≥ th√¥ng tin", "ki·∫øn th·ª©c chung", "d·ª±a tr√™n ki·∫øn th·ª©c",
                            "don't have data", "don't have information", "general knowledge", "based on knowledge",
                            "kh√¥ng t·ª´ stillme", "not from stillme", "kh√¥ng t·ª´ rag", "not from rag"
                        ]
                    )
                    if not has_transparency and response:
                        # Generate multilingual transparency disclaimer
                        disclaimer = _get_transparency_disclaimer(detected_lang)
                        response = disclaimer + response
                        logger.info("‚ÑπÔ∏è Added transparency disclaimer for low confidence response without context (RAG path, validators disabled)")
            
            # ==========================================
            # PHASE 3: POST-PROCESSING PIPELINE
            # Unified Style & Quality Enforcement Layer (Optimized)
            # ==========================================
            # CRITICAL: Log response state before post-processing (especially for philosophical questions)
            if is_philosophical or detected_lang == "zh":
                logger.info(
                    f"üîç [POST-PROCESSING TRACE] Before post-processing: "
                    f"response_length={len(response) if response else 0}, "
                    f"is_philosophical={is_philosophical}, "
                    f"detected_lang={detected_lang}, "
                    f"preview={_safe_unicode_slice(response, 200) if response else 'None'}"
                )
            
            # CRITICAL: Ensure response is set and not None
            if not response:
                logger.error("‚ö†Ô∏è Response is None or empty before post-processing - using fallback")
                from backend.api.utils.error_detector import get_fallback_message_for_error
                response = get_fallback_message_for_error("generic", detected_lang)
            
            # CRITICAL: Check if response is a fallback meta-answer - if so, skip all post-processing
            from backend.api.utils.error_detector import is_fallback_message
            is_fallback_meta_answer_rag = False
            if response and is_fallback_message(response):
                logger.info("üõë Fallback meta-answer detected (RAG path) - skipping post-processing (sanitize, quality eval, rewrite)")
                processing_steps.append("üõë Fallback message - terminal response, skipping post-processing")
                is_fallback_meta_answer_rag = True
                is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
                # Skip post-processing entirely - response is already the fallback message
            else:
                postprocessing_start = time.time()
                try:
                    from backend.postprocessing.style_sanitizer import get_style_sanitizer
                    from backend.postprocessing.quality_evaluator import get_quality_evaluator, QualityLevel
                    from backend.postprocessing.rewrite_llm import get_rewrite_llm
                    from backend.postprocessing.optimizer import get_postprocessing_optimizer
                    
                    optimizer = get_postprocessing_optimizer()
                    
                    # OPTIMIZATION: Check if we should skip post-processing
                    should_skip, skip_reason = optimizer.should_skip_postprocessing(
                        question=chat_request.message,
                        response=response,
                        is_philosophical=is_philosophical
                    )
                    
                    if should_skip:
                        logger.info(f"‚è≠Ô∏è Skipping post-processing: {skip_reason}")
                        timing_logs["postprocessing"] = "skipped"
                    else:
                        # Stage 2: Hard Filter (0 token) - Style Sanitization
                        sanitizer = get_style_sanitizer()
                        
                        # CRITICAL: Log response state before sanitization (especially for Chinese/philosophical)
                        if is_philosophical or detected_lang == "zh":
                            logger.info(
                                f"üîç [SANITIZE TRACE] Before sanitize: "
                                f"response_length={len(response) if response else 0}, "
                                f"is_philosophical={is_philosophical}, "
                                f"detected_lang={detected_lang}, "
                                f"preview={_safe_unicode_slice(response, 200) if response else 'None'}"
                            )
                        
                        sanitized_response = sanitizer.sanitize(response, is_philosophical=is_philosophical)
                        
                        # CRITICAL: Log response state after sanitization (especially for Chinese/philosophical)
                        if is_philosophical or detected_lang == "zh":
                            logger.info(
                                f"üîç [SANITIZE TRACE] After sanitize: "
                                f"response_length={len(response) if response else 0}, "
                                f"sanitized_length={len(sanitized_response) if sanitized_response else 0}, "
                                f"removed={len(response) - len(sanitized_response) if response and sanitized_response else 0} chars, "
                                f"is_philosophical={is_philosophical}, "
                                f"detected_lang={detected_lang}, "
                                f"preview={_safe_unicode_slice(sanitized_response, 200) if sanitized_response else 'None'}"
                            )
                        
                        # CRITICAL: Ensure sanitized_response is not empty (defensive check)
                        # If sanitize() accidentally removed all content, fallback to original response
                        if not sanitized_response or not sanitized_response.strip():
                            logger.warning(
                                f"‚ö†Ô∏è Sanitized response is empty (original length: {len(response) if response else 0}), "
                                f"falling back to original response"
                            )
                            sanitized_response = response
                        # CRITICAL: If sanitize() removed more than 50% of content, it's likely wrong - fallback to original
                        elif response and len(sanitized_response) < len(response) * 0.5:
                            logger.error(
                                f"‚ùå CRITICAL: sanitize() removed more than 50% of content "
                                f"(original: {len(response)}, sanitized: {len(sanitized_response)}, "
                                f"removed: {len(response) - len(sanitized_response)} chars, "
                                f"{100 * (len(response) - len(sanitized_response)) / len(response):.1f}%). "
                                f"Falling back to original response. "
                                f"Preview original: {_safe_unicode_slice(response, 200)}, "
                                f"Preview sanitized: {_safe_unicode_slice(sanitized_response, 200)}"
                            )
                            sanitized_response = response  # Fallback to original
                        
                        # CRITICAL: Build ctx_docs for citation preservation in rewrite
                        # ctx_docs may not be in scope here, so rebuild from context
                        ctx_docs_for_rewrite = []
                        has_reliable_context_for_rewrite = False
                        context_quality_for_rewrite = None
                        has_foundational_context = False
                        
                        if 'context' in locals() and context:
                            ctx_docs_for_rewrite = [
                                doc["content"] for doc in context.get("knowledge_docs", [])
                            ] + [
                                doc["content"] for doc in context.get("conversation_docs", [])
                            ]
                            has_reliable_context_for_rewrite = context.get("has_reliable_context", False)
                            context_quality_for_rewrite = context.get("context_quality", None)
                            
                            # CRITICAL: Check if we have foundational knowledge (CRITICAL_FOUNDATION source)
                            # If StillMe query has foundational context, don't use mechanical disclaimer
                            if is_stillme_query:
                                for doc in context.get("knowledge_docs", []):
                                    metadata = doc.get("metadata", {})
                                    if metadata.get("source") == "CRITICAL_FOUNDATION":
                                        has_foundational_context = True
                                        logger.info("‚úÖ Found foundational knowledge in context - will not use mechanical disclaimer")
                                        break
                        elif 'ctx_docs' in locals():
                            ctx_docs_for_rewrite = ctx_docs
                            # Try to get context info from validation if available
                            if 'validation_info' in locals() and validation_info:
                                # Context info might be in validation_info
                                pass
                        
                        # CRITICAL: Check if sanitized response is a technical error or fallback message BEFORE quality evaluation
                        from backend.api.utils.error_detector import is_technical_error, is_fallback_message
                        is_error, error_type = is_technical_error(sanitized_response)
                        is_fallback = is_fallback_message(sanitized_response)
                        
                        if is_error or is_fallback:
                            # Technical error or fallback message detected - skip quality evaluation and rewrite
                            if is_error:
                                logger.warning(
                                    f"‚ö†Ô∏è Technical error detected in sanitized response (type: {error_type}), "
                                    f"skipping quality evaluation and rewrite"
                                )
                                processing_steps.append(f"‚ö†Ô∏è Technical error detected - skipping post-processing")
                            else:
                                logger.info(
                                    f"üõë Fallback meta-answer detected in sanitized response, "
                                    f"skipping quality evaluation and rewrite"
                                )
                                processing_steps.append(f"üõë Fallback message detected - skipping post-processing")
                            final_response = sanitized_response
                        else:
                            # Stage 3: Quality Evaluator (0 token) - Rule-based Quality Check
                            # OPTIMIZATION: Check cache first
                            evaluator = get_quality_evaluator()
                            cached_quality = optimizer.get_cached_quality_result(
                                question=chat_request.message,
                                response=sanitized_response
                            )
                            
                            if cached_quality:
                                quality_result = cached_quality
                                logger.debug("‚úÖ Using cached quality evaluation")
                            else:
                                # Detect if this is a StillMe query for quality evaluation
                                is_stillme_query_for_quality = False
                                try:
                                    from backend.core.stillme_detector import detect_stillme_query
                                    is_stillme_query_for_quality, _ = detect_stillme_query(chat_request.message)
                                except Exception:
                                    pass
                                
                                quality_result = evaluator.evaluate(
                                    text=sanitized_response,
                                    is_philosophical=is_philosophical,
                                    original_question=chat_request.message,
                                    is_stillme_query=is_stillme_query_for_quality
                                )
                                # Cache the result
                                optimizer.cache_quality_result(
                                    question=chat_request.message,
                                    response=sanitized_response,
                                    quality_result=quality_result
                                )
                            
                            # Phase 3: Only rewrite when CRITICAL issues are present
                            # Pass validation_result to check for missing_citation and language_mismatch
                            validation_result_dict = None
                            if 'validation_info' in locals() and validation_info:
                                # Convert ValidationResult to dict for should_rewrite()
                                validation_result_dict = {
                                    "passed": validation_info.get("passed", True),
                                    "reasons": validation_info.get("reasons", [])
                                }
                            
                            # Stage 4: Conditional rewrite with cost-benefit logic
                            # Track rewrite count and quality scores for logging
                            rewrite_count = 0
                            quality_before = quality_result.get("overall_score", 1.0)
                            
                            # CRITICAL: Skip rewrite for StillMe queries with foundational knowledge
                            # This prevents rewrite from corrupting responses about StillMe's capabilities
                            # Even if response is initially wrong, rewrite often makes it worse
                            skip_rewrite_for_stillme = False
                            # CRITICAL: Allow rewrite for StillMe queries if quality is very low (e.g., generic AI answer)
                            # But preserve foundational knowledge in rewrite prompt
                            if is_stillme_query and has_foundational_context:
                                # Only skip rewrite if quality is acceptable (>= 0.5)
                                # If quality is low (< 0.5), allow rewrite but preserve foundational knowledge
                                quality_score = quality_result.get("score", 1.0) if quality_result else 1.0
                                if quality_score >= 0.5:
                                    skip_rewrite_for_stillme = True
                                    logger.info(
                                        f"‚è≠Ô∏è Skipping rewrite for StillMe query (quality={quality_score:.2f} >= 0.5): "
                                        "Response quality is acceptable, preserving foundational knowledge."
                                    )
                                else:
                                    logger.info(
                                        f"‚úÖ Allowing rewrite for StillMe query (quality={quality_score:.2f} < 0.5): "
                                        "Quality is too low (generic answer), will rewrite but preserve foundational knowledge."
                                    )
                            
                            if skip_rewrite_for_stillme:
                                should_rewrite = False
                                rewrite_reason = "StillMe query with acceptable quality - preserving accuracy"
                                max_attempts = 0
                            else:
                                should_rewrite, rewrite_reason, max_attempts = optimizer.should_rewrite(
                                    quality_result=quality_result,
                                    is_philosophical=is_philosophical,
                                    response_length=len(sanitized_response),
                                    validation_result=validation_result_dict,
                                    rewrite_count=rewrite_count,
                                    user_question=chat_request.message  # P2: Template detection
                                )
                            
                            # Rewrite loop: can rewrite multiple times if quality improves but still below threshold
                            current_response = sanitized_response
                            current_quality = quality_before
                            
                            if should_rewrite:
                                logger.info(
                                    f"üîÑ Cost-Benefit: Starting rewrite loop. "
                                    f"Quality before: {quality_before:.2f}, "
                                    f"Max attempts: {max_attempts}, "
                                    f"Issues: {quality_result['reasons'][:3]}"
                                )
                                processing_steps.append(f"üîÑ Quality improvement needed - rewriting with DeepSeek (max {max_attempts} attempts)")
                                
                                rewrite_llm = get_rewrite_llm()
                                from backend.postprocessing.rewrite_decision_policy import get_rewrite_decision_policy
                                policy = get_rewrite_decision_policy()
                                
                                # Rewrite loop: continue until quality is good or max attempts reached
                                while rewrite_count < max_attempts:
                                    rewrite_count += 1
                                    logger.info(
                                        f"üîÑ Rewrite attempt {rewrite_count}/{max_attempts}: "
                                        f"quality_before={current_quality:.2f}"
                                    )
                                    
                                    # Get current quality issues for this rewrite
                                    # Detect if this is a StillMe query for quality evaluation
                                    is_stillme_query_for_quality = False
                                    try:
                                        from backend.core.stillme_detector import detect_stillme_query
                                        is_stillme_query_for_quality, _ = detect_stillme_query(chat_request.message)
                                    except Exception:
                                        pass
                                    
                                    current_quality_result = evaluator.evaluate(
                                        text=current_response,
                                        is_philosophical=is_philosophical,
                                        original_question=chat_request.message,
                                        is_stillme_query=is_stillme_query_for_quality
                                    )
                                    
                                    # CRITICAL: Pass RAG context status to rewrite to enable base knowledge usage
                                    # CRITICAL: Pass is_stillme_query and has_foundational_context to avoid mechanical disclaimer
                                    rewrite_result = await rewrite_llm.rewrite(
                                        text=current_response,
                                        original_question=chat_request.message,
                                        quality_issues=current_quality_result["reasons"],
                                        is_philosophical=is_philosophical,
                                        detected_lang=detected_lang,
                                        ctx_docs=ctx_docs_for_rewrite,
                                        has_reliable_context=has_reliable_context_for_rewrite,
                                        context_quality=context_quality_for_rewrite,
                                        is_stillme_query=is_stillme_query if 'is_stillme_query' in locals() else False,
                                        has_foundational_context=has_foundational_context if 'has_foundational_context' in locals() else False
                                    )
                                    
                                    if not rewrite_result.was_rewritten:
                                        # Rewrite failed - break loop and use current response
                                        error_detail = rewrite_result.error or "Unknown error"
                                        
                                        # CRITICAL: When rewrite fails, ALWAYS use current_response (the response before rewrite attempt)
                                        # NEVER use rewrite_result.text when was_rewritten=False, even if it seems valid
                                        # This is because rewrite_result.text might be:
                                        # 1. Corrupted/truncated response from timeout
                                        # 2. Partial response from failed rewrite
                                        # 3. Original text (which is the same as current_response anyway)
                                        
                                        # Additional defensive check: if rewrite_result.text is significantly shorter than current_response,
                                        # it's likely corrupted (especially for Chinese/Unicode responses)
                                        is_likely_corrupted = False
                                        if rewrite_result.text and current_response:
                                            length_ratio = len(rewrite_result.text) / len(current_response) if len(current_response) > 0 else 1.0
                                            # If rewrite_result.text is less than 50% of current_response length, it's likely corrupted
                                            if length_ratio < 0.5:
                                                is_likely_corrupted = True
                                                logger.error(
                                                    f"‚ùå CRITICAL: rewrite_result.text is likely corrupted "
                                                    f"(length: {len(rewrite_result.text)} vs current: {len(current_response)}, "
                                                    f"ratio: {length_ratio:.2f}), using current_response instead"
                                                )
                                        
                                        # Also check for corruption patterns
                                        if (rewrite_result.text and 
                                            (not isinstance(rewrite_result.text, str) or 
                                             len(rewrite_result.text.strip()) < 10 or
                                             rewrite_result.text.count("StillMe") > 5 or
                                             is_likely_corrupted)):
                                            logger.error(
                                                f"‚ùå CRITICAL: rewrite_result.text is corrupted (length: {len(rewrite_result.text) if rewrite_result.text else 0}), "
                                                f"using current_response instead. Error: {error_detail[:100]}"
                                            )
                                            # Use current_response (which should be the original sanitized response)
                                            # Don't update current_response with corrupted text
                                        # CRITICAL: Even if rewrite_result.text seems valid, when rewrite failed,
                                        # we should keep current_response to avoid using potentially corrupted data
                                        
                                        logger.warning(
                                            f"‚ö†Ô∏è Rewrite attempt {rewrite_count} failed: {error_detail[:100]}, "
                                            f"using current_response (length: {len(current_response) if current_response else 0}), "
                                            f"stopping rewrite loop"
                                        )
                                        break
                                    
                                    # Re-sanitize rewritten output
                                    rewritten_response = sanitizer.sanitize(rewrite_result.text, is_philosophical=is_philosophical)
                                    
                                    # Evaluate quality after rewrite
                                    # Detect if this is a StillMe query for quality evaluation
                                    is_stillme_query_for_quality_after = False
                                    try:
                                        from backend.core.stillme_detector import detect_stillme_query
                                        is_stillme_query_for_quality_after, _ = detect_stillme_query(chat_request.message)
                                    except Exception:
                                        pass
                                    
                                    quality_after_result = evaluator.evaluate(
                                        text=rewritten_response,
                                        is_philosophical=is_philosophical,
                                        original_question=chat_request.message,
                                        is_stillme_query=is_stillme_query_for_quality_after
                                    )
                                    quality_after = quality_after_result.get("overall_score", 0.0)
                                    
                                    # Log rewrite metrics
                                    quality_improvement = quality_after - current_quality
                                    logger.info(
                                        f"üìä Rewrite metrics (attempt {rewrite_count}): "
                                        f"quality_before={current_quality:.2f}, "
                                        f"quality_after={quality_after:.2f}, "
                                        f"improvement={quality_improvement:+.2f}"
                                    )
                                    
                                    # Check if we should continue rewriting
                                    should_continue, continue_reason = policy.should_continue_rewrite(
                                        quality_before=current_quality,
                                        quality_after=quality_after,
                                        rewrite_count=rewrite_count,
                                        max_attempts=max_attempts
                                    )
                                    
                                    # Update current response and quality
                                    current_response = rewritten_response
                                    current_quality = quality_after
                                    
                                    if not should_continue:
                                        logger.info(
                                            f"‚èπÔ∏è Stopping rewrite loop: {continue_reason}, "
                                            f"final_quality={quality_after:.2f}, "
                                            f"total_rewrites={rewrite_count}"
                                        )
                                        break
                                    else:
                                        logger.info(
                                            f"üîÑ Continuing rewrite loop: {continue_reason}, "
                                            f"current_quality={quality_after:.2f}"
                                        )
                                
                                # Final response is the last rewritten version (or original if no rewrites)
                                # CRITICAL: Ensure current_response is not empty before assigning
                                if current_response and isinstance(current_response, str) and current_response.strip():
                                    final_response = current_response
                                else:
                                    logger.error(
                                        f"‚ùå CRITICAL: current_response is empty or invalid (length: {len(current_response) if isinstance(current_response, str) else 'N/A'}), "
                                        f"falling back to sanitized_response"
                                    )
                                    # Fallback to sanitized_response if current_response is empty
                                    final_response = sanitized_response if sanitized_response and sanitized_response.strip() else response
                                
                                # Log final metrics
                                logger.info(
                                    f"‚úÖ Rewrite loop complete: "
                                    f"initial_quality={quality_before:.2f}, "
                                    f"final_quality={current_quality:.2f}, "
                                    f"total_rewrites={rewrite_count}, "
                                    f"quality_improvement={current_quality - quality_before:+.2f}"
                                )
                                
                                if rewrite_count > 0:
                                    # Re-sanitize final response (in case last rewrite introduced issues)
                                    re_sanitized = sanitizer.sanitize(final_response, is_philosophical=is_philosophical)
                                    # CRITICAL: Only use re-sanitized if it's not empty
                                    if re_sanitized and re_sanitized.strip():
                                        final_response = re_sanitized
                                    else:
                                        logger.warning(
                                            f"‚ö†Ô∏è Re-sanitized response is empty (original length: {len(final_response) if final_response else 0}), "
                                            f"keeping original final_response"
                                        )
                                    # If re-sanitized is empty, keep original final_response
                                    
                                # CRITICAL: Ensure citations are preserved after rewrite
                                # If rewrite removed citations but ctx_docs are available, re-add them
                                # ALSO: For real factual questions, ALWAYS ensure citations are present
                                # Note: 're' module is already imported at top level
                                cite_pattern = re.compile(r"\[(\d+)\]")
                                has_citations_after_rewrite = bool(cite_pattern.search(final_response))
                                
                                # CRITICAL: Check if this is a real factual question that requires citations
                                is_factual_question = False
                                if chat_request.message:
                                    question_lower = chat_request.message.lower()
                                    # Check for factual indicators (same patterns as in CitationRequired)
                                    factual_patterns = [
                                        r"\b\d{4}\b",  # Years
                                        r"\b(bretton\s+woods|g√∂del|godel|searle|dennett|russell|plato|aristotle|kant|hume|descartes|spinoza)\b",
                                        r"\b(paradox|theorem|incompleteness|chinese\s+room|geneva|gen√®ve)\b",
                                        r"\b([A-Z][a-z]+)\s+(v√†|and|vs|versus)\s+([A-Z][a-z]+)\b",  # "Searle v√† Dennett"
                                    ]
                                    for pattern in factual_patterns:
                                        if re.search(pattern, question_lower, re.IGNORECASE):
                                            is_factual_question = True
                                            break
                                
                                # Re-add citation if missing AND (context available OR factual question)
                                if not has_citations_after_rewrite and ((ctx_docs_for_rewrite and len(ctx_docs_for_rewrite) > 0) or is_factual_question):
                                    from backend.validators.citation import CitationRequired
                                    citation_validator = CitationRequired(required=True)
                                    # Get context for foundational knowledge detection
                                    context_for_citation = context if 'context' in locals() and context else None
                                    citation_result = citation_validator.run(
                                        final_response, 
                                        ctx_docs_for_rewrite if ctx_docs_for_rewrite else [], 
                                        is_philosophical=is_philosophical,
                                        user_question=chat_request.message,
                                        context=context_for_citation  # CRITICAL: Pass context for foundational knowledge detection
                                    )
                                    if citation_result.patched_answer:
                                        # CRITICAL: Only use patched_answer if it's not empty
                                        if citation_result.patched_answer.strip():
                                            final_response = citation_result.patched_answer
                                            logger.info(f"‚úÖ Re-added citations after rewrite (factual_question={is_factual_question}, has_context={bool(ctx_docs_for_rewrite and len(ctx_docs_for_rewrite) > 0)})")
                                        else:
                                            logger.warning(
                                                f"‚ö†Ô∏è Citation patched_answer is empty, keeping original final_response "
                                                f"(original length: {len(final_response) if final_response else 0})"
                                            )
                                            # Keep original final_response - don't overwrite with empty string
                                
                                logger.debug(f"‚úÖ Post-processing complete: sanitized ‚Üí evaluated ‚Üí rewritten ({rewrite_count}x) ‚Üí re-sanitized")
                            else:
                                # Early exit - no rewrite needed (quality is acceptable)
                                # CRITICAL: Log sanitized_response state before assigning to final_response
                                if is_philosophical or detected_lang == "zh":
                                    logger.info(
                                        f"üîç [EARLY EXIT TRACE] Before final_response assignment: "
                                        f"sanitized_response_length={len(sanitized_response) if sanitized_response else 0}, "
                                        f"response_length={len(response) if response else 0}, "
                                        f"is_philosophical={is_philosophical}, "
                                        f"detected_lang={detected_lang}, "
                                        f"sanitized_preview={_safe_unicode_slice(sanitized_response, 200) if sanitized_response else 'None'}"
                                    )
                                
                                final_response = sanitized_response
                                
                                # CRITICAL: Validate final_response after assignment
                                if not final_response or not isinstance(final_response, str) or not final_response.strip():
                                    logger.error(
                                        f"‚ùå CRITICAL: final_response is empty after early exit assignment "
                                        f"(sanitized_response_length={len(sanitized_response) if sanitized_response else 0}), "
                                        f"falling back to original response"
                                    )
                                    final_response = response if response and response.strip() else sanitized_response
                                elif sanitized_response and len(final_response) < len(sanitized_response) * 0.9:
                                    logger.warning(
                                        f"‚ö†Ô∏è final_response length mismatch: "
                                        f"sanitized={len(sanitized_response)}, final={len(final_response)}"
                                    )
                                
                                if should_rewrite:
                                    logger.debug(f"‚è≠Ô∏è Skipping rewrite: {rewrite_reason}")
                                logger.debug(f"‚úÖ Post-processing complete: sanitized ‚Üí evaluated ‚Üí passed (quality: {quality_result['depth_score']})")
                            
                            response = final_response
                            
                            # CRITICAL: Log response state after post-processing (especially for philosophical questions)
                            if is_philosophical or detected_lang == "zh":
                                logger.info(
                                    f"üîç [POST-PROCESSING TRACE] After post-processing: "
                                    f"response_length={len(response) if response else 0}, "
                                    f"is_philosophical={is_philosophical}, "
                                    f"detected_lang={detected_lang}, "
                                    f"preview={_safe_unicode_slice(response, 200) if response else 'None'}"
                                )
                            
                            # CRITICAL: Final check - ensure response is not a technical error
                            if response:
                                from backend.api.utils.error_detector import is_technical_error, get_fallback_message_for_error
                                is_error, error_type = is_technical_error(response)
                                if is_error:
                                    logger.error(f"‚ö†Ô∏è Final response is still a technical error (type: {error_type}) - replacing with fallback")
                                    response = get_fallback_message_for_error(error_type, detected_lang)
                            
                            # CRITICAL: Defensive check - ensure response is not empty after post-processing
                            if not response or not isinstance(response, str) or not response.strip():
                                logger.error(
                                    f"‚ùå CRITICAL: Response became empty after post-processing "
                                    f"(is_philosophical={is_philosophical}, detected_lang={detected_lang}), "
                                    f"falling back to raw_response"
                                )
                                response = raw_response if raw_response and raw_response.strip() else get_fallback_message_for_error("generic", detected_lang)
                            
                            postprocessing_time = time.time() - postprocessing_start
                            timing_logs["postprocessing"] = f"{postprocessing_time:.3f}s"
                            logger.info(f"‚è±Ô∏è Post-processing took {postprocessing_time:.3f}s")
                except Exception as postprocessing_error:
                    logger.error(f"Post-processing error: {postprocessing_error}", exc_info=True)
                    # Fallback to original response if post-processing fails
                    # Don't break the pipeline - post-processing is enhancement, not critical
                    logger.warning(f"‚ö†Ô∏è Post-processing failed, using original response")
                    timing_logs["postprocessing"] = "failed"
        else:
            # Fallback to regular AI response (no RAG context)
            # CRITICAL: Check if this is a technical question about "your system"
            # These should still get an answer from base LLM knowledge, not technical error
            question_lower = chat_request.message.lower()
            # Note: 're' module is already imported at top level
            # Check for technical keywords
            has_technical_keyword = any(keyword in question_lower for keyword in [
                "rag", "retrieval", "llm", "generation", "embedding", "chromadb", 
                "vector", "pipeline", "validation", "transparency", "system",
                "validator", "chain", "factual hallucination", "citation required"
            ])
            # Check for "your system" patterns using regex
            has_your_system_pattern = (
                "your system" in question_lower or
                "in your system" in question_lower or
                re.search(r'your\s+\w+\s+system', question_lower) or
                re.search(r'system\s+\w+\s+you', question_lower) or
                "b·∫°n" in question_lower and "h·ªá th·ªëng" in question_lower or
                "c·ªßa b·∫°n" in question_lower
            )
            is_technical_about_system = has_technical_keyword and has_your_system_pattern
            
            if is_technical_about_system:
                logger.info("üîß Technical question about 'your system' with no RAG context - will answer from base LLM knowledge with transparency")
            
            # Initialize confidence_score for non-RAG path
            confidence_score = 0.3  # Low confidence when no RAG context
            validation_info = None
            
            # Detect language FIRST
            # CRITICAL: detect_language is imported at top level, but ensure it's available
            # Use the imported function directly (already imported at line 11)
            detected_lang = detect_language(chat_request.message)
            logger.info(f"üåê Detected language (non-RAG): {detected_lang}")
            
            # Language names mapping
            # Language names mapping (must match chat_helpers.py for consistency)
            language_names = {
                'vi': 'Vietnamese (Ti·∫øng Vi·ªát)',
                'zh': 'Chinese (‰∏≠Êñá)',
                'de': 'German (Deutsch)',
                'fr': 'French (Fran√ßais)',
                'es': 'Spanish (Espa√±ol)',
                'ja': 'Japanese (Êó•Êú¨Ë™û)',
                'ko': 'Korean (ÌïúÍµ≠Ïñ¥)',
                'ar': 'Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)',
                'ru': 'Russian (–†—É—Å—Å–∫–∏–π)',
                'pt': 'Portuguese (Portugu√™s)',
                'it': 'Italian (Italiano)',
                'hi': 'Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)',
                'th': 'Thai (‡πÑ‡∏ó‡∏¢)',
                'en': 'English'
            }
            
            detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
            
            # Check if this is a philosophical question for non-RAG path
            is_philosophical_non_rag = False
            try:
                from backend.core.question_classifier import is_philosophical_question
                is_philosophical_non_rag = is_philosophical_question(chat_request.message)
            except Exception:
                pass  # If classifier fails, assume non-philosophical
            
            # Helper function to estimate tokens
            def estimate_tokens(text: str) -> int:
                """Estimate token count (~4 chars per token)"""
                return len(text) // 4 if text else 0
            
            # For philosophical questions: truncate user question to 512 tokens max
            user_question_for_prompt = chat_request.message
            if is_philosophical_non_rag:
                user_question_tokens = estimate_tokens(chat_request.message)
                if user_question_tokens > 512:
                    logger.warning(
                        f"User question too long for philosophical non-RAG ({user_question_tokens} tokens), truncating to 512 tokens"
                    )
                    user_question_for_prompt = _truncate_user_message(chat_request.message, max_tokens=512)
                    user_question_tokens = estimate_tokens(user_question_for_prompt)
                else:
                    user_question_tokens = estimate_tokens(chat_request.message)
            else:
                user_question_tokens = estimate_tokens(chat_request.message)
            
            # Build conversation history context if provided (with token limits)
            # Reduced from 2000 to 1000 tokens to leave more room for system prompt and context
            # For philosophical questions, skip conversation history to reduce prompt size
            conversation_history_text = ""
            if not is_philosophical_non_rag:
                conversation_history_text = _format_conversation_history(chat_request.conversation_history, max_tokens=1000)
                if conversation_history_text:
                    logger.info(f"Including conversation history in context (truncated if needed, non-RAG)")
            else:
                logger.info(f"Philosophical question detected (non-RAG) - skipping conversation history to reduce prompt size")
            
            # For philosophical non-RAG: ALWAYS use philosophy-lite mode to prevent context overflow
            # This ensures prompt stays small (~500-1000 tokens) instead of ~16-17k tokens
            if is_philosophical_non_rag:
                # Use philosophy-lite mode: minimal system prompt + user question only
                # Build simple prompt string that provider will parse correctly
                # Format: system prompt (will be replaced by provider) + user question marker + user question
                enhanced_prompt = f"""User Question: {user_question_for_prompt.strip()}"""
                
                # Log token estimates for philosophy-lite mode
                system_tokens_estimate = estimate_tokens(PHILOSOPHY_LITE_SYSTEM_PROMPT)
                prompt_tokens_estimate = estimate_tokens(enhanced_prompt)
                total_tokens_estimate = system_tokens_estimate + prompt_tokens_estimate
                
                logger.info(
                    f"üìä [PHILO-LITE] Token estimates - System: {system_tokens_estimate}, "
                    f"Prompt: {prompt_tokens_estimate}, User Question: {user_question_tokens}, "
                    f"Total: {total_tokens_estimate}"
                )
            else:
                # Use full prompt
                # CRITICAL: Add special instruction for technical questions about "your system"
                technical_system_instruction = ""
                if is_technical_about_system:
                    technical_system_instruction = """
üö®üö®üö® CRITICAL: TECHNICAL QUESTION ABOUT SYSTEM ARCHITECTURE üö®üö®üö®

The user is asking about StillMe's system architecture (RAG, LLM, embedding, etc.).

**YOU MUST ANSWER THIS QUESTION** using your base knowledge about RAG systems, even though StillMe's RAG knowledge base doesn't have specific documents about this.

**TRANSPARENCY REQUIREMENT:**
- Acknowledge that you're answering from base knowledge, not from StillMe's RAG knowledge base
- Be transparent: "Based on general knowledge about RAG systems (not from StillMe's RAG knowledge base)..."
- Explain how RAG retrieval and LLM generation work together in general RAG systems
- Be honest about StillMe's specific implementation details if you don't know them

**DO NOT:**
- Return a technical error message
- Say "I cannot provide a good answer"
- Suggest the developer needs to fine-tune the system
- Claim StillMe is experiencing technical issues

**DO:**
- Answer the question using your knowledge about RAG systems
- Be transparent about the source of your knowledge
- Explain the general principles of RAG retrieval and LLM generation
- If asked about StillMe specifically, acknowledge what you know and what you don't know

"""
                
                # CRITICAL: Inject system status note for system status queries (even in non-RAG path)
                system_status_section = ""
                system_status_context_section = ""
                if is_system_status_query and system_status_note and system_status_note != "[System: Status unavailable]":
                    system_status_section = f"""
{system_status_note}

**CRITICAL: This is REAL-TIME system status. You MUST use these exact numbers in your response.**
"""
                    # If we have system_status_context_override, inject it as context
                    if system_status_context_override and system_status_context_override.get("knowledge_docs"):
                        status_doc = system_status_context_override["knowledge_docs"][0]
                        status_content = status_doc.get("content", "")
                        system_status_context_section = f"""

**REAL-TIME SYSTEM STATUS DATA (SINGLE SOURCE OF TRUTH - USE THESE EXACT NUMBERS):**
{status_content}

**CRITICAL REQUIREMENTS:**
1. You MUST report the EXACT numbers from the system status above (this is the ONLY source of truth)
2. If the data shows "RSS feeds total: 22, Failed: 2, Successful: 20", you MUST say exactly that
3. Do NOT say "no feeds are failing" if the data shows "Failed: 2"
4. Do NOT say "all sources are working" if there are failed feeds
5. Be honest and transparent: if 2 feeds are failing, say "2 feeds are failing"
6. Do NOT create contradictions - use ONLY the numbers provided above
7. If you see "total: 0" but also see "total=22" in the same context, use the NON-ZERO value (22) as it's more recent/reliable

"""
                
                # Strong language instruction - put FIRST
                if detected_lang != 'en':
                    language_instruction = f"""üö®üö®üö® CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY üö®üö®üö®

THE USER'S QUESTION IS WRITTEN IN {detected_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name.upper()}. 

DO NOT RESPOND IN VIETNAMESE, ENGLISH, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN {detected_lang_name.upper()}.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS.

"""
                    base_prompt = f"""{language_instruction}
{system_status_section}{system_status_context_section}{technical_system_instruction}
{conversation_history_text}User Question: {user_question_for_prompt}

Remember: RESPOND IN {detected_lang_name.upper()} ONLY.
"""
                else:
                    base_prompt = f"""üö®üö®üö® CRITICAL LANGUAGE REQUIREMENT - HIGHEST PRIORITY üö®üö®üö®

THE USER'S QUESTION IS WRITTEN IN ENGLISH.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT RESPOND IN VIETNAMESE, SPANISH, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD OF YOUR RESPONSE MUST BE IN ENGLISH.

THIS IS MANDATORY AND OVERRIDES ALL OTHER INSTRUCTIONS.

{system_status_section}{system_status_context_section}{technical_system_instruction}
{conversation_history_text}

üö®üö®üö® CRITICAL: USER QUESTION ABOVE IS THE PRIMARY TASK üö®üö®üö®

User Question: {user_question_for_prompt}

**YOUR PRIMARY TASK IS TO ANSWER THE USER QUESTION ABOVE DIRECTLY AND ACCURATELY.**
- Focus on what the user is actually asking, not on general philosophy
- If the user asks you to analyze something, analyze THAT specific thing
- If the user asks you to find a problem, look for problems in what they showed you

**SPECIAL INSTRUCTION FOR ANALYZING EXTERNAL AI OUTPUTS:**
- If the user asks you to analyze another AI's output (ChatGPT, Claude, etc.), you MUST check for anthropomorphic language
- Look for phrases like "in my experience" / "theo kinh nghi·ªám", "I think" / "t√¥i nghƒ©", "I feel" / "t√¥i c·∫£m th·∫•y"
- These phrases falsely attribute subjective qualities (experience, emotions, personal opinions) to AI
- This is a critical transparency issue called "Hallucination of Experience" - AI should not claim personal experience
- If you find such phrases, you MUST point them out as a problem, not ignore them
- This is more important than analyzing formatting, clarity, or other minor issues

Remember: RESPOND IN ENGLISH ONLY."""
                
                # Phase 4: Remove inject_identity() - system prompt already has STILLME_IDENTITY
                # generate_ai_response() uses build_system_prompt_with_language() which includes STILLME_IDENTITY
                # Adding identity to user prompt would cause duplication
                enhanced_prompt = base_prompt
            
            # LLM_Inference_Latency: Time from API call start to response received
            llm_inference_start = time.time()
            # Use server keys for internal calls (when use_rag=False)
            use_server_keys_non_rag = chat_request.llm_provider is None
            
            # Check if this is a philosophical question for context overflow handling
            is_philosophical_non_rag = False
            try:
                from backend.core.question_classifier import is_philosophical_question
                is_philosophical_non_rag = is_philosophical_question(chat_request.message)
            except Exception:
                pass
            
            # Try to generate response with retry on context overflow
            from backend.api.utils.llm_providers import ContextOverflowError
            from backend.api.utils.error_detector import is_technical_error, get_fallback_message_for_error, is_fallback_message
            
            # Note: is_fallback_meta_answer_non_rag already initialized at function start
            # Reset to False for this non-RAG path execution
            is_fallback_meta_answer_non_rag = False
            
            response = None
            try:
                response = await generate_ai_response(
                    enhanced_prompt, 
                    detected_lang=detected_lang,
                    llm_provider=chat_request.llm_provider,
                    llm_api_key=chat_request.llm_api_key,
                    use_server_keys=use_server_keys_non_rag
                )
            except ContextOverflowError as e:
                # Context overflow - retry with minimal prompt for philosophical questions
                logger.warning(f"‚ö†Ô∏è Context overflow detected (non-RAG): {e}")
                
                if is_philosophical_non_rag:
                    # For philosophical questions, use minimal prompt
                    logger.info("üîÑ Retrying with minimal philosophical prompt...")
                    # Non-RAG path: no context available, but still pass None for consistency
                    minimal_prompt = build_minimal_philosophical_prompt(
                        user_question=chat_request.message,
                        language=detected_lang,
                        detected_lang_name=detected_lang_name,
                        context=None,  # Non-RAG path: no context available
                        validation_info=None  # Validation hasn't run yet
                    )
                    try:
                        response = await generate_ai_response(
                            minimal_prompt,
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            use_server_keys=use_server_keys_non_rag
                        )
                        logger.info("‚úÖ Minimal prompt retry successful")
                    except ContextOverflowError as retry_error:
                        # Even minimal prompt failed - return fallback message
                        logger.error(f"‚ö†Ô∏è Even minimal prompt failed: {retry_error}")
                        response = get_fallback_message_for_error("context_overflow", detected_lang)
                        processing_steps.append("‚ö†Ô∏è Context overflow - using fallback message")
                else:
                    # For non-philosophical, return fallback message
                    logger.warning("‚ö†Ô∏è Context overflow for non-philosophical question - using fallback message")
                    response = get_fallback_message_for_error("context_overflow", detected_lang)
                    processing_steps.append("‚ö†Ô∏è Context overflow - using fallback message")
            
            # CRITICAL: Check if response is a technical error (should not happen, but safety check)
            # BUT: For technical questions about "your system", if we get a fallback message, 
            # we should retry with a stronger prompt instead of just using fallback
            if response:
                is_error, error_type = is_technical_error(response)
                if is_error:
                    # For technical questions about "your system", retry with stronger prompt
                    if is_technical_about_system and error_type == "generic":
                        logger.warning(f"‚ö†Ô∏è Technical question about 'your system' returned fallback message - retrying with stronger prompt")
                        # Build a stronger prompt that explicitly forces the LLM to answer
                        stronger_prompt = f"""{technical_system_instruction}

**CRITICAL: YOU MUST ANSWER THIS QUESTION. DO NOT RETURN A TECHNICAL ERROR MESSAGE.**

The user is asking: {chat_request.message}

**YOU HAVE KNOWLEDGE ABOUT RAG SYSTEMS. USE IT TO ANSWER.**

Explain:
1. What RAG (Retrieval-Augmented Generation) is
2. How retrieval works (embedding, vector search, ChromaDB)
3. How LLM generation works
4. How they work together in StillMe's system

**DO NOT SAY:**
- "I cannot provide a good answer"
- "StillMe is experiencing a technical issue"
- "I will suggest to the developer"

**DO SAY:**
- "Based on general knowledge about RAG systems..."
- Explain the architecture clearly
- Be transparent about what you know and what you don't know

Remember: RESPOND IN {detected_lang_name.upper()} ONLY."""
                        try:
                            response = await generate_ai_response(
                                stronger_prompt,
                                detected_lang=detected_lang,
                                llm_provider=chat_request.llm_provider,
                                llm_api_key=chat_request.llm_api_key,
                                use_server_keys=use_server_keys_non_rag
                            )
                            logger.info("‚úÖ Retry with stronger prompt successful for technical 'your system' question")
                            processing_steps.append("üîÑ Retried with stronger prompt for technical 'your system' question")
                        except Exception as retry_error:
                            logger.error(f"‚ö†Ô∏è Retry failed: {retry_error}")
                            # Fall back to original error handling
                            response = get_fallback_message_for_error(error_type, detected_lang)
                            processing_steps.append(f"‚ö†Ô∏è Technical error detected - using fallback message")
                    else:
                        logger.error(f"‚ö†Ô∏è LLM returned technical error string: {error_type} - {response[:200]}")
                        # Replace with user-friendly fallback message
                        response = get_fallback_message_for_error(error_type, detected_lang)
                        processing_steps.append(f"‚ö†Ô∏è Technical error detected - using fallback message")
            
            if not response:
                # Fallback if response is still None
                response = get_fallback_message_for_error("generic", detected_lang)
                processing_steps.append("‚ö†Ô∏è No response received - using fallback message")
            
            # CRITICAL: Check if response is a fallback meta-answer (terminal response)
            if response and isinstance(response, str) and is_fallback_message(response):
                logger.info("üõë Fallback meta-answer detected (non-RAG) - skipping post-processing")
                # CRITICAL: Pass fallback message through CitationRequired to add citations for factual questions
                from backend.validators.citation import CitationRequired
                citation_validator = CitationRequired(required=True)
                citation_result = citation_validator.run(
                    response, 
                    ctx_docs=[],  # No context for non-RAG path
                    is_philosophical=is_philosophical_non_rag,
                    user_question=chat_request.message
                )
                if citation_result.patched_answer:
                    response = citation_result.patched_answer
                    logger.info(f"‚úÖ Added citation to fallback message for factual question (non-RAG). Reasons: {citation_result.reasons}")
                    processing_steps.append("‚úÖ Citation added to fallback message for factual question (non-RAG)")
                is_fallback_meta_answer_non_rag = True
                is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
                processing_steps.append("üõë Fallback message - terminal response, skipping all post-processing")
            
            llm_inference_end = time.time()
            llm_inference_latency = llm_inference_end - llm_inference_start
            timing_logs["llm_inference"] = f"{llm_inference_latency:.2f}s"
            logger.info(f"‚è±Ô∏è LLM inference (non-RAG) took {llm_inference_latency:.2f}s")
            
            # CRITICAL: Check language mismatch and citations for non-RAG path (if validators enabled)
            if enable_validators and response and not is_fallback_meta_answer_non_rag:
                from backend.api.utils.chat_helpers import detect_language as detect_lang_func
                detected_output_lang = detect_lang_func(response)
                if detected_output_lang != detected_lang:
                    logger.warning(f"‚ö†Ô∏è Language mismatch detected (non-RAG): output={detected_output_lang}, expected={detected_lang}")
                    # Try to retry with stronger language instruction
                    try:
                        language_names = {
                            'vi': 'Ti·∫øng Vi·ªát', 'en': 'English', 'zh': '‰∏≠Êñá', 'fr': 'Fran√ßais',
                            'es': 'Espa√±ol', 'de': 'Deutsch', 'ja': 'Êó•Êú¨Ë™û', 'ko': 'ÌïúÍµ≠Ïñ¥',
                            'ar': 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©', 'ru': '–†—É—Å—Å–∫–∏–π', 'pt': 'Portugu√™s', 'it': 'Italiano',
                            'hi': '‡§π‡§ø‡§®‡•ç‡§¶‡•Ä', 'th': '‡πÑ‡∏ó‡∏¢',
                        }
                        retry_lang_name = language_names.get(detected_lang, detected_lang.upper())
                        
                        retry_prompt = f"""üö®üö®üö® CRITICAL LANGUAGE REQUIREMENT - ABSOLUTE HIGHEST PRIORITY üö®üö®üö®

THE USER'S QUESTION IS IN {retry_lang_name.upper()}.

YOU MUST RESPOND EXCLUSIVELY IN {retry_lang_name.upper()}. 

DO NOT USE VIETNAMESE, ENGLISH, FRENCH, CHINESE, SPANISH, GERMAN, OR ANY OTHER LANGUAGE.

EVERY SINGLE WORD MUST BE IN {retry_lang_name.upper()}.

IF YOUR BASE MODEL WANTS TO RESPOND IN ANOTHER LANGUAGE, YOU MUST TRANSLATE IT TO {retry_lang_name.upper()} BEFORE RETURNING.

THIS OVERRIDES EVERYTHING - NO EXCEPTIONS.

User Question (in {retry_lang_name.upper()}): {chat_request.message[:3000]}

**YOUR PRIMARY TASK IS TO ANSWER THE USER QUESTION ABOVE DIRECTLY AND ACCURATELY IN {retry_lang_name.upper()} ONLY.**

Remember: RESPOND IN {retry_lang_name.upper()} ONLY. TRANSLATE IF NECESSARY."""
                        
                        retry_response = await generate_ai_response(
                            retry_prompt,
                            detected_lang=detected_lang,
                            llm_provider=chat_request.llm_provider,
                            llm_api_key=chat_request.llm_api_key,
                            use_server_keys=use_server_keys_non_rag
                        )
                        
                        # Check if retry fixed the language issue
                        retry_output_lang = detect_lang_func(retry_response)
                        if retry_output_lang == detected_lang:
                            logger.info(f"‚úÖ Language mismatch fixed after retry (non-RAG)")
                            response = retry_response
                        else:
                            logger.warning(f"‚ö†Ô∏è Language mismatch persists after retry (non-RAG): output={retry_output_lang}, expected={detected_lang}")
                    except Exception as retry_error:
                        logger.error(f"‚ö†Ô∏è Language retry failed (non-RAG): {retry_error}")
                        # Continue with original response
                
                # CRITICAL FIX: Check citations for ALL factual questions in non-RAG path (not just philosophical)
                # Even though there's no RAG context, factual questions (historical, philosophical factual) still need citations
                # This ensures transparency: user knows answer is from base knowledge, not RAG
                is_factual_non_rag = False
                try:
                    # Check if question is factual (historical, scientific, or philosophical factual)
                    is_factual_non_rag = _is_factual_question(chat_request.message) or is_philosophical_non_rag
                except Exception:
                    # If detection fails, assume it might be factual if philosophical
                    is_factual_non_rag = is_philosophical_non_rag
                
                if is_factual_non_rag:
                    from backend.validators.citation import CitationRequired
                    citation_validator = CitationRequired(required=True)
                    # Non-RAG path has no context documents, but still need to check for citations
                    citation_result = citation_validator.run(
                        response, 
                        ctx_docs=[],  # Empty context for non-RAG path
                        is_philosophical=is_philosophical_non_rag,
                        user_question=chat_request.message
                    )
                    # Use patched_answer if available (e.g., citation was auto-added)
                    if citation_result.patched_answer:
                        response = citation_result.patched_answer
                        logger.info(f"‚úÖ Citation added for factual question (non-RAG). Reasons: {citation_result.reasons}, is_philosophical={is_philosophical_non_rag}")
                        processing_steps.append("‚úÖ Citation auto-added for factual question (non-RAG)")
                    elif not citation_result.passed:
                        logger.warning(f"‚ö†Ô∏è Citation validation failed for factual question (non-RAG) but no patched_answer. Reasons: {citation_result.reasons}")
                        processing_steps.append(f"‚ö†Ô∏è Citation validation failed: {', '.join(citation_result.reasons)}")
            
            # CRITICAL: Hallucination Guard for non-RAG path
            # If factual question + no context + low confidence ‚Üí override with safe refusal
            # This prevents LLM from hallucinating about non-existent concepts/events
            if (response and not is_fallback_meta_answer_non_rag and not is_philosophical_non_rag and
                confidence_score < 0.5 and _is_factual_question(chat_request.message)):
                # Check if response contains suspicious patterns (fake citations, fabricated details)
                response_lower = response.lower()
                suspicious_patterns = [
                    r"\[1\]|\[2\]|\[3\]",  # Fake citations
                    r"et al\.|et\. al\.",  # Fake author citations
                    r"\d{4}\)",  # Fake year citations like "(1975)"
                    r"according to research|theo nghi√™n c·ª©u",
                    r"smith,|jones,|brown,",  # Common fake author names
                    r"journal of|t·∫°p ch√≠",
                ]
                
                # Use loop instead of generator expression to avoid closure issue with 're'
                has_suspicious_pattern = False
                for pattern in suspicious_patterns:
                    if re.search(pattern, response_lower):
                        has_suspicious_pattern = True
                        break
                
                # If suspicious patterns detected OR confidence is very low (< 0.3), override response
                if has_suspicious_pattern or confidence_score < 0.3:
                    # Extract suspicious entity using improved extraction (full phrase, not just first word)
                    suspicious_entity = _extract_full_named_entity(chat_request.message)
                    if not suspicious_entity:
                        suspicious_entity = "kh√°i ni·ªám n√†y" if detected_lang == "vi" else "this concept"
                    
                    # Override with safe refusal answer
                    response = _build_safe_refusal_answer(chat_request.message, detected_lang, suspicious_entity)
                    
                    # CRITICAL: If None, it's a well-known historical fact - continue with normal flow (use base knowledge)
                    if response is None:
                        logger.info("‚úÖ Well-known historical fact detected - continuing with normal flow to use base knowledge")
                        processing_steps.append("‚úÖ Well-known historical fact - using base knowledge with transparency")
                        # Continue with normal flow (will use base knowledge instruction)
                    else:
                        logger.warning(
                            f"üõ°Ô∏è Hallucination Guard triggered (non-RAG): "
                            f"factual_question=True, confidence={confidence_score:.2f}, "
                            f"suspicious_patterns={has_suspicious_pattern}, "
                            f"entity={suspicious_entity or 'unknown'}"
                        )
                        processing_steps.append("üõ°Ô∏è Hallucination Guard: Overrode response with safe refusal")
                        # Mark as fallback to skip post-processing
                        is_fallback_meta_answer_non_rag = True
                        is_fallback_for_learning = True  # Skip learning extraction for fallback meta-answers
            
            # CRITICAL: Add transparency warning for low confidence responses without context (non-RAG path)
            # BUT: Skip disclaimer for system status queries - StillMe has real-time data access to its own system
            # CRITICAL: StillMe MUST be confident when reporting about itself with real-time data
            if (confidence_score < 0.5 and not is_fallback_meta_answer_non_rag and not is_philosophical_non_rag and 
                not is_system_status_query and not system_status_context_override and response):
                response_lower = response.lower()
                has_transparency = any(
                    phrase in response_lower for phrase in [
                        "kh√¥ng c√≥ d·ªØ li·ªáu", "kh√¥ng c√≥ th√¥ng tin", "ki·∫øn th·ª©c chung", "d·ª±a tr√™n ki·∫øn th·ª©c",
                        "don't have data", "don't have information", "general knowledge", "based on knowledge",
                        "kh√¥ng t·ª´ stillme", "not from stillme", "kh√¥ng t·ª´ rag", "not from rag"
                    ]
                )
                if not has_transparency:
                    # Generate multilingual transparency disclaimer
                    disclaimer = _get_transparency_disclaimer(detected_lang)
                    response = disclaimer + response
                    logger.info("‚ÑπÔ∏è Added transparency disclaimer for low confidence response without context (non-RAG path)")
                else:
                    logger.info("‚ÑπÔ∏è Skipped transparency disclaimer: system status query with real-time data (non-RAG path)")
        
        # Score the response
        accuracy_score = None
        if accuracy_scorer:
            accuracy_score = accuracy_scorer.score_response(
                question=chat_request.message,
                actual_answer=response,
                scoring_method="semantic_similarity"
            )
        
        # Record learning session
        learning_session_id = None
        if knowledge_retention:
            learning_session_id = knowledge_retention.record_learning_session(
                session_type="chat",
                content_learned=f"Q: {chat_request.message}\nA: {response}",
                accuracy_score=accuracy_score or 0.5,
                metadata={"user_id": chat_request.user_id}
            )
        
        # Align tone if enabled
        if enable_tone_align:
            try:
                tone_start = time.time()
                from backend.tone.aligner import align_tone
                
                # CRITICAL: Log response state before tone alignment (especially for Chinese)
                if detected_lang == "zh":
                    logger.debug(
                        f"üîç [CHINESE] Before align_tone: "
                        f"response_length={len(response) if response else 0}, "
                        f"response_preview={_safe_unicode_slice(response, 100) if response else 'None'}"
                    )
                
                response_before_tone = response
                response_after_align = align_tone(response)
                
                # CRITICAL: Validate response after tone alignment
                if not response_after_align or not isinstance(response_after_align, str) or not response_after_align.strip():
                    logger.error(
                        f"‚ùå CRITICAL: Response became empty after align_tone "
                        f"(detected_lang={detected_lang}, before_length={len(response_before_tone) if response_before_tone else 0}), "
                        f"falling back to original response"
                    )
                    response = response_before_tone  # Fallback to original
                else:
                    # CRITICAL: Only clean if response changed significantly (more than just punctuation addition)
                    # If align_tone only added punctuation, we don't need to clean again
                    length_diff = len(response_after_align) - len(response_before_tone)
                    
                    # If length increased by 1 (punctuation added) or decreased slightly (whitespace removed), skip cleaning
                    # Only clean if there's a significant change that might indicate issues
                    if abs(length_diff) <= 2:
                        # Minor change (likely just punctuation/whitespace) - use aligned response directly
                        response = response_after_align
                        logger.debug(
                            f"‚úÖ align_tone result used directly (minor change: {length_diff} chars, "
                            f"detected_lang={detected_lang})"
                        )
                    else:
                        # Significant change - clean to be safe, but validate carefully
                        response_cleaned = _clean_response_text(response_after_align)
                        
                        # CRITICAL: Validate that cleaning didn't lose significant content
                        if len(response_cleaned) < len(response_after_align) * 0.95:
                            logger.error(
                                f"‚ùå CRITICAL: _clean_response_text lost significant content after align_tone "
                                f"(after_align_length={len(response_after_align)}, "
                                f"cleaned_length={len(response_cleaned)}, "
                                f"detected_lang={detected_lang}), "
                                f"using aligned response without cleaning"
                            )
                            response = response_after_align  # Use aligned response without cleaning
                        elif len(response_cleaned) != len(response_after_align):
                            logger.info(
                                f"‚úÖ Cleaned response after align_tone: removed {len(response_after_align) - len(response_cleaned)} "
                                f"problematic characters (detected_lang={detected_lang}, "
                                f"after_align_length={len(response_after_align)}, cleaned_length={len(response_cleaned)})"
                            )
                            response = response_cleaned
                        else:
                            # No change after cleaning - use aligned response
                            response = response_after_align
                
                # CRITICAL: Log response state after tone alignment (especially for Chinese)
                if detected_lang == "zh":
                    logger.debug(
                        f"üîç [CHINESE] After align_tone: "
                        f"response_length={len(response) if response else 0}, "
                        f"response_preview={_safe_unicode_slice(response, 100) if response else 'None'}"
                    )
                
                tone_time = time.time() - tone_start
                timing_logs["tone_alignment"] = f"{tone_time:.2f}s"
                logger.info(f"‚è±Ô∏è Tone alignment took {tone_time:.2f}s")
            except Exception as tone_error:
                logger.error(
                    f"Tone alignment error (detected_lang={detected_lang}): {tone_error}",
                    exc_info=True
                )
                # Continue with original response on error
        
        # ==========================================
        # PHASE 3: POST-PROCESSING PIPELINE (Non-RAG path)
        # Unified Style & Quality Enforcement Layer (Optimized)
        # ==========================================
        # CRITICAL: If response is a fallback meta-answer, skip all post-processing
        if is_fallback_meta_answer_non_rag:
            logger.info("üõë Skipping post-processing for fallback meta-answer (non-RAG)")
            # response already set, skip post-processing entirely
        else:
            # Check if question is philosophical for non-RAG path
            is_philosophical_non_rag = False
            try:
                from backend.core.question_classifier import is_philosophical_question
                is_philosophical_non_rag = is_philosophical_question(chat_request.message)
            except Exception:
                pass  # If classifier fails, assume non-philosophical
            
            postprocessing_start = time.time()
            # Initialize quality_result to prevent UnboundLocalError when fallback is detected
            quality_result = None
            final_response = None
            
            try:
                from backend.postprocessing.style_sanitizer import get_style_sanitizer
                from backend.postprocessing.quality_evaluator import get_quality_evaluator, QualityLevel
                from backend.postprocessing.rewrite_llm import get_rewrite_llm
                from backend.postprocessing.optimizer import get_postprocessing_optimizer
                
                optimizer = get_postprocessing_optimizer()
                
                # OPTIMIZATION: Check if we should skip post-processing
                should_skip, skip_reason = optimizer.should_skip_postprocessing(
                    question=chat_request.message,
                    response=response,
                    is_philosophical=is_philosophical_non_rag
                )
                
                if should_skip:
                    logger.info(f"‚è≠Ô∏è Skipping post-processing (non-RAG): {skip_reason}")
                    timing_logs["postprocessing"] = "skipped"
                    # CRITICAL: When skipping post-processing, preserve original response
                    final_response = response
                else:
                    # Stage 2: Hard Filter (0 token) - Style Sanitization
                    sanitizer = get_style_sanitizer()
                    sanitized_response = sanitizer.sanitize(response, is_philosophical=is_philosophical_non_rag)
                    
                    # CRITICAL: Ensure sanitized_response is not empty (defensive check)
                    # If sanitize() accidentally removed all content, fallback to original response
                    if not sanitized_response or not sanitized_response.strip():
                        logger.warning(
                            f"‚ö†Ô∏è Sanitized response is empty (original length: {len(response) if response else 0}), "
                            f"falling back to original response"
                        )
                        sanitized_response = response
                    
                    # CRITICAL: Check if sanitized response is a technical error or fallback message BEFORE quality evaluation
                    from backend.api.utils.error_detector import is_technical_error, is_fallback_message
                    is_error, error_type = is_technical_error(sanitized_response)
                    is_fallback = is_fallback_message(sanitized_response)
                    
                    if is_error or is_fallback:
                        # Technical error or fallback message detected - skip quality evaluation and rewrite
                        if is_error:
                            logger.warning(
                                f"‚ö†Ô∏è Technical error detected in sanitized response (non-RAG, type: {error_type}), "
                                f"skipping quality evaluation and rewrite"
                            )
                            processing_steps.append(f"‚ö†Ô∏è Technical error detected - skipping post-processing")
                        else:
                            logger.info(
                                f"üõë Fallback meta-answer detected in sanitized response (non-RAG), "
                                f"skipping quality evaluation and rewrite"
                            )
                            processing_steps.append(f"üõë Fallback message detected - skipping post-processing")
                        final_response = sanitized_response
                        # Skip all remaining post-processing (quality evaluation, rewrite)
                        # quality_result remains None, which is fine - we won't use it
                    else:
                        # Stage 3: Quality Evaluator (0 token) - Rule-based Quality Check
                        # OPTIMIZATION: Check cache first
                        evaluator = get_quality_evaluator()
                        cached_quality = optimizer.get_cached_quality_result(
                            question=chat_request.message,
                            response=sanitized_response
                        )
                        
                        if cached_quality:
                            quality_result = cached_quality
                            logger.debug("‚úÖ Using cached quality evaluation (non-RAG)")
                        else:
                            # Detect if this is a StillMe query for quality evaluation
                            is_stillme_query_for_quality = False
                            try:
                                from backend.core.stillme_detector import detect_stillme_query
                                is_stillme_query_for_quality, _ = detect_stillme_query(chat_request.message)
                            except Exception:
                                pass
                            
                            quality_result = evaluator.evaluate(
                                text=sanitized_response,
                                is_philosophical=is_philosophical_non_rag,
                                original_question=chat_request.message,
                                is_stillme_query=is_stillme_query_for_quality
                            )
                        # Cache the result
                        optimizer.cache_quality_result(
                            question=chat_request.message,
                            response=sanitized_response,
                            quality_result=quality_result
                        )
                        
                        # Phase 3: Only rewrite when CRITICAL issues are present
                        # Critical issues: missing citation, anthropomorphic language, language mismatch, topic drift, template-like
                        # Non-critical issues (depth, unpacking, style) do NOT trigger rewrite
                        
                        # CRITICAL: Check if this is a StillMe query with foundational knowledge
                        # Even in non-RAG path, we may have had context earlier (e.g., from RAG path that failed validation)
                        # Detect StillMe query and check if we should skip rewrite
                        skip_rewrite_for_stillme_non_rag = False
                        try:
                            from backend.core.stillme_detector import detect_stillme_query
                            is_stillme_query_non_rag, _ = detect_stillme_query(chat_request.message)
                            
                            # Check if response mentions foundational knowledge or StillMe capabilities
                            # If it's a StillMe query, skip rewrite to preserve accuracy
                            if is_stillme_query_non_rag:
                                skip_rewrite_for_stillme_non_rag = True
                                logger.info(
                                    "‚è≠Ô∏è Skipping rewrite for StillMe query (non-RAG path): "
                                    "Rewrite may corrupt StillMe capability information. "
                                    "Using original response to preserve accuracy."
                                )
                        except Exception as stillme_detect_error:
                            logger.debug(f"Could not detect StillMe query in non-RAG path: {stillme_detect_error}")
                        
                        if skip_rewrite_for_stillme_non_rag:
                            should_rewrite = False
                            rewrite_reason = "StillMe query - preserving accuracy"
                            max_attempts = 0
                        else:
                            should_rewrite, rewrite_reason, max_attempts = optimizer.should_rewrite(
                                quality_result=quality_result,
                                is_philosophical=is_philosophical_non_rag,
                                response_length=len(sanitized_response),
                                user_question=chat_request.message  # P2: Template detection
                            )
                        
                        # Stage 4: ALWAYS rewrite (100% policy) - M·ª•c ti√™u: minh b·∫°ch, trung th·ª±c, gi·∫£m ·∫£o gi√°c
                        if should_rewrite:
                            logger.info(
                                f"‚ö†Ô∏è Quality evaluator flagged output for rewrite (non-RAG). "
                                f"Issues: {quality_result['reasons']}, "
                                f"score: {quality_result.get('overall_score', 'N/A')}, "
                                f"length: {len(sanitized_response)}"
                            )
                            processing_steps.append(f"üîÑ Quality improvement needed - rewriting with DeepSeek")
                            
                            rewrite_llm = get_rewrite_llm()
                            # Non-RAG path: no ctx_docs available, pass empty list
                            # CRITICAL: Check if this is AI_SELF_MODEL domain
                            detected_domain = detect_domain(chat_request.message)
                            is_ai_self_model_domain = (detected_domain == DomainType.AI_SELF_MODEL)
                            
                            rewrite_result = await rewrite_llm.rewrite(
                                text=sanitized_response,
                                original_question=chat_request.message,
                                quality_issues=quality_result["reasons"],
                                is_philosophical=is_philosophical_non_rag,
                                detected_lang=detected_lang,
                                ctx_docs=[],  # Non-RAG path has no context documents
                                is_ai_self_model=is_ai_self_model_domain
                            )
                            
                            if rewrite_result.was_rewritten:
                                # Re-sanitize rewritten output (in case rewrite introduced issues)
                                re_sanitized = sanitizer.sanitize(rewrite_result.text, is_philosophical=is_philosophical_non_rag)
                                # CRITICAL: Only use re-sanitized if it's not empty
                                if re_sanitized and re_sanitized.strip():
                                    final_response = re_sanitized
                                    logger.debug(f"‚úÖ Post-processing complete (non-RAG): sanitized ‚Üí evaluated ‚Üí rewritten ‚Üí re-sanitized")
                                else:
                                    logger.warning(
                                        f"‚ö†Ô∏è Re-sanitized response is empty (rewrite_result length: {len(rewrite_result.text) if rewrite_result.text else 0}), "
                                        f"falling back to sanitized_response"
                                    )
                                    final_response = sanitized_response
                                    logger.debug(f"‚úÖ Post-processing complete (non-RAG): sanitized ‚Üí evaluated ‚Üí rewritten ‚Üí fallback to sanitized (re-sanitize was empty)")
                            else:
                                # Fallback to sanitized original - rewrite failed
                                final_response = sanitized_response
                                error_detail = rewrite_result.error or "Unknown error"
                                logger.info(
                                    f"‚ÑπÔ∏è DeepSeek rewrite skipped (non-RAG, error: {error_detail[:100]}), "
                                    f"using original sanitized response (this is normal if API is unavailable or timeout)"
                                )
                                processing_steps.append(f"‚ÑπÔ∏è Rewrite skipped, using original (sanitized)")
                        else:
                            final_response = sanitized_response
                            if should_rewrite and quality_result:
                                logger.debug(f"‚è≠Ô∏è Skipping rewrite (non-RAG): {rewrite_reason}")
                            if quality_result:
                                logger.debug(f"‚úÖ Post-processing complete (non-RAG): sanitized ‚Üí evaluated ‚Üí passed (quality: {quality_result['depth_score']})")
                            else:
                                logger.debug(f"‚úÖ Post-processing complete (non-RAG): sanitized ‚Üí passed (no quality evaluation)")
                
                response = final_response
                
                # CRITICAL: Final check - ensure response is not a technical error
                if response:
                    from backend.api.utils.error_detector import is_technical_error, get_fallback_message_for_error
                    is_error, error_type = is_technical_error(response)
                    if is_error:
                        logger.error(f"‚ö†Ô∏è Final response (non-RAG) is still a technical error (type: {error_type}) - replacing with fallback")
                        response = get_fallback_message_for_error(error_type, detected_lang)
                
                postprocessing_time = time.time() - postprocessing_start
                timing_logs["postprocessing"] = f"{postprocessing_time:.3f}s"
                logger.info(f"‚è±Ô∏è Post-processing (non-RAG) took {postprocessing_time:.3f}s")
            except Exception as postprocessing_error:
                logger.error(f"Post-processing error (non-RAG): {postprocessing_error}", exc_info=True)
                # Fallback to original response if post-processing fails
                # Don't break the pipeline - post-processing is enhancement, not critical
                logger.warning(f"‚ö†Ô∏è Post-processing failed (non-RAG), using original response")
                timing_logs["postprocessing"] = "failed"
                # CRITICAL: Ensure final_response is set to original response if post-processing fails
                if final_response is None:
                    final_response = response
        
        # CRITICAL: Ensure response is set from final_response, or keep original if final_response is None or empty
        # CRITICAL FIX: Use safe Unicode slicing for logging (prevents breaking multi-byte characters)
        response_preview = _safe_unicode_slice(response, 200) if response else 'None'
        final_response_preview = _safe_unicode_slice(final_response, 200) if final_response else 'None'
        
        # CRITICAL: Special logging for Chinese language to debug Unicode issues
        if detected_lang == "zh":
            logger.info(
                f"üîç [TRACE] [CHINESE] Before final_response assignment: "
                f"response_length={len(response) if response else 0}, "
                f"response_preview={response_preview}, "
                f"final_response_length={len(final_response) if final_response else 0}, "
                f"final_response_preview={final_response_preview}, "
                f"response_type={type(response)}, final_response_type={type(final_response)}"
            )
        else:
            logger.info(
                f"üîç [TRACE] Before final_response assignment: "
                f"response={response_preview}, final_response={final_response_preview}, "
                f"response_type={type(response)}, final_response_type={type(final_response)}"
            )
        
        # CRITICAL FIX: Only use final_response if it's not None AND not empty
        # This prevents empty string from overwriting valid response content
        if final_response is not None and isinstance(final_response, str) and final_response.strip():
            # CRITICAL: Clean final_response from problematic characters before using
            final_response_cleaned = _clean_response_text(final_response)
            if len(final_response_cleaned) != len(final_response):
                logger.warning(
                    f"‚ö†Ô∏è Cleaned final_response: removed {len(final_response) - len(final_response_cleaned)} "
                    f"problematic characters (detected_lang={detected_lang})"
                )
            
            # CRITICAL: Ensure line breaks are preserved (defensive check)
            # Count newlines before and after cleaning
            newlines_before = final_response.count('\n')
            newlines_after = final_response_cleaned.count('\n')
            if newlines_after < newlines_before * 0.9:  # If more than 10% of newlines lost
                logger.warning(
                    f"‚ö†Ô∏è Line breaks may have been lost during cleaning: "
                    f"before={newlines_before}, after={newlines_after}"
                )
                # Try to restore line breaks by checking if original had them
                if '\n' in final_response and '\n' not in final_response_cleaned:
                    logger.error(f"‚ùå CRITICAL: All line breaks were removed! Restoring from original.")
                    final_response_cleaned = final_response  # Use original if all newlines lost
            
            # CRITICAL: Auto-fix missing line breaks after headings and bullets
            # If LLM didn't follow instruction, we fix it here
            final_response_cleaned = _fix_missing_line_breaks(final_response_cleaned)
            
            # final_response is valid and non-empty - use it
            response = final_response_cleaned
            
            # CRITICAL: Log after assignment with safe slicing
            response_after_preview = _safe_unicode_slice(response, 200) if response else 'None'
            if detected_lang == "zh":
                logger.info(
                    f"üîç [TRACE] [CHINESE] After final_response assignment: "
                    f"response_length={len(response) if response else 0}, "
                    f"response_preview={response_after_preview}, response_type={type(response)}"
                )
            else:
                logger.info(f"üîç [TRACE] After final_response assignment: response={response_after_preview}, response_type={type(response)}")
        elif final_response is not None and (not isinstance(final_response, str) or not final_response.strip()):
            # CRITICAL: final_response is empty string or invalid - keep original response
            logger.error(
                f"‚ùå [TRACE] CRITICAL: final_response is empty or invalid (type: {type(final_response)}, "
                f"length: {len(final_response) if isinstance(final_response, str) else 'N/A'}), "
                f"keeping original response to prevent content loss. "
                f"original_response_length={len(response) if response else 0}"
            )
            # Keep original response - don't overwrite with empty string
            # Log warning but don't change response
            processing_steps.append("‚ö†Ô∏è CRITICAL: final_response was empty/invalid - preserved original response")
        else:
            # final_response is None - keep original response
            logger.warning(f"‚ö†Ô∏è [TRACE] final_response is None, keeping original response: response={response[:200] if response else 'None'}, response_type={type(response)}")
            # CRITICAL: If both response and final_response are None, we have a problem
            if response is None:
                logger.error(f"‚ùå [TRACE] CRITICAL: Both response and final_response are None! This should never happen. Using fallback.")
                from backend.api.utils.error_detector import get_fallback_message_for_error
                response = get_fallback_message_for_error("generic", detected_lang or "vi")
                processing_steps.append("‚ö†Ô∏è CRITICAL: Both response and final_response were None - using fallback")
        
        # Calculate total response latency
        # Total_Response_Latency: Time from request received to response returned
        total_response_end = time.time()
        total_response_latency = total_response_end - start_time
        
        # Format latency metrics log as specified by user
        # B·∫ÆT BU·ªòC HI·ªÇN TH·ªä LOG: In ra ngay l·∫≠p t·ª©c sau c√¢u tr·∫£ l·ªùi
        latency_metrics_text = f"""--- LATENCY METRICS ---
RAG_Retrieval_Latency: {rag_retrieval_latency:.2f} gi√¢y
LLM_Inference_Latency: {llm_inference_latency:.2f} gi√¢y
Total_Response_Latency: {total_response_latency:.2f} gi√¢y
-----------------------"""
        
        logger.info(latency_metrics_text)
        
        # Add latency metrics to timing_logs for API response
        timing_logs["rag_retrieval_latency"] = f"{rag_retrieval_latency:.2f}s"
        timing_logs["llm_inference_latency"] = f"{llm_inference_latency:.2f}s"
        timing_logs["total_response_latency"] = f"{total_response_latency:.2f}s"
        timing_logs["total"] = f"{total_response_latency:.2f}s"
        # Add formatted latency metrics text for frontend display
        timing_logs["latency_metrics_formatted"] = latency_metrics_text
        logger.info(f"üìä Timing breakdown: {timing_logs}")
        
        # Analyze conversation for learning opportunities
        learning_proposal = None
        permission_request = None
        proposal_id = None
        
        # CRITICAL: Detect if base knowledge was used - this indicates a knowledge gap
        used_base_knowledge = False
        if response:
            response_lower = response.lower()
            base_knowledge_indicators = [
                "based on general knowledge", "not from stillme", "not from rag",
                "ki·∫øn th·ª©c chung", "kh√¥ng t·ª´ stillme", "kh√¥ng t·ª´ rag",
                "training data", "d·ªØ li·ªáu hu·∫•n luy·ªán", "base knowledge",
                "general knowledge", "ki·∫øn th·ª©c n·ªÅn t·∫£ng"
            ]
            used_base_knowledge = any(indicator in response_lower for indicator in base_knowledge_indicators)
        
        if rag_retrieval and chat_request.use_rag:
            try:
                from backend.services.conversation_learning_extractor import get_conversation_learning_extractor
                from backend.services.learning_proposal_storage import get_learning_proposal_storage
                
                extractor = get_conversation_learning_extractor()
                storage = get_learning_proposal_storage()
                
                # PRIORITY: If base knowledge was used, extract topic for learning proposal
                # Check if: (1) No context, OR (2) Context exists but not relevant (low overlap)
                # Fix: Ensure validation_result is always defined before use
                has_no_context = not context or context.get("total_context_docs", 0) == 0
                has_low_relevance = False
                
                # CRITICAL: Extract validation_result from validation_info if available
                validation_result = None
                if validation_info and isinstance(validation_info, dict):
                    validation_result = validation_info.get("validation_result")
                elif hasattr(validation_info, 'validation_result'):
                    validation_result = validation_info.validation_result
                
                if validation_result and hasattr(validation_result, 'reasons'):
                    has_low_relevance = any("citation_relevance_warning" in r for r in validation_result.reasons)
                
                # CRITICAL: Only analyze for learning if response is NOT a fallback meta-answer
                if not is_fallback_for_learning and used_base_knowledge and (has_no_context or has_low_relevance):
                    # No RAG context OR irrelevant context + base knowledge used = knowledge gap detected
                    # Extract topic from user's question for learning proposal
                    logger.info(f"üîç Base knowledge used - detecting knowledge gap for learning proposal (no_context: {has_no_context}, low_relevance: {has_low_relevance})")
                    try:
                        # Extract main topic from user question
                        user_question = chat_request.message
                        # Simple topic extraction: use first 200 chars or key phrases
                        topic_snippet = user_question[:200] if len(user_question) > 200 else user_question
                        
                        # Create learning proposal for this knowledge gap
                        base_knowledge_proposal = {
                            "knowledge_snippet": f"Topic: {topic_snippet}\n\nStillMe used base LLM knowledge to answer this question, indicating a knowledge gap in RAG. This topic should be prioritized for learning from trusted sources (arXiv, Wikipedia, RSS feeds).",
                            "source": "knowledge_gap_detection",
                            "knowledge_score": 0.7,  # High priority - user asked about it
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            "reason": f"StillMe used base knowledge to answer about '{topic_snippet[:50]}...'. This indicates RAG knowledge gap - should learn this topic from trusted sources.",
                            "is_knowledge_gap": True,
                            "user_question": user_question,
                            "detected_from": "base_knowledge_usage"
                        }
                        
                        # Store learning proposal
                        proposal_id = storage.save_proposal(
                            proposal=base_knowledge_proposal,
                            user_id=chat_request.user_id
                        )
                        base_knowledge_proposal["proposal_id"] = proposal_id
                        learning_proposal = base_knowledge_proposal
                        logger.info(f"‚úÖ Knowledge gap learning proposal created (id: {proposal_id}, topic: {topic_snippet[:50]}...)")
                    except Exception as gap_error:
                        logger.warning(f"Failed to create knowledge gap proposal: {gap_error}")
                
                # Also check for valuable knowledge from user/assistant (existing logic)
                # CRITICAL: Only analyze if response is NOT a fallback meta-answer
                if not is_fallback_for_learning and not learning_proposal:  # Only if we didn't already create a gap proposal
                    learning_proposal = extractor.analyze_conversation_for_learning(
                        user_message=chat_request.message,
                        assistant_response=response,
                        context=context
                    )
                
                if learning_proposal:
                    # If we didn't create proposal above, store it now
                    if "proposal_id" not in learning_proposal:
                        proposal_id = storage.save_proposal(
                            proposal=learning_proposal,
                            user_id=chat_request.user_id
                        )
                        learning_proposal["proposal_id"] = proposal_id
                    
                    # Format permission request
                    permission_request = extractor.format_permission_request(
                        learning_proposal=learning_proposal,
                        language=detected_lang
                    )
                    logger.info(f"Learning proposal generated (id: {proposal_id}, score: {learning_proposal.get('knowledge_score', 0):.2f})")
            except Exception as extractor_error:
                logger.warning(f"Conversation learning extractor error: {extractor_error}")
        
        # Store conversation in vector DB
        if rag_retrieval:
            # P2: Fix conversation embedding blocking - use asyncio.to_thread to truly non-block
            # This ensures embedding generation runs in thread pool, not blocking event loop
            async def store_conversation_background():
                try:
                    # P2: Run blocking I/O in thread pool to avoid blocking event loop
                    # Note: 'asyncio' is already imported at top level (line 32)
                    await asyncio.to_thread(
                        rag_retrieval.add_learning_content,
                        content=f"Q: {chat_request.message}\nA: {response}",
                        source="user_chat",
                        content_type="conversation",
                        metadata={
                            "user_id": chat_request.user_id,
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            "accuracy_score": accuracy_score
                        }
                    )
                    logger.debug("‚úÖ Conversation stored in background (non-blocking)")
                except Exception as store_error:
                    logger.error(f"‚ùå Failed to store conversation in background: {store_error}", exc_info=True)
            
            # Create background task (fire and forget - don't await)
            asyncio.create_task(store_conversation_background())
            logger.debug("üöÄ Conversation storage queued in background task (P2: truly non-blocking)")
        
        # Knowledge Alert: Retrieve important knowledge related to query
        knowledge_alert = None
        if rag_retrieval:
            try:
                important_knowledge = rag_retrieval.retrieve_important_knowledge(
                    query=chat_request.message,
                    limit=1,
                    min_importance=0.7
                )
                
                if important_knowledge:
                    doc = important_knowledge[0]
                    metadata = doc.get("metadata", {})
                    knowledge_alert = {
                        "title": metadata.get("title", "Important Knowledge"),
                        "source": metadata.get("source", "Unknown"),
                        "importance_score": metadata.get("importance_score", 0.7),
                        "content_preview": doc.get("content", "")[:200] + "..." if len(doc.get("content", "")) > 200 else doc.get("content", "")
                    }
                    logger.info(f"Knowledge alert generated: {knowledge_alert.get('title', 'Unknown')}")
            except Exception as alert_error:
                logger.warning(f"Knowledge alert error: {alert_error}")
        
        # Generate learning suggestions from knowledge gaps if context is empty or low confidence
        learning_suggestions = None
        if (confidence_score is not None and confidence_score < 0.5) or (context and context.get("total_context_docs", 0) == 0):
            try:
                self_diagnosis = get_self_diagnosis()
                if self_diagnosis:
                    gap_result = self_diagnosis.identify_knowledge_gaps(chat_request.message, threshold=0.5)
                    if gap_result.get("has_gap"):
                        suggestion = gap_result.get("suggestion")
                        if suggestion:
                            learning_suggestions = [suggestion]
                        else:
                            # Extract key terms from query for learning suggestions
                            words = re.findall(r'\b\w+\b', chat_request.message.lower())
                            # Filter out common words
                            common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'what', 'how', 'why', 'when', 'where', 'who', 'which', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'to', 'of', 'in', 'on', 'at', 'for', 'with', 'by', 'from', 'as', 'about', 'into', 'through', 'during', 'including', 'against', 'among', 'throughout', 'despite', 'towards', 'upon', 'concerning', 'to', 'of', 'in', 'on', 'at', 'for', 'with', 'by', 'from', 'as', 'about', 'into', 'through', 'during', 'including', 'against', 'among', 'throughout', 'despite', 'towards', 'upon', 'concerning'}
                            key_terms = [w for w in words if len(w) > 3 and w not in common_words][:3]
                            if key_terms:
                                learning_suggestions = [f"Consider learning more about: {', '.join(key_terms)}"]
            except Exception as suggestion_error:
                logger.warning(f"Failed to generate learning suggestions: {suggestion_error}")
        
        # Generate unique message ID for feedback tracking
        import uuid
        message_id = f"msg_{uuid.uuid4().hex[:16]}"
        
        # If style learning response exists, prepend it to the response
        if style_learning_response:
            response = f"{style_learning_response}\n\n---\n\n{response}"
        
        # CONVERSATIONAL INTELLIGENCE: Add disclaimer for MEDIUM ambiguity
        # Philosophy: Answer with assumptions acknowledged, not just answer blindly
        # Based on StillMe Manifesto Principle 5: "EMBRACE 'I DON'T KNOW' AS INTELLECTUAL HONESTY"
        if 'ambiguity_level' in locals() and ambiguity_level == "MEDIUM" and ambiguity_score >= 0.4:
            try:
                detected_lang_for_disclaimer = detected_lang if 'detected_lang' in locals() else detect_language(response or chat_request.message)
                if detected_lang_for_disclaimer == "vi":
                    disclaimer = f"\n\nüí° *L∆∞u √Ω: M√¨nh ƒë√£ suy lu·∫≠n √Ω ƒë·ªãnh c·ªßa b·∫°n d·ª±a tr√™n ng·ªØ c·∫£nh. N·∫øu m√¨nh hi·ªÉu sai, b·∫°n c√≥ th·ªÉ l√†m r√µ ƒë·ªÉ m√¨nh tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.*"
                else:
                    disclaimer = f"\n\nüí° *Note: I've inferred your intent based on context. If I misunderstood, please clarify so I can answer more accurately.*"
                
                # Add disclaimer at the end, but before citation
                if response and "[general knowledge]" not in response[-100:]:  # Don't add if citation already at end
                    response = response + disclaimer
                elif response:
                    # Citation is at end, add before citation
                    # CRITICAL: Use Unicode-safe string operations
                    citation_pos = response.rfind("[")
                    if citation_pos > 0:
                        # CRITICAL: Safe string slicing (Python handles Unicode correctly)
                        response_before = response
                        response = response[:citation_pos] + disclaimer + " " + response[citation_pos:]
                        # CRITICAL: Validate result is not empty
                        if not response or not response.strip():
                            logger.warning(
                                f"‚ö†Ô∏è Response became empty after adding disclaimer before citation "
                                f"(detected_lang={detected_lang_for_disclaimer}), "
                                f"falling back to original response"
                            )
                            response = response_before  # Fallback to original
                    else:
                        response = response + disclaimer
                
                logger.info(f"‚úÖ Added MEDIUM ambiguity disclaimer to response")
                if validation_info:
                    validation_info["ambiguity_disclaimer_added"] = True
                    validation_info["ambiguity_score"] = ambiguity_score
                    validation_info["ambiguity_level"] = ambiguity_level
            except Exception as disclaimer_error:
                logger.warning(f"‚ö†Ô∏è Failed to add ambiguity disclaimer: {disclaimer_error}")
        
        # CRITICAL: Final safety check - ensure response is never None or empty before returning
        # CRITICAL FIX: Use safe Unicode slicing for logging
        response_final_preview = _safe_unicode_slice(response, 200) if response else 'None'
        raw_response_preview = _safe_unicode_slice(raw_response, 200) if raw_response else 'None'
        final_response_final_preview = _safe_unicode_slice(final_response, 200) if final_response else 'None'
        
        # CRITICAL: Special logging for Chinese language
        if detected_lang == "zh":
            logger.info(
                f"üîç [TRACE] [CHINESE] Final check before return: "
                f"response_length={len(response) if response else 0}, "
                f"response_preview={response_final_preview}, "
                f"response_type={type(response)}"
            )
        else:
            logger.info(
                f"üîç [TRACE] Final check before return: "
                f"response={response_final_preview}, "
                f"response_type={type(response)}, response_length={len(response) if response else 0}"
            )
        
        # CRITICAL: Clean response from problematic characters before final validation
        if response and isinstance(response, str):
            original_response_length = len(response)
            response = _clean_response_text(response)
            if len(response) != original_response_length:
                logger.warning(
                    f"‚ö†Ô∏è Cleaned response before return: removed {original_response_length - len(response)} "
                    f"problematic characters (detected_lang={detected_lang})"
                )
            
            # CRITICAL: Auto-fix missing line breaks as final defensive check
            response = _fix_missing_line_breaks(response)
        
        if not response or not isinstance(response, str) or not response.strip():
            logger.error(
                f"‚ö†Ô∏è Response is None, empty, or invalid before returning ChatResponse - using fallback. "
                f"response={response}, type={type(response)}, detected_lang={detected_lang}"
            )
            logger.error(
                f"‚ö†Ô∏è Debug info: raw_response={raw_response_preview}, "
                f"final_response={final_response_final_preview}"
            )
            logger.error(f"‚ö†Ô∏è Processing steps: {processing_steps[-5:] if len(processing_steps) > 5 else processing_steps}")
            from backend.api.utils.error_detector import get_fallback_message_for_error
            response = get_fallback_message_for_error("generic", detected_lang or "vi")
            processing_steps.append("‚ö†Ô∏è Response validation failed - using fallback message")
        
        # Add timestamp attribution to normal RAG responses for transparency (consistent with external data)
        # Skip if this is an external data response (already has timestamp) or fallback message
        # CRITICAL: Skip for pure philosophical questions (reasoning, not factual claims)
        from backend.api.utils.error_detector import is_fallback_message
        is_fallback = is_fallback_message(response) if response else False
        has_external_data_timestamp = "[Source:" in response or "[Ngu·ªìn:" in response
        
        # CRITICAL: Check if this is a self-knowledge question about StillMe's codebase
        # If so, skip timestamp addition (or only add foundational knowledge citation)
        is_self_knowledge_question = False
        if chat_request.message:
            question_lower = chat_request.message.lower()
            codebase_self_patterns = [
                r"codebase.*c·ªßa.*b·∫°n",
                r"codebase.*of.*you",
                r"codebase.*stillme",
                r"validator.*trong.*codebase",
                r"validator.*in.*codebase",
                r"l·ªõp.*validator.*trong.*codebase",
                r"layer.*validator.*in.*codebase",
                r"bao nhi√™u.*l·ªõp.*validator.*codebase",
                r"how many.*layer.*validator.*codebase",
                r"li·ªát k√™.*l·ªõp.*validator.*codebase",
                r"list.*validator.*layer.*codebase"
            ]
            # Note: 're' module is already imported at top level (line 30)
            # Use top-level import to avoid UnboundLocalError
            for pattern in codebase_self_patterns:
                if re.search(pattern, question_lower, re.IGNORECASE):
                    is_self_knowledge_question = True
                    logger.info(f"‚úÖ Self-knowledge question detected - will skip external citations in timestamp")
                    break
        
        # CRITICAL: Skip timestamp addition for philosophical questions (they don't need citations/timestamps)
        # For self-knowledge questions, we still add timestamp but will filter out external citations in _add_timestamp_to_response
        should_add_timestamp = not is_fallback and not has_external_data_timestamp and response and not is_philosophical
        if should_add_timestamp:
            try:
                # CRITICAL: Log response state before adding timestamp (especially for Chinese)
                if detected_lang == "zh":
                    logger.debug(
                        f"üîç [CHINESE] Before _add_timestamp_to_response: "
                        f"response_length={len(response) if response else 0}, "
                        f"response_preview={_safe_unicode_slice(response, 100) if response else 'None'}"
                    )
                
                # Pass context and user_question to extract source links if available
                # CRITICAL: Pass user_question so _add_timestamp_to_response can filter out external citations for self-knowledge questions
                response_before_timestamp = response
                response_after_timestamp = _add_timestamp_to_response(
                    response, 
                    detected_lang or "en", 
                    context, 
                    user_question=chat_request.message,
                    is_system_architecture_query=is_system_architecture_query
                )
                
                # CRITICAL: Validate response after adding timestamp
                if not response_after_timestamp or not isinstance(response_after_timestamp, str) or not response_after_timestamp.strip():
                    logger.error(
                        f"‚ùå CRITICAL: Response became empty after _add_timestamp_to_response "
                        f"(detected_lang={detected_lang}, before_length={len(response_before_timestamp) if response_before_timestamp else 0}), "
                        f"falling back to original response"
                    )
                    response = response_before_timestamp  # Fallback to original
                else:
                    # CRITICAL: Only use timestamp-added response if it's not significantly shorter
                    # If timestamp addition caused significant content loss, fallback to original
                    if len(response_after_timestamp) < len(response_before_timestamp) * 0.9:
                        logger.error(
                            f"‚ùå CRITICAL: _add_timestamp_to_response caused significant content loss "
                            f"(before: {len(response_before_timestamp)}, after: {len(response_after_timestamp)}), "
                            f"falling back to original response"
                        )
                        response = response_before_timestamp
                    else:
                        response = response_after_timestamp
                
                # CRITICAL: Log response state after adding timestamp (especially for Chinese)
                if detected_lang == "zh":
                    logger.debug(
                        f"üîç [CHINESE] After _add_timestamp_to_response: "
                        f"response_length={len(response) if response else 0}, "
                        f"response_preview={_safe_unicode_slice(response, 100) if response else 'None'}"
                    )
                else:
                    logger.debug("‚úÖ Added timestamp attribution to RAG response")
            except Exception as e:
                logger.error(
                    f"‚ö†Ô∏è Failed to add timestamp to response (detected_lang={detected_lang}): {e}",
                    exc_info=True
                )
                # CRITICAL: If timestamp addition fails, keep original response (don't lose content)
                # response already contains original value, no need to change
        
        # Time Estimation Integration: Check if user is asking about time estimation
        try:
            from backend.core.time_estimation_intent import detect_time_estimation_intent
            from stillme_core.monitoring import get_estimation_engine, format_self_aware_response
            
            is_time_estimation, task_description = detect_time_estimation_intent(chat_request.message)
            
            if is_time_estimation and task_description and not is_fallback:
                try:
                    estimator = get_estimation_engine()
                    
                    # Determine task type and complexity from query
                    task_type = "learning"  # Default for StillMe chatbot
                    complexity = "moderate"  # Default
                    size = 100  # Default
                    
                    # Try to infer from task description
                    task_lower = task_description.lower()
                    if "learn" in task_lower or "h·ªçc" in task_lower:
                        task_type = "learning"
                        # Try to extract number of items
                        # Note: 're' module is already imported at top level
                        num_match = re.search(r'(\d+)', task_description)
                        if num_match:
                            size = int(num_match.group(1))
                    elif "validate" in task_lower or "ki·ªÉm tra" in task_lower:
                        task_type = "validation"
                    elif "refactor" in task_lower or "migrate" in task_lower:
                        task_type = "refactoring"
                    
                    # Estimate time
                    estimate = estimator.estimate(
                        task_description=task_description,
                        task_type=task_type,
                        complexity=complexity,
                        size=size
                    )
                    
                    # CONTEXTUAL RELEVANCE CHECK: Only add time estimate if it's contextually relevant
                    # Don't add if response is about human learning or general knowledge (not StillMe's task execution)
                    response_lower = response.lower()
                    
                    # Check if response is about human learning (not StillMe's task execution)
                    human_learning_indicators = [
                        "your learning", "your pace", "you can", "you will", "for you",
                        "human learning", "con ng∆∞·ªùi h·ªçc", "b·∫°n h·ªçc", "ng∆∞·ªùi h·ªçc",
                        "personal", "individual", "your specific", "your cognitive",
                        "hours total", "hours of", "tens to hundreds of hours",
                        "20-50 hours", "50-150 hours", "150-300 hours",
                        "based on general knowledge", "general learning principles",
                        "learning theory", "cognitive psychology"
                    ]
                    
                    is_about_human_learning = any(
                        indicator in response_lower for indicator in human_learning_indicators
                    )
                    
                    # Check if response is about StillMe's task execution (not capabilities)
                    stillme_task_execution_indicators = [
                        "my execution", "my task", "my performance",
                        "i track", "i estimate", "my historical",
                        "stillme th·ª±c thi", "stillme ∆∞·ªõc l∆∞·ª£ng", "execution time",
                        "task completion", "ho√†n th√†nh t√°c v·ª•"
                    ]
                    
                    is_about_stillme_task_execution = any(
                        indicator in response_lower for indicator in stillme_task_execution_indicators
                    )
                    
                    # CRITICAL: Exclude capability questions (validation warnings, learning frequency, etc.)
                    # These are about StillMe's features, NOT about task execution time
                    capability_question_indicators = [
                        "validation", "warning", "c·∫£nh b√°o", "x√°c th·ª±c",
                        "learning frequency", "t·∫ßn su·∫•t", "c·∫≠p nh·∫≠t",
                        "timestamp", "th·ªùi ƒëi·ªÉm", "source", "ngu·ªìn",
                        "confidence score", "ƒëi·ªÉm tin c·∫≠y", "threshold", "ng∆∞·ª°ng",
                        "capability", "t√≠nh nƒÉng", "kh·∫£ nƒÉng", "h·ªá th·ªëng h·ªçc",
                        "how does stillme", "stillme hi·ªÉn th·ªã", "stillme cung c·∫•p",
                        "stillme s·∫Ω", "stillme tuy√™n b·ªë"
                    ]
                    
                    is_capability_question = any(
                        indicator in chat_request.message.lower() for indicator in capability_question_indicators
                    )
                    
                    # Decision: Add estimate only if:
                    # 1. User explicitly asked about time estimation (is_time_estimation is True), AND
                    # 2. Response is about StillMe's task execution (not capabilities), AND
                    # 3. Question is NOT about StillMe's capabilities/features
                    should_add_estimate = (
                        is_time_estimation and  # User asked about time estimation
                        is_about_stillme_task_execution and  # Response is about task execution
                        not is_capability_question  # Question is NOT about capabilities
                    )
                    
                    if should_add_estimate:
                        # Determine language for time estimate
                        # Check if response is in Vietnamese (even if detected_lang was wrong)
                        is_vietnamese_response = (
                            "ti·∫øng vi·ªát" in response_lower or
                            "vietnamese" in response_lower or
                            any(char in "√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë" for char in response)
                        )
                        estimate_language = "vi" if (detected_lang == "vi" or is_vietnamese_response) else "en"
                        
                        # Format with AI identity in correct language
                        estimate_text = format_self_aware_response(estimate, include_identity=True, language=estimate_language)
                        
                        if estimate_language == "vi":
                            response = f"{response}\n\n---\n\n‚è±Ô∏è **∆Ø·ªõc t√≠nh th·ªùi gian:**\n{estimate_text}"
                        else:
                            response = f"{response}\n\n---\n\n‚è±Ô∏è **Time Estimate:**\n{estimate_text}"
                        
                        logger.info(f"‚úÖ Added time estimation to response: {task_description}")
                    else:
                        logger.info(f"‚è≠Ô∏è Skipped time estimation: Response is about human learning, not StillMe's task execution")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to add time estimation: {e}")
        except ImportError as e:
            logger.debug(f"Time estimation not available: {e}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error in time estimation integration: {e}")
        
        # Calculate epistemic state (after validation, before returning response)
        from backend.core.epistemic_state import calculate_epistemic_state, EpistemicState
        try:
            # Extract context_docs_count from context or validation_info
            ctx_docs_count = 0
            max_similarity = None  # TRUST-EFFICIENT: Extract max_similarity for accurate epistemic state
            if context and isinstance(context, dict):
                ctx_docs_count = context.get("total_context_docs", 0)
                # Extract max_similarity from knowledge_docs
                knowledge_docs = context.get("knowledge_docs", [])
                if knowledge_docs:
                    similarity_scores = []
                    for doc in knowledge_docs:
                        if isinstance(doc, dict):
                            similarity_scores.append(doc.get('similarity', 0.0))
                        elif hasattr(doc, 'similarity'):
                            similarity_scores.append(doc.similarity if isinstance(doc.similarity, (int, float)) else 0.0)
                        else:
                            similarity_scores.append(0.0)
                    if similarity_scores:
                        max_similarity = max(similarity_scores)
            elif validation_info and isinstance(validation_info, dict):
                ctx_docs_count = validation_info.get("context_docs_count", 0)
            
            epistemic_state = calculate_epistemic_state(
                validation_info=validation_info,
                confidence_score=confidence_score,
                response_text=response,
                context_docs_count=ctx_docs_count,
                max_similarity=max_similarity  # TRUST-EFFICIENT: Pass similarity for accurate state
            )
            confidence_str = f"{confidence_score:.2f}" if confidence_score else 'N/A'
            similarity_str = f"{max_similarity:.3f}" if max_similarity is not None else 'N/A'
            
            # MANIFESTO ALIGNMENT: Calculate Transparency Scorecard
            # Based on StillMe Manifesto Principle 6: "LOG EVERYTHING BECAUSE SECRETS CORRUPT TRUST"
            try:
                from backend.utils.transparency_scorecard import get_transparency_scorer
                from backend.utils.citation_formatter import get_citation_formatter
                from backend.core.epistemic_reasoning import get_epistemic_reasoning
                
                transparency_scorer = get_transparency_scorer()
                citation_formatter = get_citation_formatter()
                
                # Extract citation from response
                citation = None
                if response:
                    # Try to find citation in response
                    # Note: 're' module is already imported at top level
                    cite_pattern = re.compile(r'\[([^\]]+)\]')
                    matches = cite_pattern.findall(response)
                    if matches:
                        citation = f"[{matches[-1]}]"  # Use last citation found
                
                # Count validators that ran
                validators_run = 12  # Default: assume all validators
                if validation_info and isinstance(validation_info, dict):
                    # Try to infer from validation reasons
                    reasons = validation_info.get("reasons", [])
                    # If no critical failures, assume most validators ran
                    if validation_info.get("passed", False):
                        validators_run = 12
                    else:
                        # Some validators may have been skipped
                        validators_run = max(8, 12 - len([r for r in reasons if "timeout" in r.lower() or "skipped" in r.lower()]))
                
                # Check for epistemic explanation
                has_epistemic_explanation = False
                has_uncertainty_expression = False
                if response:
                    response_lower = response.lower()
                    # Check for epistemic reasoning patterns
                    epistemic_patterns = [
                        "v√¨:", "because:", "do", "b·ªüi v√¨", "nguy√™n nh√¢n",
                        "limited context", "conflicting", "outdated", "low similarity"
                    ]
                    has_epistemic_explanation = any(pattern in response_lower for pattern in epistemic_patterns)
                    
                    # Check for uncertainty expression
                    uncertainty_patterns = [
                        "kh√¥ng ch·∫Øc", "uncertain", "not certain", "don't know",
                        "kh√¥ng c√≥ ƒë·ªß", "sufficient information"
                    ]
                    has_uncertainty_expression = any(pattern in response_lower for pattern in uncertainty_patterns)
                
                # Calculate scorecard
                scorecard = transparency_scorer.calculate_scorecard(
                    citation=citation,
                    validators_run=validators_run,
                    validators_total=12,
                    has_epistemic_explanation=has_epistemic_explanation,
                    has_uncertainty_expression=has_uncertainty_expression,
                    log_count=15,  # Approximate log count (can be improved)
                    expected_logs=15
                )
                
                overall_score = scorecard.calculate_overall()
                scorecard_dict = scorecard.to_dict()
                
                logger.info(
                    f"üìä Transparency Scorecard: {overall_score:.1%} "
                    f"(citation={scorecard.citation_specificity:.2f}, "
                    f"validation={scorecard.validation_completeness:.2f}, "
                    f"epistemic={scorecard.epistemic_honesty:.2f}, "
                    f"traceability={scorecard.process_traceability:.2f})"
                )
                
                # Add to validation_info for API response
                if validation_info:
                    validation_info["transparency_scorecard"] = scorecard_dict
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not calculate transparency scorecard: {e}")
            
            logger.info(f"üìä EpistemicState: {epistemic_state.value} (confidence={confidence_str}, ctx_docs={ctx_docs_count}, max_similarity={similarity_str})")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to calculate epistemic state: {e}, defaulting to UNKNOWN")
            epistemic_state = EpistemicState.UNKNOWN
        
        # End decision logging session
        if 'decision_logger' in locals() and decision_logger.current_session:
            final_outcome = f"Response generated with confidence={confidence_score:.2f}, epistemic_state={epistemic_state.value if epistemic_state else 'UNKNOWN'}"
            decision_logger.end_session(final_outcome)
        
        return ChatResponse(
            response=response,
            message_id=message_id,
            context_used=context,
            accuracy_score=accuracy_score,
            confidence_score=confidence_score,
            validation_info=validation_info,
            learning_suggestions=learning_suggestions,
            learning_session_id=learning_session_id,
            knowledge_alert=knowledge_alert,
            learning_proposal=learning_proposal,  # Learning proposal (if valuable knowledge detected)
            permission_request=permission_request,  # Permission request message
            timing=timing_logs,
            latency_metrics=latency_metrics_text,  # B·∫ÆT BU·ªòC HI·ªÇN TH·ªä LOG trong response cho frontend
            processing_steps=processing_steps,  # Real-time processing steps for status indicator
            epistemic_state=epistemic_state.value if epistemic_state else None  # Epistemic state: KNOWN/UNCERTAIN/UNKNOWN
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (they have proper status codes)
        raise
    except RateLimitExceeded:
        # Re-raise rate limit exceptions so global handler can return proper 429 status
        raise
    except Exception as e:
        # Log detailed error with context (without sensitive message content)
        logger.error(f"Chat error: {e}", exc_info=True)
        # Security: Don't log full message content (may contain sensitive info)
        # Only log message length and metadata
        logger.error(
            f"Request details: message_length={len(chat_request.message)}, "
            f"user_id={chat_request.user_id}, use_rag={chat_request.use_rag}"
        )
        
        # Check if it's a specific error we can handle
        error_str = str(e).lower()
        if "rag" in error_str and "not available" in error_str:
            raise HTTPException(status_code=503, detail="RAG system is not available. Please check backend initialization.")
        elif "embedding" in error_str or "model" in error_str:
            raise HTTPException(status_code=503, detail="Embedding service is not available. Please check backend logs.")
        else:
            raise HTTPException(
                status_code=500,
                detail=f"Chat error: {str(e)}. Please check backend logs for details."
            )


@router.post("/smart_router", response_model=ChatResponse)
async def chat_smart_router(request: Request, chat_request: ChatRequest):
    """
    Smart router that automatically selects the best chat endpoint.
    This is the main endpoint used by the dashboard.
    """
    try:
        # Use the RAG-enhanced chat endpoint as default
        return await chat_with_rag(request, chat_request)
    except HTTPException:
        # Re-raise HTTP exceptions (they have proper status codes)
        raise
    except RateLimitExceeded:
        # Re-raise rate limit exceptions so global handler can return proper 429 status
        raise
    except Exception as e:
        # Log detailed error for debugging
        logger.error(f"Smart router error: {e}", exc_info=True)
        # Return a more helpful error message
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}. Please check backend logs for details."
        )


# Legacy endpoints for backward compatibility
@router.post("/openai")
async def chat_openai(request: ChatRequest):
    """Legacy OpenAI chat endpoint"""
    # Create a dummy Request object for chat_with_rag
    # Note: This is a workaround - in production, we should refactor to not require Request
    class DummyRequest:
        pass
    dummy_request = DummyRequest()
    return await chat_with_rag(dummy_request, request)


@router.post("/deepseek")
async def chat_deepseek(request: ChatRequest):
    """Legacy DeepSeek chat endpoint"""
    # Create a dummy Request object for chat_with_rag
    class DummyRequest:
        pass
    dummy_request = DummyRequest()
    return await chat_with_rag(dummy_request, request)


@router.post("/ask", response_model=ChatResponse)
@limiter.limit(get_chat_rate_limit, key_func=get_rate_limit_key_func)  # Chat rate limit (dynamic based on API key)
async def ask_question(request: Request, chat_request: ChatRequest):
    """
    Simplified question-answering endpoint.
    Alias for /rag endpoint with RAG enabled by default.
    
    This endpoint is designed for simple Q&A use cases where RAG context
    is always desired. It's a convenience wrapper around the full RAG chat endpoint.
    """
    # Ensure RAG is enabled for /ask endpoint
    chat_request.use_rag = True
    # Use default context limit if not specified
    if chat_request.context_limit is None or chat_request.context_limit < 1:
        chat_request.context_limit = 3
    
    # Delegate to the main RAG chat endpoint
    return await chat_with_rag(request, chat_request)


@router.post("/validate")
@limiter.limit("20/minute", key_func=get_rate_limit_key_func)
async def validate_content(request: Request, chat_request: ChatRequest):
    """
    Standalone content validation endpoint.
    
    Validates user input/question for:
    - Ethical compliance
    - Content safety
    - Format validation
    
    Returns validation result without generating a response.
    This is useful for pre-validation before processing expensive RAG/LLM calls.
    """
    from backend.validators.chain import ValidatorChain
    from backend.validators.citation import CitationRequired
    from backend.validators.evidence_overlap import EvidenceOverlap
    from backend.validators.numeric import NumericUnitsBasic
    from backend.validators.ethics_adapter import EthicsAdapter
    
    try:
        # Get RAG retrieval for context (if needed for validation)
        rag_retrieval = get_rag_retrieval()
        
        # Get context if RAG is enabled
        context_docs = []
        if rag_retrieval and chat_request.use_rag:
            try:
                context = rag_retrieval.retrieve_context(
                    query=chat_request.message,
                    knowledge_limit=min(chat_request.context_limit, 3),  # Limit for validation
                    conversation_limit=0  # Don't need conversation for validation
                )
                context_docs = [
                    doc["content"] for doc in context.get("knowledge_docs", [])
                ]
            except Exception as context_error:
                logger.warning(f"Could not retrieve context for validation: {context_error}")
                context_docs = []
        
        # Create validator chain
        enable_validators = os.getenv("ENABLE_VALIDATORS", "false").lower() == "true"
        
        if enable_validators:
            from backend.services.ethics_guard import check_content_ethics
            chain = ValidatorChain([
                CitationRequired(),  # Not applicable for input, but included for completeness
                EvidenceOverlap(threshold=0.01),
                NumericUnitsBasic(),
                EthicsAdapter(guard_callable=check_content_ethics)  # Real ethics guard implementation
            ])
            
            # Validate the message itself (treating it as "answer" to check)
            # Note: This validates the user input, not a response
            validation_result = chain.run(chat_request.message, context_docs)
            
            # Record metrics
            try:
                from backend.validators.metrics import get_metrics
                metrics = get_metrics()
                metrics.record_validation(
                    passed=validation_result.passed,
                    reasons=validation_result.reasons,
                    overlap_score=0.0  # Not applicable for input validation
                )
            except Exception as metrics_error:
                logger.warning(f"Could not record validation metrics: {metrics_error}")
            
            return {
                "is_valid": validation_result.passed,
                "message": chat_request.message,
                "validation_details": {
                    "passed": validation_result.passed,
                    "reasons": validation_result.reasons,
                    "patched_content": validation_result.patched if hasattr(validation_result, 'patched') else None
                },
                "context_used": {
                    "context_docs_count": len(context_docs),
                    "rag_enabled": chat_request.use_rag
                }
            }
        else:
            # If validators are disabled, do basic validation only
            return {
                "is_valid": True,
                "message": chat_request.message,
                "validation_details": {
                    "passed": True,
                    "reasons": ["Validators disabled - basic format check passed"],
                    "note": "Full validation disabled via ENABLE_VALIDATORS=false"
                },
                "context_used": {
                    "context_docs_count": len(context_docs),
                    "rag_enabled": chat_request.use_rag
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in /validate endpoint: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Validation error: {str(e)}. Please check backend logs for details."
        )

