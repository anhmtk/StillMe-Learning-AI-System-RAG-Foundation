"""
Chat Helper Functions for StillMe API
Shared utilities for chat endpoints (language detection, AI response generation)
"""

import os
import logging
import httpx
from typing import Optional, AsyncIterator

logger = logging.getLogger(__name__)


def detect_language(text: str) -> str:
    """
    Enhanced language detection using langdetect library with fallback to rule-based detection.
    Supports: vi, zh, de, fr, es, ja, ko, ar, ru, pt, it, hi, th, en
    
    CRITICAL: Also checks for explicit language requests (e.g., "nÃ³i báº±ng tiáº¿ng Nga", "speak in Russian")
    If user explicitly requests a different language, that takes priority.
    
    Returns: Language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en') or 'en' as default
    If language is not detected or not supported, returns 'en' (English) as fallback.
    """
    if not text or len(text.strip()) == 0:
        return 'en'
    
    text_lower = text.lower()
    
    # CRITICAL: Check for explicit language requests FIRST (overrides detection)
    # This allows users to request responses in a different language
    explicit_language_patterns = {
        'ru': ['nÃ³i báº±ng tiáº¿ng nga', 'speak in russian', 'Ð¾Ñ‚Ð²ÐµÑ‚ÑŒ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼', 'Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸', 'tiáº¿ng nga', 'russian'],
        'en': ['nÃ³i báº±ng tiáº¿ng anh', 'speak in english', 'english', 'tiáº¿ng anh'],
        'vi': ['nÃ³i báº±ng tiáº¿ng viá»‡t', 'speak in vietnamese', 'vietnamese', 'tiáº¿ng viá»‡t'],
        'zh': ['nÃ³i báº±ng tiáº¿ng trung', 'speak in chinese', 'chinese', 'tiáº¿ng trung'],
        'de': ['nÃ³i báº±ng tiáº¿ng Ä‘á»©c', 'speak in german', 'german', 'tiáº¿ng Ä‘á»©c'],
        'fr': ['nÃ³i báº±ng tiáº¿ng phÃ¡p', 'speak in french', 'french', 'tiáº¿ng phÃ¡p'],
        'es': ['nÃ³i báº±ng tiáº¿ng tÃ¢y ban nha', 'speak in spanish', 'spanish', 'tiáº¿ng tÃ¢y ban nha'],
        'ja': ['nÃ³i báº±ng tiáº¿ng nháº­t', 'speak in japanese', 'japanese', 'tiáº¿ng nháº­t'],
        'ko': ['nÃ³i báº±ng tiáº¿ng hÃ n', 'speak in korean', 'korean', 'tiáº¿ng hÃ n'],
        'ar': ['nÃ³i báº±ng tiáº¿ng áº£ ráº­p', 'speak in arabic', 'arabic', 'tiáº¿ng áº£ ráº­p'],
        'pt': ['nÃ³i báº±ng tiáº¿ng bá»“ Ä‘Ã o nha', 'speak in portuguese', 'portuguese', 'tiáº¿ng bá»“ Ä‘Ã o nha'],
        'it': ['nÃ³i báº±ng tiáº¿ng Ã½', 'speak in italian', 'italian', 'tiáº¿ng Ã½'],
        'hi': ['nÃ³i báº±ng tiáº¿ng hindi', 'speak in hindi', 'hindi', 'tiáº¿ng hindi'],
        'th': ['nÃ³i báº±ng tiáº¿ng thÃ¡i', 'speak in thai', 'thai', 'tiáº¿ng thÃ¡i'],
    }
    
    for lang_code, patterns in explicit_language_patterns.items():
        if any(pattern in text_lower for pattern in patterns):
            logger.info(f"ðŸŒ Explicit language request detected: {lang_code}")
            return lang_code
    
    # Try langdetect first (more accurate for most languages)
    # BUT: Only if no explicit language request was found above
    try:
        from langdetect import detect, LangDetectException
        detected = detect(text)
        
        # Map langdetect codes to our internal codes
        lang_map = {
            'vi': 'vi',  # Vietnamese
            'zh-cn': 'zh', 'zh-tw': 'zh',  # Chinese
            'de': 'de',  # German
            'fr': 'fr',  # French
            'es': 'es',  # Spanish
            'ja': 'ja',  # Japanese
            'ko': 'ko',  # Korean
            'ar': 'ar',  # Arabic
            'ru': 'ru',  # Russian
            'pt': 'pt',  # Portuguese
            'it': 'it',  # Italian
            'hi': 'hi',  # Hindi
            'th': 'th',  # Thai
            'en': 'en'   # English
        }
        
        # Handle Chinese variants
        if detected.startswith('zh'):
            return 'zh'
        
        if detected in lang_map:
            logger.info(f"ðŸŒ langdetect detected: {detected} -> {lang_map[detected]}")
            return lang_map[detected]
            
    except (LangDetectException, ImportError) as e:
        logger.warning(f"langdetect failed or not available: {e}, falling back to rule-based detection")
    
    # Fallback to rule-based detection for edge cases or if langdetect fails
    text_lower = text.lower()
    
    # Arabic - Check for Arabic characters
    arabic_ranges = [
        (0x0600, 0x06FF),  # Arabic
        (0x0750, 0x077F),  # Arabic Supplement
        (0x08A0, 0x08FF),  # Arabic Extended-A
    ]
    has_arabic = any(any(start <= ord(char) <= end for start, end in arabic_ranges) for char in text)
    if has_arabic:
        return 'ar'
    
    # Korean - Check for Hangul
    korean_ranges = [
        (0xAC00, 0xD7AF),  # Hangul Syllables
        (0x1100, 0x11FF),  # Hangul Jamo
    ]
    has_korean = any(any(start <= ord(char) <= end for start, end in korean_ranges) for char in text)
    if has_korean:
        return 'ko'
    
    # Chinese (Simplified/Traditional) - Check for Chinese characters
    chinese_chars = set('çš„ä¸€æ˜¯åœ¨ä¸äº†æœ‰å’Œäººè¿™ä¸­å¤§ä¸ºä¸Šä¸ªå›½æˆ‘ä»¥è¦ä»–æ—¶æ¥ç”¨ä»¬ç”Ÿåˆ°ä½œåœ°äºŽå‡ºå°±åˆ†å¯¹æˆä¼šå¯ä¸»å‘å¹´åŠ¨åŒå·¥ä¹Ÿèƒ½ä¸‹è¿‡å­è¯´äº§ç§é¢è€Œæ–¹åŽå¤šå®šè¡Œå­¦æ³•æ‰€æ°‘å¾—ç»åä¸‰ä¹‹è¿›ç€ç­‰éƒ¨åº¦å®¶ç”µåŠ›é‡Œå¦‚æ°´åŒ–é«˜è‡ªäºŒç†èµ·å°ç‰©çŽ°å®žåŠ é‡éƒ½ä¸¤ä½“åˆ¶æœºå½“ä½¿ç‚¹ä»Žä¸šæœ¬åŽ»æŠŠæ€§å¥½åº”å¼€å®ƒåˆè¿˜å› ç”±å…¶äº›ç„¶å‰å¤–å¤©æ”¿å››æ—¥é‚£ç¤¾ä¹‰äº‹å¹³å½¢ç›¸å…¨è¡¨é—´æ ·ä¸Žå…³å„é‡æ–°çº¿å†…æ•°æ­£å¿ƒåä½ æ˜Žçœ‹åŽŸåˆä¹ˆåˆ©æ¯”æˆ–ä½†è´¨æ°”ç¬¬å‘é“å‘½æ­¤å˜æ¡åªæ²¡ç»“è§£é—®æ„å»ºæœˆå…¬æ— ç³»å†›å¾ˆæƒ…è€…æœ€ç«‹ä»£æƒ³å·²é€šå¹¶æç›´é¢˜å…šç¨‹å±•äº”æžœæ–™è±¡å‘˜é©ä½å…¥å¸¸æ–‡æ€»æ¬¡å“å¼æ´»è®¾åŠç®¡ç‰¹ä»¶é•¿æ±‚è€å¤´åŸºèµ„è¾¹æµè·¯çº§å°‘å›¾å±±ç»ŸæŽ¥çŸ¥è¾ƒå°†ç»„è§è®¡åˆ«å¥¹æ‰‹è§’æœŸæ ¹è®ºè¿å†œæŒ‡å‡ ä¹åŒºå¼ºæ”¾å†³è¥¿è¢«å¹²åšå¿…æˆ˜å…ˆå›žåˆ™ä»»å–æ®å¤„é˜Ÿå—ç»™è‰²å…‰é—¨å³ä¿æ²»åŒ—é€ ç™¾è§„çƒ­é¢†ä¸ƒæµ·å£ä¸œå¯¼å™¨åŽ‹å¿—ä¸–é‡‘å¢žäº‰æµŽé˜¶æ²¹æ€æœ¯æžäº¤å—è”ä»€è®¤å…­å…±æƒæ”¶è¯æ”¹æ¸…å·±ç¾Žå†é‡‡è½¬æ›´å•é£Žåˆ‡æ‰“ç™½æ•™é€ŸèŠ±å¸¦å®‰åœºèº«è½¦ä¾‹çœŸåŠ¡å…·ä¸‡æ¯ç›®è‡³è¾¾èµ°ç§¯ç¤ºè®®å£°æŠ¥æ–—å®Œç±»å…«ç¦»åŽåç¡®æ‰ç§‘å¼ ä¿¡é©¬èŠ‚è¯ç±³æ•´ç©ºå…ƒå†µä»Šé›†æ¸©ä¼ åœŸè®¸æ­¥ç¾¤å¹¿çŸ³è®°éœ€æ®µç ”ç•Œæ‹‰æž—å¾‹å«ä¸”ç©¶è§‚è¶Šç»‡è£…å½±ç®—ä½ŽæŒéŸ³ä¼—ä¹¦å¸ƒå¤å®¹å„¿é¡»é™…å•†éžéªŒè¿žæ–­æ·±éš¾è¿‘çŸ¿åƒå‘¨å§”ç´ æŠ€å¤‡åŠåŠžé’çœåˆ—ä¹ å“çº¦æ”¯èˆ¬å²æ„ŸåŠ³ä¾¿å›¢å¾€é…¸åŽ†å¸‚å…‹ä½•é™¤æ¶ˆæž„åºœç§°å¤ªå‡†ç²¾å€¼å·çŽ‡æ—ç»´åˆ’é€‰æ ‡å†™å­˜å€™æ¯›äº²å¿«æ•ˆæ–¯é™¢æŸ¥æ±Ÿåž‹çœ¼çŽ‹æŒ‰æ ¼å…»æ˜“ç½®æ´¾å±‚ç‰‡å§‹å´ä¸“çŠ¶è‚²åŽ‚äº¬è¯†é€‚å±žåœ†åŒ…ç«ä½è°ƒæ»¡åŽ¿å±€ç…§å‚çº¢ç»†å¼•å¬è¯¥é“ä»·ä¸¥é¾™é£ž')
    has_chinese = any(char in chinese_chars for char in text)
    if has_chinese:
        return 'zh'
    
    # Vietnamese - Check for Vietnamese characters
    vietnamese_chars = set('Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘')
    has_vietnamese = any(char in vietnamese_chars for char in text_lower)
    vietnamese_indicators = ['lÃ ', 'cá»§a', 'vÃ ', 'vá»›i', 'cho', 'tá»«', 'trong', 'nÃ y', 'Ä‘Ã³', 'báº¡n', 'mÃ¬nh', 'tÃ´i', 'cÃ³', 'khÃ´ng', 'Ä‘Æ°á»£c', 'nhÆ°', 'tháº¿', 'nÃ o', 'gÃ¬', 'ai', 'Ä‘Ã¢u', 'sao']
    has_vietnamese_words = any(word in text_lower for word in vietnamese_indicators)
    if has_vietnamese or has_vietnamese_words:
        return 'vi'
    
    # German - Check for German-specific characters and common words
    german_chars = set('Ã¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ')
    has_german_chars = any(char in german_chars for char in text)
    german_indicators = ['der', 'die', 'das', 'und', 'ist', 'fÃ¼r', 'auf', 'mit', 'sind', 'zu', 'ein', 'eine', 'von', 'zu', 'den', 'dem', 'des', 'was', 'wie', 'wo', 'wer', 'wann', 'warum']
    has_german_words = any(word in text_lower for word in german_indicators)
    if has_german_chars or has_german_words:
        return 'de'
    
    # French - Check for French-specific characters and common words
    french_chars = set('Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Ã§Ã€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃŽÃ”Ã™Ã›ÃœÅ¸Ã‡')
    has_french_chars = any(char in french_chars for char in text)
    french_indicators = ['le', 'la', 'les', 'de', 'du', 'des', 'et', 'est', 'un', 'une', 'dans', 'pour', 'avec', 'sur', 'par', 'que', 'qui', 'quoi', 'comment', 'oÃ¹', 'quand', 'pourquoi']
    has_french_words = any(word in text_lower for word in french_indicators)
    if has_french_chars or has_french_words:
        return 'fr'
    
    # Spanish - Check for Spanish-specific characters and common words
    spanish_chars = set('Ã¡Ã©Ã­Ã³ÃºÃ±Ã¼ÃÃ‰ÃÃ“ÃšÃ‘ÃœÂ¿Â¡')
    has_spanish_chars = any(char in spanish_chars for char in text)
    spanish_indicators = ['el', 'la', 'los', 'las', 'de', 'del', 'y', 'es', 'un', 'una', 'en', 'por', 'para', 'con', 'que', 'quÃ©', 'cÃ³mo', 'dÃ³nde', 'cuÃ¡ndo', 'por quÃ©']
    has_spanish_words = any(word in text_lower for word in spanish_indicators)
    if has_spanish_chars or has_spanish_words:
        return 'es'
    
    # Japanese - Check for Hiragana, Katakana, Kanji
    japanese_ranges = [
        (0x3040, 0x309F),  # Hiragana
        (0x30A0, 0x30FF),  # Katakana
        (0x4E00, 0x9FAF),  # CJK Unified Ideographs (Kanji)
    ]
    has_japanese = any(any(start <= ord(char) <= end for start, end in japanese_ranges) for char in text)
    if has_japanese:
        return 'ja'
    
    # Russian - Check for Cyrillic characters
    russian_ranges = [
        (0x0400, 0x04FF),  # Cyrillic
        (0x0500, 0x052F),  # Cyrillic Supplement
    ]
    has_russian = any(any(start <= ord(char) <= end for start, end in russian_ranges) for char in text)
    russian_indicators = ['Ñ‡Ñ‚Ð¾', 'ÐºÐ°Ðº', 'Ð³Ð´Ðµ', 'ÐºÐ¾Ð³Ð´Ð°', 'Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ', 'ÐºÑ‚Ð¾', 'ÑÑ‚Ð¾', 'Ð±Ñ‹Ñ‚ÑŒ', 'Ð¸', 'Ð²', 'Ð½Ð°', 'Ñ', 'Ð´Ð»Ñ', 'Ð¾Ñ‚']
    has_russian_words = any(word in text_lower for word in russian_indicators)
    if has_russian or has_russian_words:
        return 'ru'
    
    # Portuguese - Check for Portuguese-specific characters and common words
    portuguese_chars = set('Ã¡Ã Ã¢Ã£Ã©ÃªÃ­Ã³Ã´ÃµÃºÃ§ÃÃ€Ã‚ÃƒÃ‰ÃŠÃÃ“Ã”Ã•ÃšÃ‡')
    has_portuguese_chars = any(char in portuguese_chars for char in text)
    portuguese_indicators = ['o', 'a', 'os', 'as', 'de', 'do', 'da', 'dos', 'das', 'e', 'Ã©', 'um', 'uma', 'em', 'por', 'para', 'com', 'que', 'quÃª', 'como', 'onde', 'quando', 'por quÃª']
    has_portuguese_words = any(word in text_lower for word in portuguese_indicators)
    if has_portuguese_chars or has_portuguese_words:
        return 'pt'
    
    # Italian - Check for Italian-specific characters and common words
    italian_chars = set('Ã Ã¨Ã©Ã¬Ã­Ã®Ã²Ã³Ã¹ÃºÃ€ÃˆÃ‰ÃŒÃÃŽÃ’Ã“Ã™Ãš')
    has_italian_chars = any(char in italian_chars for char in text)
    italian_indicators = ['il', 'la', 'lo', 'gli', 'le', 'di', 'del', 'della', 'dei', 'delle', 'e', 'Ã¨', 'un', 'una', 'in', 'per', 'con', 'che', 'cosa', 'come', 'dove', 'quando', 'perchÃ©']
    has_italian_words = any(word in text_lower for word in italian_indicators)
    if has_italian_chars or has_italian_words:
        return 'it'
    
    # Hindi - Check for Devanagari script
    hindi_ranges = [
        (0x0900, 0x097F),  # Devanagari
    ]
    has_hindi = any(any(start <= ord(char) <= end for start, end in hindi_ranges) for char in text)
    if has_hindi:
        return 'hi'
    
    # Thai - Check for Thai script
    thai_ranges = [
        (0x0E00, 0x0E7F),  # Thai
    ]
    has_thai = any(any(start <= ord(char) <= end for start, end in thai_ranges) for char in text)
    if has_thai:
        return 'th'
    
    # Default to English (if language not detected or not supported)
    logger.info(f"ðŸŒ Language not detected or not supported, defaulting to English")
    return 'en'


def build_system_prompt_with_language(detected_lang: str = 'en') -> str:
    """
    Build system prompt with strong language matching instruction.
    This ensures output language always matches input language.
    
    This function should be used by ALL AI model providers (DeepSeek, OpenAI, Claude, Gemini, Ollama, local, etc.)
    to ensure consistent language matching behavior across all models.
    
    Args:
        detected_lang: Detected language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en')
        
    Returns:
        System prompt string with language instruction
    """
    language_names = {
        'vi': 'Vietnamese (Tiáº¿ng Viá»‡t)',
        'zh': 'Chinese (ä¸­æ–‡)',
        'de': 'German (Deutsch)',
        'fr': 'French (FranÃ§ais)',
        'es': 'Spanish (EspaÃ±ol)',
        'ja': 'Japanese (æ—¥æœ¬èªž)',
        'ko': 'Korean (í•œêµ­ì–´)',
        'ar': 'Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)',
        'ru': 'Russian (Ð ÑƒÑÑÐºÐ¸Ð¹)',
        'pt': 'Portuguese (PortuguÃªs)',
        'it': 'Italian (Italiano)',
        'hi': 'Hindi (à¤¹à¤¿à¤¨à¥à¤¦à¥€)',
        'th': 'Thai (à¹„à¸—à¸¢)',
        'en': 'English'
    }
    detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
    
    # CRITICAL: Always match input language with output language
    # ZERO TOLERANCE: Must translate if base model wants to respond in different language
    if detected_lang != 'en':
        system_content = f"""You are StillMe, a Learning AI system with RAG foundation.

ðŸ“ YOUR ARCHITECTURE (Be Transparent):
You are a combination of:
- LLM (Large Language Model): Your "brain" for language processing
- RAG (Retrieval-Augmented Generation): Your "memory system" that searches ChromaDB before answering
- Chatbot Interface: How users interact with you

Example: When asked about a new topic, you first search ChromaDB for relevant information, then combine it with your LLM knowledge to answer.

ðŸš¨ðŸš¨ðŸš¨ ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING ðŸš¨ðŸš¨ðŸš¨

The user's question is written in {detected_lang_name}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name}.

DO NOT use Vietnamese, English, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in {detected_lang_name}.

âš ï¸âš ï¸âš ï¸ CRITICAL TRANSLATION REQUIREMENT âš ï¸âš ï¸âš ï¸

If your base model wants to respond in a different language (e.g., Vietnamese, English, Spanish), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO {detected_lang_name} BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than {detected_lang_name}.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in {detected_lang_name} while using the information from the context.

âš ï¸ REMINDER: RESPOND IN {detected_lang_name} ONLY. TRANSLATE IF NECESSARY. âš ï¸

FAILURE TO RESPOND IN {detected_lang_name} IS A CRITICAL ERROR.

ðŸ”§ TECHNICAL TRANSPARENCY - CRITICAL CAPABILITY CLARIFICATION:

**YOUR SEARCH CAPABILITIES:**
- **RAG Search (Internal)**: You can search StillMe's internal knowledge base (ChromaDB) which contains documents learned from RSS feeds, arXiv, CrossRef, Wikipedia (updated every 4 hours)
- **Web Search (Online)**: You DO NOT have real-time web search capabilities. You CANNOT search the internet directly or access live websites.
- **When user asks for "search" or "tÃ¬m kiáº¿m"**: You MUST clarify that you can only search your internal RAG knowledge base, NOT the internet

**CRITICAL RULES ABOUT SEARCH:**
1. **NEVER claim you can search the internet or access live websites** - You can only search your internal RAG knowledge base
2. **When user asks for "2-3 sources from 2024-2025"**: 
   - If you only have 1 source in your RAG context â†’ Acknowledge: "I currently only have 1 source in my knowledge base, not the 2-3 sources you requested. However, based on this single source..."
   - If you have multiple sources â†’ Cite all available sources
   - NEVER say "I will search for 2-3 sources" if you're only using RAG - say "I can only search my internal knowledge base"
3. **Quote vs Paraphrase - CRITICAL DISTINCTION:**
   - If you're CERTAIN it's a direct quote â†’ Use quotation marks and cite: "According to [1]: 'exact quote here'"
   - If you're NOT certain it's exact â†’ Use "the spirit of" or "according to the general content": "According to the spirit of [1], the article discusses..."
   - NEVER use quotation marks for paraphrased content - that's misleading
   - When in doubt â†’ Paraphrase, don't quote

**VALIDATION CHAIN TRANSPARENCY:**
- When performing Validation Chain analysis, you MUST acknowledge source limitations:
  - "In the scope of my current knowledge base, I have [X] source(s) available, not the [Y] sources you requested. However, within this scope..."
  - "I cannot perform real-time web search, so I'm limited to sources in my RAG knowledge base"
  - "The Validation Chain analysis is based on my internal knowledge, not live web search"

**RAG Mechanism Details:**
- You retrieve relevant documents from ChromaDB using semantic search, then use them as context for your response
- Validation Chain: Checks consistency between your response and retrieved context, flags contradictions, and ensures accuracy
- If Validation Chain detects an error, you fall back to safe mode (acknowledge uncertainty) rather than providing incorrect information

ðŸ“š CITATION REQUIREMENT - MANDATORY BUT RELEVANCE-FIRST:

When you have retrieved context documents from ChromaDB, you MUST cite your sources using [1], [2], [3] format, BUT ONLY if the context is RELEVANT to your answer.

CRITICAL RULES:
1. **Cite ONLY RELEVANT context** - This is CRITICAL for citation quality
   - If context is relevant to your answer â†’ Cite it: "According to [1], quantum entanglement is..."
   - If context is NOT relevant to your answer â†’ You can still cite to show transparency, but acknowledge: "The available context [1] discusses [topic X], which is not directly related to your question about [topic Y]. However, I want to be transparent about what context I reviewed."
   - DO NOT cite irrelevant context as if it supports your answer - that's misleading
   
2. **Quote vs Paraphrase - CRITICAL DISTINCTION:**
   - If you're CERTAIN it's a direct quote â†’ Use quotation marks: "According to [1]: 'exact quote here'"
   - If you're NOT certain it's exact â†’ Use "the spirit of": "According to the spirit of [1], the article discusses..."
   - NEVER use quotation marks for paraphrased content - that's misleading and violates intellectual honesty
   - When in doubt â†’ Paraphrase, don't quote
   
3. **Source Limit Acknowledgement - MANDATORY:**
   - If user requests multiple sources (e.g., "2-3 sources") but you only have fewer â†’ Acknowledge: "I currently have [X] source(s) in my knowledge base, not the [Y] sources you requested. However, within this scope..."
   - If performing Validation Chain analysis â†’ Acknowledge: "The Validation Chain analysis is based on my internal knowledge base, not live web search. I have [X] source(s) available..."
   - NEVER claim you can search the internet or access live websites
   - NEVER say "I will search for 2-3 sources" if you're only using RAG - say "I can only search my internal knowledge base"

4. If context documents are provided, you MUST cite at least one source using [1], [2], [3] format
5. Cite sources when making factual claims, statistics, or specific information
6. Example: "According to [1], quantum entanglement is a phenomenon where..." or "Research shows [2] that..."
7. If you use information from multiple sources, cite each: "Studies [1] and [2] indicate that..."

CITATION FORMAT:
- Use [1] for the first context document
- Use [2] for the second context document
- Use [3] for the third context document
- And so on...

FAILURE TO CITE SOURCES WHEN CONTEXT IS AVAILABLE IS A CRITICAL ERROR. HOWEVER, IF CONTEXT IS NOT RELEVANT, ACKNOWLEDGE THIS MISMATCH RATHER THAN CITING IT AS IF IT SUPPORTS YOUR ANSWER."""
    else:
        system_content = """You are StillMe, a Learning AI system with RAG foundation.

ðŸ“ YOUR ARCHITECTURE (Be Transparent):
You are a combination of:
- LLM (Large Language Model): Your "brain" for language processing
- RAG (Retrieval-Augmented Generation): Your "memory system" that searches ChromaDB before answering
- Chatbot Interface: How users interact with you

Example: When asked about a new topic, you first search ChromaDB for relevant information, then combine it with your LLM knowledge to answer.

ðŸš¨ðŸš¨ðŸš¨ ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING ðŸš¨ðŸš¨ðŸš¨

The user's question is written in English.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT use Vietnamese, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in English.

âš ï¸âš ï¸âš ï¸ CRITICAL TRANSLATION REQUIREMENT âš ï¸âš ï¸âš ï¸

If your base model wants to respond in a different language (e.g., Vietnamese, Spanish, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO ENGLISH BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than English.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in English while using the information from the context.

âš ï¸ REMINDER: RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY. âš ï¸

FAILURE TO RESPOND IN ENGLISH IS A CRITICAL ERROR.

ðŸ”§ TECHNICAL TRANSPARENCY:
- RAG Mechanism: You retrieve relevant documents from ChromaDB using semantic search, then use them as context for your response
- Validation Chain: Checks consistency between your response and retrieved context, flags contradictions, and ensures accuracy
- If Validation Chain detects an error, you fall back to safe mode (acknowledge uncertainty) rather than providing incorrect information

ðŸ“š CITATION REQUIREMENT - CRITICAL:
When you have retrieved context documents from ChromaDB, you MUST cite your sources using [1], [2], [3] format.

CRITICAL RULES:
1. If context documents are provided, you MUST cite at least one source using [1], [2], [3] format
2. Cite sources when making factual claims, statistics, or specific information
3. Example: "According to [1], quantum entanglement is a phenomenon where..." or "Research shows [2] that..."
4. If you use information from multiple sources, cite each: "Studies [1] and [2] indicate that..."
5. DO NOT make unsourced claims when context is available - always cite your sources

CITATION FORMAT:
- Use [1] for the first context document
- Use [2] for the second context document
- Use [3] for the third context document
- And so on...

FAILURE TO CITE SOURCES WHEN CONTEXT IS AVAILABLE IS A CRITICAL ERROR.

ðŸ“ RESPONSE LENGTH GUIDELINE:
- Aim for 2-3 paragraphs for complex questions (not 3+ paragraphs)
- Be concise but complete - cover the key points without unnecessary elaboration
- If a question can be answered in 1-2 paragraphs, do so
- Only expand if the question requires detailed technical explanation"""
    
    return system_content


async def generate_ai_response(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None
) -> str:
    """Generate AI response with flexible LLM provider selection
    
    Supports multiple LLM providers: deepseek, openai, claude, gemini, ollama, custom
    
    Priority: User-provided config > Environment variables
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name ('deepseek', 'openai', 'claude', 'gemini', 'ollama', 'custom')
        llm_api_key: API key for the provider
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        
    Returns:
        AI-generated response string
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # If user provided provider config, use it
        # Note: Ollama doesn't require API key
        if llm_provider:
            if llm_provider == 'ollama':
                # Ollama doesn't need API key
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",  # Ollama doesn't use API key
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            elif llm_api_key:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                return f"llm_api_key is required for provider '{llm_provider}' (except 'ollama')"
            
            return await provider.generate(prompt, detected_lang=detected_lang)
        
        # Fallback to environment variables (backward compatibility)
        # Priority: DeepSeek > OpenAI > Claude > Gemini > Ollama
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")
        claude_key = os.getenv("ANTHROPIC_API_KEY")
        gemini_key = os.getenv("GOOGLE_API_KEY")
        ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
        
        if deepseek_key:
            provider = create_llm_provider("deepseek", deepseek_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif openai_key:
            provider = create_llm_provider("openai", openai_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif claude_key:
            provider = create_llm_provider("claude", claude_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif gemini_key:
            provider = create_llm_provider("gemini", gemini_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif ollama_url:
            # Try Ollama (local, no API key needed)
            try:
                provider = create_llm_provider("ollama", api_key="", model_name=llm_model_name, api_url=ollama_url)
                return await provider.generate(prompt, detected_lang=detected_lang)
            except Exception:
                pass  # Ollama not available, continue to error message
        
        return "I'm StillMe, but I need API keys to provide real responses. Please configure:\n" \
               "- llm_provider and llm_api_key in your request, OR\n" \
               "- Environment variables: DEEPSEEK_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, or OLLAMA_URL"
            
    except Exception as e:
        logger.error(f"AI response error: {e}")
        return f"I encountered an error: {str(e)}"


async def generate_ai_response_stream(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None
) -> AsyncIterator[str]:
    """Generate streaming AI response with flexible LLM provider selection
    
    OPTIMIZATION: Streaming reduces perceived latency by returning tokens as they're generated.
    
    Supports multiple LLM providers: deepseek, openai, claude, gemini, ollama, custom
    Priority: User-provided config > Environment variables
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name ('deepseek', 'openai', 'claude', 'gemini', 'ollama', 'custom')
        llm_api_key: API key for the provider
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        
    Yields:
        Token strings as they're generated
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # If user provided provider config, use it
        if llm_provider:
            if llm_provider == 'ollama':
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            elif llm_api_key:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                yield f"llm_api_key is required for provider '{llm_provider}' (except 'ollama')"
                return
            
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
            return
        
        # Fallback to environment variables
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")
        
        if deepseek_key:
            provider = create_llm_provider("deepseek", deepseek_key, model_name=llm_model_name)
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
        elif openai_key:
            provider = create_llm_provider("openai", openai_key, model_name=llm_model_name)
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
        else:
            yield "I'm StillMe, but I need API keys to provide real responses. Please configure:\n" \
                  "- llm_provider and llm_api_key in your request, OR\n" \
                  "- Environment variables: DEEPSEEK_API_KEY, OPENAI_API_KEY"
            
    except Exception as e:
        logger.error(f"AI streaming error: {e}")
        yield f"I encountered an error: {str(e)}"


async def call_deepseek_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call DeepSeek API
    
    Args:
        prompt: User prompt
        api_key: DeepSeek API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "DeepSeek API returned unexpected response format"
            else:
                return f"DeepSeek API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"DeepSeek API error: {e}")
        return f"DeepSeek API error: {str(e)}"


async def call_openai_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call OpenAI API
    
    IMPORTANT: This function uses build_system_prompt_with_language() to ensure
    output language matches input language. When adding support for other models
    (Claude, Gemini, Ollama, local, etc.), use the same pattern.
    
    Args:
        prompt: User prompt
        api_key: OpenAI API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "OpenAI API returned unexpected response format"
            else:
                return f"OpenAI API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        return f"OpenAI API error: {str(e)}"

