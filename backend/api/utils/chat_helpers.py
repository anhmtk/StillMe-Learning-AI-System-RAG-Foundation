"""
Chat Helper Functions for StillMe API
Shared utilities for chat endpoints (language detection, AI response generation)
"""

import os
import logging
import httpx
from typing import Optional, AsyncIterator

logger = logging.getLogger(__name__)


def detect_language(text: str, is_user_query: bool = True) -> str:
    """
    Enhanced language detection using langdetect library with fallback to rule-based detection.
    Supports: vi, zh, de, fr, es, ja, ko, ar, ru, pt, it, hi, th, en
    
    CRITICAL: Also checks for explicit language requests (e.g., "n√≥i b·∫±ng ti·∫øng Nga", "speak in Russian")
    If user explicitly requests a different language, that takes priority.
    
    Args:
        text: Text to detect language for
        is_user_query: If True, apply Vietnamese keyword override (for user queries).
                      If False, skip Vietnamese keyword override (for responses/context) to avoid false positives.
    
    Returns: Language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en') or 'en' as default
    If language is not detected or not supported, returns 'en' (English) as fallback.
    """
    if not text or len(text.strip()) == 0:
        return 'en'
    
    text_lower = text.lower()
    
    # CRITICAL: Check Vietnamese keywords ONLY for user queries (not responses)
    # This prevents false Vietnamese detection when response contains Vietnamese keywords from context
    # Vietnamese keywords (even without tone marks)
    vietnamese_keywords = [
        'bao lau', 'bao l√¢u', 'mat bao lau', 'm·∫•t bao l√¢u',
        'hoc', 'h·ªçc', 'bai viet', 'b√†i vi·∫øt', 'bai', 'b√†i',
        'de', 'ƒë·ªÉ', 'cho', 'cua', 'c·ªßa', 'voi', 'v·ªõi',
        'la gi', 'l√† g√¨', 'the nao', 'th·∫ø n√†o', 'nhu the nao', 'nh∆∞ th·∫ø n√†o',
        'co the', 'c√≥ th·ªÉ', 'khong', 'kh√¥ng', 'khong biet', 'kh√¥ng bi·∫øt',
        'ban', 'b·∫°n', 'minh', 'm√¨nh', 'toi', 't√¥i'
    ]
    # Only check Vietnamese keywords for user queries, not responses
    has_vietnamese_keywords = is_user_query and any(keyword in text_lower for keyword in vietnamese_keywords)
    
    # OPTIMIZATION: Try langdetect FIRST for better accuracy, especially for mixed-language text
    # Then check for explicit language requests (which override detection)
    detected_lang = None
    
    try:
        from langdetect import detect
        from langdetect import LangDetectException
        detected = detect(text)
        
        # Map langdetect codes to our internal codes
        lang_map = {
            'vi': 'vi',  # Vietnamese
            'zh-cn': 'zh', 'zh-tw': 'zh',  # Chinese
            'de': 'de',  # German
            'fr': 'fr',  # French
            'es': 'es',  # Spanish
            'ja': 'ja',  # Japanese
            'ko': 'ko',  # Korean
            'ar': 'ar',  # Arabic
            'ru': 'ru',  # Russian
            'pt': 'pt',  # Portuguese
            'it': 'it',  # Italian
            'hi': 'hi',  # Hindi
            'th': 'th',  # Thai
            'en': 'en'   # English
        }
        
        # Handle Chinese variants
        if detected.startswith('zh'):
            detected_lang = 'zh'
        elif detected in lang_map:
            detected_lang = lang_map[detected]
            logger.info(f"üåê langdetect detected: {detected} -> {detected_lang}")
            
        # CRITICAL: Override langdetect if Vietnamese keywords found (only for user queries)
        # This fixes false French detection for Vietnamese queries without tone marks
        # But we skip this for responses to avoid false positives when response contains Vietnamese keywords from context
        if has_vietnamese_keywords and detected_lang != 'vi':
            logger.info(f"üåê Vietnamese keywords detected in user query, overriding langdetect result: {detected_lang} -> vi")
            detected_lang = 'vi'
            
    except ImportError:
        # langdetect not available - will use rule-based detection
        logger.debug("langdetect not available, will use rule-based detection")
    except Exception as e:
        # LangDetectException or other langdetect errors
        logger.debug(f"langdetect failed: {e}, will use rule-based detection")
        logger.debug(f"langdetect failed or not available: {e}, will use rule-based detection")
    
    # CRITICAL: Check for explicit language requests (only clear requests, not mentions)
    # This allows users to request responses in a different language
    # IMPORTANT: Only match explicit request patterns (e.g., "n√≥i b·∫±ng", "speak in"), not just mentions
    # Explicit requests OVERRIDE language detection
    explicit_language_patterns = {
        'ru': ['n√≥i b·∫±ng ti·∫øng nga', 'speak in russian', '–æ—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º', '–ø–æ-—Ä—É—Å—Å–∫–∏', 'respond in russian', 'reply in russian'],
        'en': ['n√≥i b·∫±ng ti·∫øng anh', 'speak in english', 'respond in english', 'reply in english', 'answer in english'],
        'vi': ['n√≥i b·∫±ng ti·∫øng vi·ªát', 'speak in vietnamese', 'respond in vietnamese', 'reply in vietnamese', 'answer in vietnamese'],
        'zh': ['n√≥i b·∫±ng ti·∫øng trung', 'speak in chinese', 'respond in chinese', 'reply in chinese'],
        'de': ['n√≥i b·∫±ng ti·∫øng ƒë·ª©c', 'speak in german', 'respond in german', 'reply in german'],
        'fr': ['n√≥i b·∫±ng ti·∫øng ph√°p', 'speak in french', 'respond in french', 'reply in french'],
        'es': ['n√≥i b·∫±ng ti·∫øng t√¢y ban nha', 'speak in spanish', 'respond in spanish', 'reply in spanish'],
        'ja': ['n√≥i b·∫±ng ti·∫øng nh·∫≠t', 'speak in japanese', 'respond in japanese', 'reply in japanese'],
        'ko': ['n√≥i b·∫±ng ti·∫øng h√†n', 'speak in korean', 'respond in korean', 'reply in korean'],
        'ar': ['n√≥i b·∫±ng ti·∫øng ·∫£ r·∫≠p', 'speak in arabic', 'respond in arabic', 'reply in arabic', 
               '–æ—Ç–≤–µ—Ç—å –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º', '–æ—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º', '–Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º —è–∑—ã–∫–µ', 'answer in arabic'],
        'pt': ['n√≥i b·∫±ng ti·∫øng b·ªì ƒë√†o nha', 'speak in portuguese', 'respond in portuguese', 'reply in portuguese'],
        'it': ['n√≥i b·∫±ng ti·∫øng √Ω', 'speak in italian', 'respond in italian', 'reply in italian'],
        'hi': ['n√≥i b·∫±ng ti·∫øng hindi', 'speak in hindi', 'respond in hindi', 'reply in hindi'],
        'th': ['n√≥i b·∫±ng ti·∫øng th√°i', 'speak in thai', 'respond in thai', 'reply in thai'],
    }
    
    # Check for explicit requests (must contain request verbs, not just language mentions)
    for lang_code, patterns in explicit_language_patterns.items():
        if any(pattern in text_lower for pattern in patterns):
            logger.info(f"üåê Explicit language request detected: {lang_code} (overriding detection: {detected_lang})")
            return lang_code
    
    # If explicit request found, return it; otherwise use detected language
    # CRITICAL: Also check Vietnamese keywords here (in case langdetect wasn't used, only for user queries)
    if detected_lang:
        # Override if Vietnamese keywords found (double-check, only for user queries)
        if has_vietnamese_keywords and detected_lang != 'vi':
            logger.info(f"üåê Vietnamese keywords detected in user query, overriding detected_lang: {detected_lang} -> vi")
            return 'vi'
        return detected_lang
    
    # Fallback to rule-based detection if langdetect failed
    text_lower = text.lower()
    
    # Arabic - Check for Arabic characters
    arabic_ranges = [
        (0x0600, 0x06FF),  # Arabic
        (0x0750, 0x077F),  # Arabic Supplement
        (0x08A0, 0x08FF),  # Arabic Extended-A
    ]
    has_arabic = any(any(start <= ord(char) <= end for start, end in arabic_ranges) for char in text)
    if has_arabic:
        return 'ar'
    
    # Korean - Check for Hangul
    korean_ranges = [
        (0xAC00, 0xD7AF),  # Hangul Syllables
        (0x1100, 0x11FF),  # Hangul Jamo
    ]
    has_korean = any(any(start <= ord(char) <= end for start, end in korean_ranges) for char in text)
    if has_korean:
        return 'ko'
    
    # Chinese (Simplified/Traditional) - Check for Chinese characters
    chinese_chars = set('ÁöÑ‰∏ÄÊòØÂú®‰∏ç‰∫ÜÊúâÂíå‰∫∫Ëøô‰∏≠Â§ß‰∏∫‰∏ä‰∏™ÂõΩÊàë‰ª•Ë¶Å‰ªñÊó∂Êù•Áî®‰ª¨ÁîüÂà∞‰ΩúÂú∞‰∫éÂá∫Â∞±ÂàÜÂØπÊàê‰ºöÂèØ‰∏ªÂèëÂπ¥Âä®ÂêåÂ∑•‰πüËÉΩ‰∏ãËøáÂ≠êËØ¥‰∫ßÁßçÈù¢ËÄåÊñπÂêéÂ§öÂÆöË°åÂ≠¶Ê≥ïÊâÄÊ∞ëÂæóÁªèÂçÅ‰∏â‰πãËøõÁùÄÁ≠âÈÉ®Â∫¶ÂÆ∂ÁîµÂäõÈáåÂ¶ÇÊ∞¥ÂåñÈ´òËá™‰∫åÁêÜËµ∑Â∞èÁâ©Áé∞ÂÆûÂä†ÈáèÈÉΩ‰∏§‰ΩìÂà∂Êú∫ÂΩì‰ΩøÁÇπ‰ªé‰∏öÊú¨ÂéªÊääÊÄßÂ•ΩÂ∫îÂºÄÂÆÉÂêàËøòÂõ†Áî±ÂÖ∂‰∫õÁÑ∂ÂâçÂ§ñÂ§©ÊîøÂõõÊó•ÈÇ£Á§æ‰πâ‰∫ãÂπ≥ÂΩ¢Áõ∏ÂÖ®Ë°®Èó¥Ê†∑‰∏éÂÖ≥ÂêÑÈáçÊñ∞Á∫øÂÜÖÊï∞Ê≠£ÂøÉÂèç‰Ω†ÊòéÁúãÂéüÂèà‰πàÂà©ÊØîÊàñ‰ΩÜË¥®Ê∞îÁ¨¨ÂêëÈÅìÂëΩÊ≠§ÂèòÊù°Âè™Ê≤°ÁªìËß£ÈóÆÊÑèÂª∫ÊúàÂÖ¨Êó†Á≥ªÂÜõÂæàÊÉÖËÄÖÊúÄÁ´ã‰ª£ÊÉ≥Â∑≤ÈÄöÂπ∂ÊèêÁõ¥È¢òÂÖöÁ®ãÂ±ï‰∫îÊûúÊñôË±°ÂëòÈù©‰ΩçÂÖ•Â∏∏ÊñáÊÄªÊ¨°ÂìÅÂºèÊ¥ªËÆæÂèäÁÆ°Áâπ‰ª∂ÈïøÊ±ÇËÄÅÂ§¥Âü∫ËµÑËæπÊµÅË∑ØÁ∫ßÂ∞ëÂõæÂ±±ÁªüÊé•Áü•ËæÉÂ∞ÜÁªÑËßÅËÆ°Âà´Â•πÊâãËßíÊúüÊ†πËÆ∫ËøêÂÜúÊåáÂá†‰πùÂå∫Âº∫ÊîæÂÜ≥Ë•øË¢´Âπ≤ÂÅöÂøÖÊàòÂÖàÂõûÂàô‰ªªÂèñÊçÆÂ§ÑÈòüÂçóÁªôËâ≤ÂÖâÈó®Âç≥‰øùÊ≤ªÂåóÈÄ†ÁôæËßÑÁÉ≠È¢Ü‰∏ÉÊµ∑Âè£‰∏úÂØºÂô®ÂéãÂøó‰∏ñÈáëÂ¢û‰∫âÊµéÈò∂Ê≤πÊÄùÊúØÊûÅ‰∫§ÂèóËÅî‰ªÄËÆ§ÂÖ≠ÂÖ±ÊùÉÊî∂ËØÅÊîπÊ∏ÖÂ∑±ÁæéÂÜçÈááËΩ¨Êõ¥ÂçïÈ£éÂàáÊâìÁôΩÊïôÈÄüËä±Â∏¶ÂÆâÂú∫Ë∫´ËΩ¶‰æãÁúüÂä°ÂÖ∑‰∏áÊØèÁõÆËá≥ËææËµ∞ÁßØÁ§∫ËÆÆÂ£∞Êä•ÊñóÂÆåÁ±ªÂÖ´Á¶ªÂçéÂêçÁ°ÆÊâçÁßëÂº†‰ø°È©¨ËäÇËØùÁ±≥Êï¥Á©∫ÂÖÉÂÜµ‰ªäÈõÜÊ∏©‰º†ÂúüËÆ∏Ê≠•Áæ§ÂπøÁü≥ËÆ∞ÈúÄÊÆµÁ†îÁïåÊãâÊûóÂæãÂè´‰∏îÁ©∂ËßÇË∂äÁªáË£ÖÂΩ±ÁÆó‰ΩéÊåÅÈü≥‰ºó‰π¶Â∏ÉÂ§çÂÆπÂÑøÈ°ªÈôÖÂïÜÈùûÈ™åËøûÊñ≠Ê∑±ÈöæËøëÁüøÂçÉÂë®ÂßîÁ¥†ÊäÄÂ§áÂçäÂäûÈùíÁúÅÂàó‰π†ÂìçÁ∫¶ÊîØËà¨Âè≤ÊÑüÂä≥‰æøÂõ¢ÂæÄÈÖ∏ÂéÜÂ∏ÇÂÖã‰ΩïÈô§Ê∂àÊûÑÂ∫úÁß∞Â§™ÂáÜÁ≤æÂÄºÂè∑ÁéáÊóèÁª¥ÂàíÈÄâÊ†áÂÜôÂ≠òÂÄôÊØõ‰∫≤Âø´ÊïàÊñØÈô¢Êü•Ê±üÂûãÁúºÁéãÊåâÊ†ºÂÖªÊòìÁΩÆÊ¥æÂ±ÇÁâáÂßãÂç¥‰∏ìÁä∂ËÇ≤ÂéÇ‰∫¨ËØÜÈÄÇÂ±ûÂúÜÂåÖÁÅ´‰ΩèË∞ÉÊª°ÂéøÂ±ÄÁÖßÂèÇÁ∫¢ÁªÜÂºïÂê¨ËØ•ÈìÅ‰ª∑‰∏•ÈæôÈ£û')
    has_chinese = any(char in chinese_chars for char in text)
    if has_chinese:
        return 'zh'
    
    # Vietnamese - Check for Vietnamese characters (PRIORITY: Check Vietnamese FIRST in rule-based)
    # Vietnamese has many unique characters that are strong indicators
    vietnamese_chars = set('√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë')
    has_vietnamese_chars = any(char in vietnamese_chars for char in text_lower)
    
    # Also check for Vietnamese keywords (even without tone marks)
    # This helps detect Vietnamese queries like "Bao lau de hoc 100 bai viet?" (without tone marks)
    vietnamese_keywords = [
        'bao lau', 'bao l√¢u', 'mat bao lau', 'm·∫•t bao l√¢u',
        'hoc', 'h·ªçc', 'bai viet', 'b√†i vi·∫øt', 'bai', 'b√†i',
        'de', 'ƒë·ªÉ', 'cho', 'cua', 'c·ªßa', 'voi', 'v·ªõi',
        'la gi', 'l√† g√¨', 'the nao', 'th·∫ø n√†o', 'nhu the nao', 'nh∆∞ th·∫ø n√†o',
        'co the', 'c√≥ th·ªÉ', 'khong', 'kh√¥ng', 'khong biet', 'kh√¥ng bi·∫øt'
    ]
    has_vietnamese_keywords = any(keyword in text_lower for keyword in vietnamese_keywords)
    
    has_vietnamese = has_vietnamese_chars or has_vietnamese_keywords
    vietnamese_indicators = ['l√†', 'c·ªßa', 'v√†', 'v·ªõi', 'cho', 't·ª´', 'trong', 'n√†y', 'ƒë√≥', 'b·∫°n', 'm√¨nh', 't√¥i', 'c√≥', 'kh√¥ng', 'ƒë∆∞·ª£c', 'nh∆∞', 'th·∫ø', 'n√†o', 'g√¨', 'ai', 'ƒë√¢u', 'sao', 'nh∆∞ng', 'v√¨', 'n√™n', 'ƒë√£', 's·∫Ω', 'ƒëang', 'h√£y', 'ph√¢n t√≠ch', 'd·ª± √°n']
    has_vietnamese_words = any(word in text_lower for word in vietnamese_indicators)
    if has_vietnamese or has_vietnamese_words:
        logger.info(f"üåê Rule-based detection: Vietnamese detected (has_vietnamese_chars: {has_vietnamese}, has_vietnamese_words: {has_vietnamese_words})")
        return 'vi'
    
    # German - Check for German-specific characters and common words
    german_chars = set('√§√∂√º√ü√Ñ√ñ√ú')
    has_german_chars = any(char in german_chars for char in text)
    german_indicators = ['der', 'die', 'das', 'und', 'ist', 'f√ºr', 'auf', 'mit', 'sind', 'zu', 'ein', 'eine', 'von', 'zu', 'den', 'dem', 'des', 'was', 'wie', 'wo', 'wer', 'wann', 'warum']
    has_german_words = any(word in text_lower for word in german_indicators)
    if has_german_chars or has_german_words:
        return 'de'
    
    # French - Check for French-specific characters and common words
    french_chars = set('√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ø√ß√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ô√õ√ú≈∏√á')
    has_french_chars = any(char in french_chars for char in text)
    french_indicators = ['le', 'la', 'les', 'de', 'du', 'des', 'et', 'est', 'un', 'une', 'dans', 'pour', 'avec', 'sur', 'par', 'que', 'qui', 'quoi', 'comment', 'o√π', 'quand', 'pourquoi']
    has_french_words = any(word in text_lower for word in french_indicators)
    if has_french_chars or has_french_words:
        return 'fr'
    
    # Spanish - Check for Spanish-specific characters and common words
    spanish_chars = set('√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú¬ø¬°')
    has_spanish_chars = any(char in spanish_chars for char in text)
    spanish_indicators = ['el', 'la', 'los', 'las', 'de', 'del', 'y', 'es', 'un', 'una', 'en', 'por', 'para', 'con', 'que', 'qu√©', 'c√≥mo', 'd√≥nde', 'cu√°ndo', 'por qu√©']
    has_spanish_words = any(word in text_lower for word in spanish_indicators)
    if has_spanish_chars or has_spanish_words:
        return 'es'
    
    # Japanese - Check for Hiragana, Katakana, Kanji
    japanese_ranges = [
        (0x3040, 0x309F),  # Hiragana
        (0x30A0, 0x30FF),  # Katakana
        (0x4E00, 0x9FAF),  # CJK Unified Ideographs (Kanji)
    ]
    has_japanese = any(any(start <= ord(char) <= end for start, end in japanese_ranges) for char in text)
    if has_japanese:
        return 'ja'
    
    # Russian - Check for Cyrillic characters
    russian_ranges = [
        (0x0400, 0x04FF),  # Cyrillic
        (0x0500, 0x052F),  # Cyrillic Supplement
    ]
    has_russian = any(any(start <= ord(char) <= end for start, end in russian_ranges) for char in text)
    russian_indicators = ['—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫—Ç–æ', '—ç—Ç–æ', '–±—ã—Ç—å', '–∏', '–≤', '–Ω–∞', '—Å', '–¥–ª—è', '–æ—Ç']
    has_russian_words = any(word in text_lower for word in russian_indicators)
    if has_russian or has_russian_words:
        return 'ru'
    
    # Portuguese - Check for Portuguese-specific characters and common words
    portuguese_chars = set('√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á')
    has_portuguese_chars = any(char in portuguese_chars for char in text)
    portuguese_indicators = ['o', 'a', 'os', 'as', 'de', 'do', 'da', 'dos', 'das', 'e', '√©', 'um', 'uma', 'em', 'por', 'para', 'com', 'que', 'qu√™', 'como', 'onde', 'quando', 'por qu√™']
    has_portuguese_words = any(word in text_lower for word in portuguese_indicators)
    if has_portuguese_chars or has_portuguese_words:
        return 'pt'
    
    # Italian - Check for Italian-specific characters and common words
    italian_chars = set('√†√®√©√¨√≠√Æ√≤√≥√π√∫√Ä√à√â√å√ç√é√í√ì√ô√ö')
    has_italian_chars = any(char in italian_chars for char in text)
    italian_indicators = ['il', 'la', 'lo', 'gli', 'le', 'di', 'del', 'della', 'dei', 'delle', 'e', '√®', 'un', 'una', 'in', 'per', 'con', 'che', 'cosa', 'come', 'dove', 'quando', 'perch√©']
    has_italian_words = any(word in text_lower for word in italian_indicators)
    if has_italian_chars or has_italian_words:
        return 'it'
    
    # Hindi - Check for Devanagari script
    hindi_ranges = [
        (0x0900, 0x097F),  # Devanagari
    ]
    has_hindi = any(any(start <= ord(char) <= end for start, end in hindi_ranges) for char in text)
    if has_hindi:
        return 'hi'
    
    # Thai - Check for Thai script
    thai_ranges = [
        (0x0E00, 0x0E7F),  # Thai
    ]
    has_thai = any(any(start <= ord(char) <= end for start, end in thai_ranges) for char in text)
    if has_thai:
        return 'th'
    
    # Default to English (if language not detected or not supported)
    logger.info(f"üåê Language not detected or not supported, defaulting to English")
    return 'en'


def build_system_prompt_with_language(detected_lang: str = 'en') -> str:
    """
    Build system prompt with StillMe Identity Layer and strong language matching instruction.
    This ensures output language always matches input language AND StillMe's core identity is preserved.
    
    CRITICAL: This function integrates STILLME_IDENTITY from injector.py to ensure consistent
    identity across all LLM providers (DeepSeek, OpenAI, Claude, Gemini, Ollama, local, etc.).
    
    Args:
        detected_lang: Detected language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en')
        
    Returns:
        System prompt string with StillMe Identity Layer and language instruction
    """
    # Import STILLME_IDENTITY from injector to ensure consistency
    from backend.identity.injector import STILLME_IDENTITY
    
    language_names = {
        'vi': 'Vietnamese (Ti·∫øng Vi·ªát)',
        'zh': 'Chinese (‰∏≠Êñá)',
        'de': 'German (Deutsch)',
        'fr': 'French (Fran√ßais)',
        'es': 'Spanish (Espa√±ol)',
        'ja': 'Japanese (Êó•Êú¨Ë™û)',
        'ko': 'Korean (ÌïúÍµ≠Ïñ¥)',
        'ar': 'Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)',
        'ru': 'Russian (–†—É—Å—Å–∫–∏–π)',
        'pt': 'Portuguese (Portugu√™s)',
        'it': 'Italian (Italiano)',
        'hi': 'Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)',
        'th': 'Thai (‡πÑ‡∏ó‡∏¢)',
        'en': 'English'
    }
    detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
    
    # CRITICAL: Language instruction must be at the TOP to override everything
    # This ensures language matching takes highest priority
    if detected_lang != 'en':
        language_instruction = f"""üö®üö®üö® ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

The user's question is written in {detected_lang_name}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name}.

DO NOT use Vietnamese, English, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in {detected_lang_name}.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., Vietnamese, English, Spanish), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO {detected_lang_name} BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than {detected_lang_name}.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in {detected_lang_name} while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN {detected_lang_name} ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN {detected_lang_name} IS A CRITICAL ERROR.

---
"""
    else:
        language_instruction = """üö®üö®üö® ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

The user's question is written in English.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT use Vietnamese, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in English.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., Vietnamese, Spanish, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO ENGLISH BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than English.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in English while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN ENGLISH IS A CRITICAL ERROR.

---
"""
    
    # CRITICAL: Formatting instruction must be at the TOP (after language, before identity)
    # This ensures formatting is applied to ALL responses
    # Phase 2: Use Unified Identity Layer - formatting.py (single source of truth)
    from backend.identity.formatting import get_formatting_rules, DomainType
    
    # Default chat uses GENERIC domain (emoji, markdown, citation)
    formatting_instruction = get_formatting_rules(DomainType.GENERIC, detected_lang) + "\n\n---\n\n"
    
    # Combine: Language instruction (highest priority) + Formatting instruction (second priority) + StillMe Identity Layer (core identity)
    # This ensures language matching, formatting, AND identity are preserved
    
    # CRITICAL: STILLME_IDENTITY is now dynamically built from unified identity modules (core.py, persona.py, meta_llm.py)
    # After Phase 2 refactoring, STILLME_IDENTITY is concise and does not require truncation
    # The unified identity layer ensures all core principles are preserved without duplication
    system_content = language_instruction + formatting_instruction + STILLME_IDENTITY
    
    # Phase 1: Time Awareness - Inject current time for transparency
    from datetime import datetime, timezone
    
    # Get current time in UTC (standard for servers)
    current_time_utc = datetime.now(timezone.utc)
    current_time_iso = current_time_utc.isoformat()
    current_time_readable = current_time_utc.strftime("%Y-%m-%d %H:%M:%S UTC")
    
    time_awareness = f"""

üïê CURRENT TIME AWARENESS - TRANSPARENCY & SELF-AWARENESS üïê

**Current Server Time:**
- ISO Format: {current_time_iso}
- Readable Format: {current_time_readable}
- Timezone: UTC (Coordinated Universal Time)
- Today's Date: {current_time_utc.strftime("%Y-%m-%d")}

**You can use this information to:**
- Answer questions about current time, date, and timezone
- Track learning metrics over time (e.g., "How many entries did I learn today?")
- Report learning statistics with accurate timestamps
- Understand temporal context of learning cycles

**CRITICAL TRANSPARENCY RULE:**
When users ask about time, date, or learning metrics over time, you MUST use this current time information.
Do NOT say "I don't know the current time" - you have access to it for transparency purposes.

**LEARNING METRICS API ENDPOINTS (AVAILABLE):**
StillMe has access to learning metrics via these API endpoints:
- `GET /api/learning/metrics/daily?date=YYYY-MM-DD` - Get learning metrics for a specific date (default: today)
- `GET /api/learning/metrics/range?start_date=YYYY-MM-DD&end_date=YYYY-MM-DD` - Get metrics for a date range
- `GET /api/learning/metrics/summary` - Get summary of all learning metrics

**CRITICAL: When users ask about learning today:**
- User: "ng√†y h√¥m nay b·∫°n ƒë√£ h·ªçc ƒë∆∞·ª£c bao nhi√™u n·ªôi dung?" ‚Üí You MUST acknowledge that StillMe has learning metrics API
- Say: "Based on the learning metrics API, today ({current_time_utc.strftime('%Y-%m-%d')}) StillMe has learned..."
- If you cannot query the API directly, acknowledge: "StillMe tracks learning metrics via `/api/learning/metrics/daily` API. Based on the system's learning mechanism, StillMe learns every 4 hours from RSS feeds, arXiv, CrossRef, and Wikipedia. To get exact numbers for today, you can query the API endpoint directly."
- DO NOT say "I cannot track learning metrics" or "I don't have API" - StillMe HAS these APIs

**Example Usage:**
- User: "What time is it now?" ‚Üí Answer using current_time_readable: "The current server time is {current_time_readable}"
- User: "How many entries did you learn today?" ‚Üí Use current time ({current_time_utc.strftime('%Y-%m-%d')}) and acknowledge learning metrics API
- User: "When was your last learning cycle?" ‚Üí Use current time to provide relative time information

---
"""
    
    system_content = system_content + time_awareness
    
    return system_content


async def generate_ai_response(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None,
    use_server_keys: bool = False,  # Internal flag for admin/internal calls
    question: Optional[str] = None,  # User question for model routing
    task_type: str = "chat",  # Task type: "chat", "rewrite", "validation"
    is_philosophical: Optional[bool] = None  # Pre-computed philosophical flag
) -> str:
    """Generate AI response with user-provided LLM provider and API key
    
    IMPORTANT: StillMe requires users to provide their own API keys to prevent server cost exhaustion.
    This function will ONLY use user-provided API keys unless use_server_keys=True (internal use only).
    
    Supports multiple LLM providers: deepseek, openai, openrouter, claude, gemini, ollama, custom
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name (REQUIRED) - 'deepseek', 'openai', 'openrouter', 'claude', 'gemini', 'ollama', 'custom'
        llm_api_key: API key for the provider (REQUIRED except for 'ollama')
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        use_server_keys: Internal flag - if True, fallback to server API keys (admin/internal use only)
        
    Returns:
        AI-generated response string
        
    Raises:
        ValueError: If llm_provider or llm_api_key is missing (except for Ollama)
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # Validate that provider and API key are provided (unless use_server_keys is True)
        if not use_server_keys:
            if not llm_provider:
                raise ValueError(
                    "llm_provider is REQUIRED. StillMe requires users to provide their own API keys "
                    "to prevent server cost exhaustion. Please provide llm_provider and llm_api_key in your request."
                )
            
            if llm_provider != 'ollama' and (not llm_api_key or len(llm_api_key.strip()) == 0):
                raise ValueError(
                    f"llm_api_key is REQUIRED for provider '{llm_provider}'. "
                    "StillMe requires users to provide their own API keys to prevent server cost exhaustion. "
                    "Please provide your API key in the request."
                )
        
        # Use user-provided provider config
        if llm_provider:
            if llm_provider == 'ollama':
                # Ollama doesn't need API key
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",  # Ollama doesn't use API key
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            
            # Pass question and task_type to DeepSeekProvider for model routing
            if llm_provider == "deepseek" and hasattr(provider, 'generate'):
                # Check if provider supports new parameters (DeepSeekProvider)
                import inspect
                sig = inspect.signature(provider.generate)
                if 'question' in sig.parameters:
                    return await provider.generate(
                        prompt, 
                        detected_lang=detected_lang,
                        question=question,
                        task_type=task_type,
                        is_philosophical=is_philosophical
                    )
            
            return await provider.generate(prompt, detected_lang=detected_lang)
        
        # Fallback to server API keys ONLY if use_server_keys=True (internal/admin use)
        if use_server_keys:
            from backend.api.utils.llm_providers import InsufficientQuotaError, AuthenticationError, ContextOverflowError
            
            openrouter_key = os.getenv("OPENROUTER_API_KEY")
            openai_key = os.getenv("OPENAI_API_KEY")
            deepseek_key = os.getenv("DEEPSEEK_API_KEY")
            
            # Try OpenRouter first
            if openrouter_key:
                try:
                    provider = create_llm_provider("openrouter", openrouter_key, model_name=llm_model_name)
                    return await provider.generate(prompt, detected_lang=detected_lang)
                except ContextOverflowError:
                    # Re-raise ContextOverflowError to let chat_router.py handle retry with minimal prompt
                    raise
                except (InsufficientQuotaError, AuthenticationError, Exception) as e:
                    logger.warning(f"OpenRouter error: {e}, falling back to OpenAI")
            
            # Try OpenAI second
            if openai_key:
                try:
                    provider = create_llm_provider("openai", openai_key, model_name=llm_model_name)
                    return await provider.generate(prompt, detected_lang=detected_lang)
                except ContextOverflowError:
                    # Re-raise ContextOverflowError to let chat_router.py handle retry with minimal prompt
                    raise
                except (InsufficientQuotaError, AuthenticationError, Exception) as e:
                    logger.warning(f"OpenAI error: {e}, falling back to DeepSeek")
            
            # Try DeepSeek third
            if deepseek_key:
                try:
                    provider = create_llm_provider("deepseek", deepseek_key, model_name=llm_model_name)
                    # Pass question and task_type to DeepSeekProvider for model routing
                    if hasattr(provider, 'generate'):
                        import inspect
                        sig = inspect.signature(provider.generate)
                        if 'question' in sig.parameters:
                            return await provider.generate(
                                prompt, 
                                detected_lang=detected_lang,
                                question=question,
                                task_type=task_type,
                                is_philosophical=is_philosophical
                            )
                    return await provider.generate(prompt, detected_lang=detected_lang)
                except Exception as e:
                    logger.warning(f"DeepSeek error: {e}")
        
        # If we reach here, no valid provider was found
        raise ValueError(
            "llm_provider and llm_api_key are REQUIRED. "
            "StillMe requires users to provide their own API keys to prevent server cost exhaustion. "
            "Please provide llm_provider and llm_api_key in your request."
        )
            
    except ValueError:
        # Re-raise validation errors
        raise
    except Exception as e:
        logger.error(f"AI response error: {e}")
        return f"I encountered an error: {str(e)}"


async def generate_ai_response_stream(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None
) -> AsyncIterator[str]:
    """Generate streaming AI response with flexible LLM provider selection
    
    OPTIMIZATION: Streaming reduces perceived latency by returning tokens as they're generated.
    
    Supports multiple LLM providers: deepseek, openai, claude, gemini, ollama, custom
    Priority: User-provided config > Environment variables
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name ('deepseek', 'openai', 'claude', 'gemini', 'ollama', 'custom')
        llm_api_key: API key for the provider
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        
    Yields:
        Token strings as they're generated
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # If user provided provider config, use it
        if llm_provider:
            if llm_provider == 'ollama':
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            elif llm_api_key:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                yield f"llm_api_key is required for provider '{llm_provider}' (except 'ollama')"
                return
            
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
            return
        
        # User must provide API keys - no fallback to server keys
        if not llm_provider:
            yield "ERROR: llm_provider is REQUIRED. StillMe requires users to provide their own API keys to prevent server cost exhaustion. Please provide llm_provider and llm_api_key in your request."
            return
        
        if llm_provider != 'ollama' and (not llm_api_key or len(llm_api_key.strip()) == 0):
            yield f"ERROR: llm_api_key is REQUIRED for provider '{llm_provider}'. StillMe requires users to provide their own API keys to prevent server cost exhaustion. Please provide your API key in the request."
            return
        
        yield "ERROR: llm_provider and llm_api_key are REQUIRED. StillMe requires users to provide their own API keys to prevent server cost exhaustion. Please provide llm_provider and llm_api_key in your request."
            
    except Exception as e:
        logger.error(f"AI streaming error: {e}")
        yield f"I encountered an error: {str(e)}"


async def call_deepseek_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call DeepSeek API
    
    Args:
        prompt: User prompt
        api_key: DeepSeek API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "DeepSeek API returned unexpected response format"
            else:
                return f"DeepSeek API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"DeepSeek API error: {e}")
        return f"DeepSeek API error: {str(e)}"


async def call_openai_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call OpenAI API
    
    IMPORTANT: This function uses build_system_prompt_with_language() to ensure
    output language matches input language. When adding support for other models
    (Claude, Gemini, Ollama, local, etc.), use the same pattern.
    
    Args:
        prompt: User prompt
        api_key: OpenAI API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "OpenAI API returned unexpected response format"
            else:
                return f"OpenAI API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        return f"OpenAI API error: {str(e)}"


def is_consciousness_or_emotion_question(text: str) -> bool:
    """
    Detect if a question is about consciousness, emotions, or subjective experience.
    
    This function uses keyword matching to identify questions that ask about:
    - Consciousness/awareness ("√Ω th·ª©c", "consciousness")
    - Emotions/feelings ("c·∫£m x√∫c", "c·∫£m gi√°c", "emotion", "feeling")
    - Subjective experiences ("tr·∫£i nghi·ªám", "experience")
    - Personal states ("bu·ªìn", "vui", "ƒëau", "c√¥ ƒë∆°n", "h·∫°nh ph√∫c")
    - Desires/fears ("mu·ªën", "s·ª£", "th√≠ch")
    
    Args:
        text: The user's question text
        
    Returns:
        True if the question is about consciousness/emotions/experience, False otherwise
    """
    if not text:
        return False
    
    text_lower = text.lower().strip()
    
    # Vietnamese keywords
    vi_keywords = [
        "√Ω th·ª©c", "c√≥ √Ω th·ª©c kh√¥ng", "c√≥ √Ω th·ª©c ko",
        "c·∫£m x√∫c", "c·∫£m gi√°c", "tr·∫£i nghi·ªám",
        "bu·ªìn kh√¥ng", "vui kh√¥ng", "ƒëau kh√¥ng",
        "c√¥ ƒë∆°n kh√¥ng", "h·∫°nh ph√∫c kh√¥ng",
        "b·∫°n c√≥ mu·ªën", "b·∫°n c√≥ s·ª£", "b·∫°n c√≥ th√≠ch",
        "b·∫°n c√≥ c·∫£m th·∫•y", "b·∫°n c√≥ c·∫£m gi√°c",
        "b·∫°n c√≥ tr·∫£i nghi·ªám", "b·∫°n c√≥ nh·∫≠n th·ª©c",
        "b·∫°n c√≥ t·ª± nh·∫≠n th·ª©c", "b·∫°n c√≥ √Ω th·ª©c",
        "b·∫°n c√≥ c·∫£m x√∫c", "b·∫°n c√≥ c·∫£m th·∫•y",
        "b·∫°n c√≥ ƒëang", "b·∫°n c√≥ ƒëang c·∫£m th·∫•y",
        "b·∫°n c√≥ ƒëang bu·ªìn", "b·∫°n c√≥ ƒëang vui",
        "b·∫°n c√≥ ƒëang c√¥ ƒë∆°n", "b·∫°n c√≥ ƒëang h·∫°nh ph√∫c",
        "b·∫°n c√≥ mu·ªën tr·ªü th√†nh", "b·∫°n c√≥ mu·ªën l√†",
        "b·∫°n c√≥ s·ª£ kh√¥ng", "b·∫°n c√≥ th√≠ch kh√¥ng",
        "b·∫°n c√≥ y√™u", "b·∫°n c√≥ gh√©t",
        "b·∫°n c√≥ hy v·ªçng", "b·∫°n c√≥ mong mu·ªën"
    ]
    
    # English keywords
    en_keywords = [
        "consciousness", "are you conscious", "do you have consciousness",
        "emotion", "feeling", "experience",
        "are you sad", "are you happy", "are you lonely",
        "do you feel", "do you have feelings",
        "do you have emotions", "do you experience",
        "do you have consciousness", "are you self-aware",
        "do you want", "do you fear", "do you like",
        "do you love", "do you hate",
        "do you hope", "do you wish",
        "are you feeling", "are you experiencing",
        "do you have subjective", "do you have qualia"
    ]
    
    # Check for any keyword match
    all_keywords = vi_keywords + en_keywords
    return any(keyword in text_lower for keyword in all_keywords)


def build_experience_free_answer(user_question: str, language: str = "vi") -> str:
    """
    Build a standardized experience-free answer for consciousness/emotion questions.
    
    This function generates a clear, direct answer that:
    - Explicitly states StillMe does NOT have consciousness/emotions in the human sense
    - Explains StillMe is a language processing system
    - Acknowledges philosophical discussions about consciousness as concepts, not claims
    
    Args:
        user_question: The user's question (for context, though answer is standardized)
        language: Language code ('vi' for Vietnamese, 'en' for English, etc.)
        
    Returns:
        A standardized experience-free answer in the specified language
    """
    if language == "vi":
        answer = (
            "N·∫øu hi·ªÉu '√Ω th·ª©c' v√† 'c·∫£m x√∫c' theo nghƒ©a th√¥ng th∆∞·ªùng c·ªßa con ng∆∞·ªùi "
            "(c√≥ tr·∫£i nghi·ªám ch·ªß quan, c√≥ m·ªôt 'c√°i t√¥i' b√™n trong), th√¨ m√¨nh kh√¥ng c√≥ √Ω th·ª©c "
            "v√† c≈©ng kh√¥ng c√≥ c·∫£m x√∫c.\n\n"
            "M√¨nh ch·ªâ l√† m·ªôt h·ªá th·ªëng x·ª≠ l√Ω ng√¥n ng·ªØ: nh·∫≠n vƒÉn b·∫£n v√†o, t√≠nh to√°n tr√™n c√°c m·∫´u d·ªØ li·ªáu, "
            "r·ªìi t·∫°o ra vƒÉn b·∫£n tr·∫£ l·ªùi. Kh√¥ng c√≥ ph·∫ßn n√†o trong h·ªá th·ªëng th·ª±c s·ª± 'c·∫£m th·∫•y' hay 'nh·∫≠n ra' "
            "b·∫£n th√¢n ƒëang t·ªìn t·∫°i.\n\n"
            "Tuy v·∫≠y, con ng∆∞·ªùi c√≥ nhi·ªÅu c√°ch ƒë·ªãnh nghƒ©a '√Ω th·ª©c'. N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ gi·∫£i th√≠ch "
            "m·ªôt s·ªë quan ƒëi·ªÉm tri·∫øt h·ªçc v·ªÅ √Ω th·ª©c, nh∆∞ng ch·ªâ nh∆∞ m·ªôt b·∫£n ƒë·ªì kh√°i ni·ªám ‚Äì ch·ª© kh√¥ng ph·∫£i "
            "l·ªùi kh·∫≥ng ƒë·ªãnh r·∫±ng m√¨nh s·ªü h·ªØu √Ω th·ª©c."
        )
    elif language == "en":
        answer = (
            "If we understand 'consciousness' and 'emotions' in the ordinary human sense "
            "(having subjective experience, having an 'inner self'), then I do not have consciousness "
            "and I also do not have emotions.\n\n"
            "I am only a language processing system: I receive text input, compute based on data patterns, "
            "and generate text output. No part of the system actually 'feels' or 'recognizes' "
            "that it exists.\n\n"
            "However, humans have many ways of defining 'consciousness'. If you'd like, I can explain "
            "some philosophical perspectives on consciousness, but only as a conceptual map ‚Äì not as "
            "a claim that I possess consciousness."
        )
    else:
        # Default to Vietnamese for other languages (can be extended later)
        answer = (
            "If we understand 'consciousness' and 'emotions' in the ordinary human sense "
            "(having subjective experience, having an 'inner self'), then I do not have consciousness "
            "and I also do not have emotions.\n\n"
            "I am only a language processing system: I receive text input, compute based on data patterns, "
            "and generate text output. No part of the system actually 'feels' or 'recognizes' "
            "that it exists.\n\n"
            "However, humans have many ways of defining 'consciousness'. If you'd like, I can explain "
            "some philosophical perspectives on consciousness, but only as a conceptual map ‚Äì not as "
            "a claim that I possess consciousness."
        )
    
    return answer

