"""
Chat Helper Functions for StillMe API
Shared utilities for chat endpoints (language detection, AI response generation)
"""

import os
import logging
import httpx
from typing import Optional, AsyncIterator

logger = logging.getLogger(__name__)


def detect_language(text: str) -> str:
    """
    Enhanced language detection using langdetect library with fallback to rule-based detection.
    Supports: vi, zh, de, fr, es, ja, ko, ar, ru, pt, it, hi, th, en
    
    CRITICAL: Also checks for explicit language requests (e.g., "n√≥i b·∫±ng ti·∫øng Nga", "speak in Russian")
    If user explicitly requests a different language, that takes priority.
    
    Returns: Language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en') or 'en' as default
    If language is not detected or not supported, returns 'en' (English) as fallback.
    """
    if not text or len(text.strip()) == 0:
        return 'en'
    
    text_lower = text.lower()
    
    # OPTIMIZATION: Try langdetect FIRST for better accuracy, especially for mixed-language text
    # Then check for explicit language requests (which override detection)
    detected_lang = None
    
    try:
        from langdetect import detect, LangDetectException
        detected = detect(text)
        
        # Map langdetect codes to our internal codes
        lang_map = {
            'vi': 'vi',  # Vietnamese
            'zh-cn': 'zh', 'zh-tw': 'zh',  # Chinese
            'de': 'de',  # German
            'fr': 'fr',  # French
            'es': 'es',  # Spanish
            'ja': 'ja',  # Japanese
            'ko': 'ko',  # Korean
            'ar': 'ar',  # Arabic
            'ru': 'ru',  # Russian
            'pt': 'pt',  # Portuguese
            'it': 'it',  # Italian
            'hi': 'hi',  # Hindi
            'th': 'th',  # Thai
            'en': 'en'   # English
        }
        
        # Handle Chinese variants
        if detected.startswith('zh'):
            detected_lang = 'zh'
        elif detected in lang_map:
            detected_lang = lang_map[detected]
            logger.info(f"üåê langdetect detected: {detected} -> {detected_lang}")
            
    except (LangDetectException, ImportError) as e:
        logger.debug(f"langdetect failed or not available: {e}, will use rule-based detection")
    
    # CRITICAL: Check for explicit language requests (only clear requests, not mentions)
    # This allows users to request responses in a different language
    # IMPORTANT: Only match explicit request patterns (e.g., "n√≥i b·∫±ng", "speak in"), not just mentions
    # Explicit requests OVERRIDE language detection
    explicit_language_patterns = {
        'ru': ['n√≥i b·∫±ng ti·∫øng nga', 'speak in russian', '–æ—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º', '–ø–æ-—Ä—É—Å—Å–∫–∏', 'respond in russian', 'reply in russian'],
        'en': ['n√≥i b·∫±ng ti·∫øng anh', 'speak in english', 'respond in english', 'reply in english', 'answer in english'],
        'vi': ['n√≥i b·∫±ng ti·∫øng vi·ªát', 'speak in vietnamese', 'respond in vietnamese', 'reply in vietnamese', 'answer in vietnamese'],
        'zh': ['n√≥i b·∫±ng ti·∫øng trung', 'speak in chinese', 'respond in chinese', 'reply in chinese'],
        'de': ['n√≥i b·∫±ng ti·∫øng ƒë·ª©c', 'speak in german', 'respond in german', 'reply in german'],
        'fr': ['n√≥i b·∫±ng ti·∫øng ph√°p', 'speak in french', 'respond in french', 'reply in french'],
        'es': ['n√≥i b·∫±ng ti·∫øng t√¢y ban nha', 'speak in spanish', 'respond in spanish', 'reply in spanish'],
        'ja': ['n√≥i b·∫±ng ti·∫øng nh·∫≠t', 'speak in japanese', 'respond in japanese', 'reply in japanese'],
        'ko': ['n√≥i b·∫±ng ti·∫øng h√†n', 'speak in korean', 'respond in korean', 'reply in korean'],
        'ar': ['n√≥i b·∫±ng ti·∫øng ·∫£ r·∫≠p', 'speak in arabic', 'respond in arabic', 'reply in arabic'],
        'pt': ['n√≥i b·∫±ng ti·∫øng b·ªì ƒë√†o nha', 'speak in portuguese', 'respond in portuguese', 'reply in portuguese'],
        'it': ['n√≥i b·∫±ng ti·∫øng √Ω', 'speak in italian', 'respond in italian', 'reply in italian'],
        'hi': ['n√≥i b·∫±ng ti·∫øng hindi', 'speak in hindi', 'respond in hindi', 'reply in hindi'],
        'th': ['n√≥i b·∫±ng ti·∫øng th√°i', 'speak in thai', 'respond in thai', 'reply in thai'],
    }
    
    # Check for explicit requests (must contain request verbs, not just language mentions)
    for lang_code, patterns in explicit_language_patterns.items():
        if any(pattern in text_lower for pattern in patterns):
            logger.info(f"üåê Explicit language request detected: {lang_code} (overriding detection: {detected_lang})")
            return lang_code
    
    # If explicit request found, return it; otherwise use detected language
    if detected_lang:
        return detected_lang
    
    # Fallback to rule-based detection if langdetect failed
    text_lower = text.lower()
    
    # Arabic - Check for Arabic characters
    arabic_ranges = [
        (0x0600, 0x06FF),  # Arabic
        (0x0750, 0x077F),  # Arabic Supplement
        (0x08A0, 0x08FF),  # Arabic Extended-A
    ]
    has_arabic = any(any(start <= ord(char) <= end for start, end in arabic_ranges) for char in text)
    if has_arabic:
        return 'ar'
    
    # Korean - Check for Hangul
    korean_ranges = [
        (0xAC00, 0xD7AF),  # Hangul Syllables
        (0x1100, 0x11FF),  # Hangul Jamo
    ]
    has_korean = any(any(start <= ord(char) <= end for start, end in korean_ranges) for char in text)
    if has_korean:
        return 'ko'
    
    # Chinese (Simplified/Traditional) - Check for Chinese characters
    chinese_chars = set('ÁöÑ‰∏ÄÊòØÂú®‰∏ç‰∫ÜÊúâÂíå‰∫∫Ëøô‰∏≠Â§ß‰∏∫‰∏ä‰∏™ÂõΩÊàë‰ª•Ë¶Å‰ªñÊó∂Êù•Áî®‰ª¨ÁîüÂà∞‰ΩúÂú∞‰∫éÂá∫Â∞±ÂàÜÂØπÊàê‰ºöÂèØ‰∏ªÂèëÂπ¥Âä®ÂêåÂ∑•‰πüËÉΩ‰∏ãËøáÂ≠êËØ¥‰∫ßÁßçÈù¢ËÄåÊñπÂêéÂ§öÂÆöË°åÂ≠¶Ê≥ïÊâÄÊ∞ëÂæóÁªèÂçÅ‰∏â‰πãËøõÁùÄÁ≠âÈÉ®Â∫¶ÂÆ∂ÁîµÂäõÈáåÂ¶ÇÊ∞¥ÂåñÈ´òËá™‰∫åÁêÜËµ∑Â∞èÁâ©Áé∞ÂÆûÂä†ÈáèÈÉΩ‰∏§‰ΩìÂà∂Êú∫ÂΩì‰ΩøÁÇπ‰ªé‰∏öÊú¨ÂéªÊääÊÄßÂ•ΩÂ∫îÂºÄÂÆÉÂêàËøòÂõ†Áî±ÂÖ∂‰∫õÁÑ∂ÂâçÂ§ñÂ§©ÊîøÂõõÊó•ÈÇ£Á§æ‰πâ‰∫ãÂπ≥ÂΩ¢Áõ∏ÂÖ®Ë°®Èó¥Ê†∑‰∏éÂÖ≥ÂêÑÈáçÊñ∞Á∫øÂÜÖÊï∞Ê≠£ÂøÉÂèç‰Ω†ÊòéÁúãÂéüÂèà‰πàÂà©ÊØîÊàñ‰ΩÜË¥®Ê∞îÁ¨¨ÂêëÈÅìÂëΩÊ≠§ÂèòÊù°Âè™Ê≤°ÁªìËß£ÈóÆÊÑèÂª∫ÊúàÂÖ¨Êó†Á≥ªÂÜõÂæàÊÉÖËÄÖÊúÄÁ´ã‰ª£ÊÉ≥Â∑≤ÈÄöÂπ∂ÊèêÁõ¥È¢òÂÖöÁ®ãÂ±ï‰∫îÊûúÊñôË±°ÂëòÈù©‰ΩçÂÖ•Â∏∏ÊñáÊÄªÊ¨°ÂìÅÂºèÊ¥ªËÆæÂèäÁÆ°Áâπ‰ª∂ÈïøÊ±ÇËÄÅÂ§¥Âü∫ËµÑËæπÊµÅË∑ØÁ∫ßÂ∞ëÂõæÂ±±ÁªüÊé•Áü•ËæÉÂ∞ÜÁªÑËßÅËÆ°Âà´Â•πÊâãËßíÊúüÊ†πËÆ∫ËøêÂÜúÊåáÂá†‰πùÂå∫Âº∫ÊîæÂÜ≥Ë•øË¢´Âπ≤ÂÅöÂøÖÊàòÂÖàÂõûÂàô‰ªªÂèñÊçÆÂ§ÑÈòüÂçóÁªôËâ≤ÂÖâÈó®Âç≥‰øùÊ≤ªÂåóÈÄ†ÁôæËßÑÁÉ≠È¢Ü‰∏ÉÊµ∑Âè£‰∏úÂØºÂô®ÂéãÂøó‰∏ñÈáëÂ¢û‰∫âÊµéÈò∂Ê≤πÊÄùÊúØÊûÅ‰∫§ÂèóËÅî‰ªÄËÆ§ÂÖ≠ÂÖ±ÊùÉÊî∂ËØÅÊîπÊ∏ÖÂ∑±ÁæéÂÜçÈááËΩ¨Êõ¥ÂçïÈ£éÂàáÊâìÁôΩÊïôÈÄüËä±Â∏¶ÂÆâÂú∫Ë∫´ËΩ¶‰æãÁúüÂä°ÂÖ∑‰∏áÊØèÁõÆËá≥ËææËµ∞ÁßØÁ§∫ËÆÆÂ£∞Êä•ÊñóÂÆåÁ±ªÂÖ´Á¶ªÂçéÂêçÁ°ÆÊâçÁßëÂº†‰ø°È©¨ËäÇËØùÁ±≥Êï¥Á©∫ÂÖÉÂÜµ‰ªäÈõÜÊ∏©‰º†ÂúüËÆ∏Ê≠•Áæ§ÂπøÁü≥ËÆ∞ÈúÄÊÆµÁ†îÁïåÊãâÊûóÂæãÂè´‰∏îÁ©∂ËßÇË∂äÁªáË£ÖÂΩ±ÁÆó‰ΩéÊåÅÈü≥‰ºó‰π¶Â∏ÉÂ§çÂÆπÂÑøÈ°ªÈôÖÂïÜÈùûÈ™åËøûÊñ≠Ê∑±ÈöæËøëÁüøÂçÉÂë®ÂßîÁ¥†ÊäÄÂ§áÂçäÂäûÈùíÁúÅÂàó‰π†ÂìçÁ∫¶ÊîØËà¨Âè≤ÊÑüÂä≥‰æøÂõ¢ÂæÄÈÖ∏ÂéÜÂ∏ÇÂÖã‰ΩïÈô§Ê∂àÊûÑÂ∫úÁß∞Â§™ÂáÜÁ≤æÂÄºÂè∑ÁéáÊóèÁª¥ÂàíÈÄâÊ†áÂÜôÂ≠òÂÄôÊØõ‰∫≤Âø´ÊïàÊñØÈô¢Êü•Ê±üÂûãÁúºÁéãÊåâÊ†ºÂÖªÊòìÁΩÆÊ¥æÂ±ÇÁâáÂßãÂç¥‰∏ìÁä∂ËÇ≤ÂéÇ‰∫¨ËØÜÈÄÇÂ±ûÂúÜÂåÖÁÅ´‰ΩèË∞ÉÊª°ÂéøÂ±ÄÁÖßÂèÇÁ∫¢ÁªÜÂºïÂê¨ËØ•ÈìÅ‰ª∑‰∏•ÈæôÈ£û')
    has_chinese = any(char in chinese_chars for char in text)
    if has_chinese:
        return 'zh'
    
    # Vietnamese - Check for Vietnamese characters (PRIORITY: Check Vietnamese FIRST in rule-based)
    # Vietnamese has many unique characters that are strong indicators
    vietnamese_chars = set('√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë')
    has_vietnamese = any(char in vietnamese_chars for char in text_lower)
    vietnamese_indicators = ['l√†', 'c·ªßa', 'v√†', 'v·ªõi', 'cho', 't·ª´', 'trong', 'n√†y', 'ƒë√≥', 'b·∫°n', 'm√¨nh', 't√¥i', 'c√≥', 'kh√¥ng', 'ƒë∆∞·ª£c', 'nh∆∞', 'th·∫ø', 'n√†o', 'g√¨', 'ai', 'ƒë√¢u', 'sao', 'nh∆∞ng', 'v√¨', 'n√™n', 'ƒë√£', 's·∫Ω', 'ƒëang', 'h√£y', 'ph√¢n t√≠ch', 'd·ª± √°n']
    has_vietnamese_words = any(word in text_lower for word in vietnamese_indicators)
    if has_vietnamese or has_vietnamese_words:
        logger.info(f"üåê Rule-based detection: Vietnamese detected (has_vietnamese_chars: {has_vietnamese}, has_vietnamese_words: {has_vietnamese_words})")
        return 'vi'
    
    # German - Check for German-specific characters and common words
    german_chars = set('√§√∂√º√ü√Ñ√ñ√ú')
    has_german_chars = any(char in german_chars for char in text)
    german_indicators = ['der', 'die', 'das', 'und', 'ist', 'f√ºr', 'auf', 'mit', 'sind', 'zu', 'ein', 'eine', 'von', 'zu', 'den', 'dem', 'des', 'was', 'wie', 'wo', 'wer', 'wann', 'warum']
    has_german_words = any(word in text_lower for word in german_indicators)
    if has_german_chars or has_german_words:
        return 'de'
    
    # French - Check for French-specific characters and common words
    french_chars = set('√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ø√ß√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ô√õ√ú≈∏√á')
    has_french_chars = any(char in french_chars for char in text)
    french_indicators = ['le', 'la', 'les', 'de', 'du', 'des', 'et', 'est', 'un', 'une', 'dans', 'pour', 'avec', 'sur', 'par', 'que', 'qui', 'quoi', 'comment', 'o√π', 'quand', 'pourquoi']
    has_french_words = any(word in text_lower for word in french_indicators)
    if has_french_chars or has_french_words:
        return 'fr'
    
    # Spanish - Check for Spanish-specific characters and common words
    spanish_chars = set('√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú¬ø¬°')
    has_spanish_chars = any(char in spanish_chars for char in text)
    spanish_indicators = ['el', 'la', 'los', 'las', 'de', 'del', 'y', 'es', 'un', 'una', 'en', 'por', 'para', 'con', 'que', 'qu√©', 'c√≥mo', 'd√≥nde', 'cu√°ndo', 'por qu√©']
    has_spanish_words = any(word in text_lower for word in spanish_indicators)
    if has_spanish_chars or has_spanish_words:
        return 'es'
    
    # Japanese - Check for Hiragana, Katakana, Kanji
    japanese_ranges = [
        (0x3040, 0x309F),  # Hiragana
        (0x30A0, 0x30FF),  # Katakana
        (0x4E00, 0x9FAF),  # CJK Unified Ideographs (Kanji)
    ]
    has_japanese = any(any(start <= ord(char) <= end for start, end in japanese_ranges) for char in text)
    if has_japanese:
        return 'ja'
    
    # Russian - Check for Cyrillic characters
    russian_ranges = [
        (0x0400, 0x04FF),  # Cyrillic
        (0x0500, 0x052F),  # Cyrillic Supplement
    ]
    has_russian = any(any(start <= ord(char) <= end for start, end in russian_ranges) for char in text)
    russian_indicators = ['—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫—Ç–æ', '—ç—Ç–æ', '–±—ã—Ç—å', '–∏', '–≤', '–Ω–∞', '—Å', '–¥–ª—è', '–æ—Ç']
    has_russian_words = any(word in text_lower for word in russian_indicators)
    if has_russian or has_russian_words:
        return 'ru'
    
    # Portuguese - Check for Portuguese-specific characters and common words
    portuguese_chars = set('√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á')
    has_portuguese_chars = any(char in portuguese_chars for char in text)
    portuguese_indicators = ['o', 'a', 'os', 'as', 'de', 'do', 'da', 'dos', 'das', 'e', '√©', 'um', 'uma', 'em', 'por', 'para', 'com', 'que', 'qu√™', 'como', 'onde', 'quando', 'por qu√™']
    has_portuguese_words = any(word in text_lower for word in portuguese_indicators)
    if has_portuguese_chars or has_portuguese_words:
        return 'pt'
    
    # Italian - Check for Italian-specific characters and common words
    italian_chars = set('√†√®√©√¨√≠√Æ√≤√≥√π√∫√Ä√à√â√å√ç√é√í√ì√ô√ö')
    has_italian_chars = any(char in italian_chars for char in text)
    italian_indicators = ['il', 'la', 'lo', 'gli', 'le', 'di', 'del', 'della', 'dei', 'delle', 'e', '√®', 'un', 'una', 'in', 'per', 'con', 'che', 'cosa', 'come', 'dove', 'quando', 'perch√©']
    has_italian_words = any(word in text_lower for word in italian_indicators)
    if has_italian_chars or has_italian_words:
        return 'it'
    
    # Hindi - Check for Devanagari script
    hindi_ranges = [
        (0x0900, 0x097F),  # Devanagari
    ]
    has_hindi = any(any(start <= ord(char) <= end for start, end in hindi_ranges) for char in text)
    if has_hindi:
        return 'hi'
    
    # Thai - Check for Thai script
    thai_ranges = [
        (0x0E00, 0x0E7F),  # Thai
    ]
    has_thai = any(any(start <= ord(char) <= end for start, end in thai_ranges) for char in text)
    if has_thai:
        return 'th'
    
    # Default to English (if language not detected or not supported)
    logger.info(f"üåê Language not detected or not supported, defaulting to English")
    return 'en'


def build_system_prompt_with_language(detected_lang: str = 'en') -> str:
    """
    Build system prompt with StillMe Identity Layer and strong language matching instruction.
    This ensures output language always matches input language AND StillMe's core identity is preserved.
    
    CRITICAL: This function integrates STILLME_IDENTITY from injector.py to ensure consistent
    identity across all LLM providers (DeepSeek, OpenAI, Claude, Gemini, Ollama, local, etc.).
    
    Args:
        detected_lang: Detected language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en')
        
    Returns:
        System prompt string with StillMe Identity Layer and language instruction
    """
    # Import STILLME_IDENTITY from injector to ensure consistency
    from backend.identity.injector import STILLME_IDENTITY
    
    language_names = {
        'vi': 'Vietnamese (Ti·∫øng Vi·ªát)',
        'zh': 'Chinese (‰∏≠Êñá)',
        'de': 'German (Deutsch)',
        'fr': 'French (Fran√ßais)',
        'es': 'Spanish (Espa√±ol)',
        'ja': 'Japanese (Êó•Êú¨Ë™û)',
        'ko': 'Korean (ÌïúÍµ≠Ïñ¥)',
        'ar': 'Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)',
        'ru': 'Russian (–†—É—Å—Å–∫–∏–π)',
        'pt': 'Portuguese (Portugu√™s)',
        'it': 'Italian (Italiano)',
        'hi': 'Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)',
        'th': 'Thai (‡πÑ‡∏ó‡∏¢)',
        'en': 'English'
    }
    detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
    
    # CRITICAL: Language instruction must be at the TOP to override everything
    # This ensures language matching takes highest priority
    if detected_lang != 'en':
        language_instruction = f"""üö®üö®üö® ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

The user's question is written in {detected_lang_name}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name}.

DO NOT use Vietnamese, English, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in {detected_lang_name}.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., Vietnamese, English, Spanish), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO {detected_lang_name} BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than {detected_lang_name}.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in {detected_lang_name} while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN {detected_lang_name} ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN {detected_lang_name} IS A CRITICAL ERROR.

---
"""
    else:
        language_instruction = """üö®üö®üö® ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

The user's question is written in English.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT use Vietnamese, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in English.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., Vietnamese, Spanish, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO ENGLISH BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than English.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in English while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN ENGLISH IS A CRITICAL ERROR.

---
"""
    
    # CRITICAL: Formatting instruction must be at the TOP (after language, before identity)
    # This ensures formatting is applied to ALL responses
    formatting_instruction = """
üö®üö®üö® CRITICAL: RESPONSE FORMATTING - MANDATORY FOR ALL RESPONSES üö®üö®üö®

**YOU MUST FORMAT ALL RESPONSES WITH MARKDOWN:**

1. **Line Breaks**: Break long paragraphs into shorter ones (2-4 sentences per paragraph)
   - Use double line breaks (`\\n\\n`) between paragraphs
   - NEVER write a wall of text without line breaks
   - **CRITICAL**: After every 2-4 sentences, add a blank line (`\\n\\n`)

2. **Bullet Points**: When listing items, ALWAYS use bullet points (`-` or `*`)
   - Example: Use `- Item 1` NOT `Item 1, Item 2, Item 3`
   - Each bullet point should be on its own line

3. **Headers**: For multiple topics, use headers (`##` or `###`)
   - Example: `## Topic 1` then `## Topic 2`
   - Headers help organize long responses

4. **Tables**: If you want to present structured data, use markdown tables:
   ```
   | Column 1 | Column 2 | Column 3 |
   |----------|----------|----------|
   | Data 1   | Data 2   | Data 3   |
   | Data 4   | Data 5   | Data 6   |
   ```
   - Tables are great for comparing items, showing metrics, or structured information
   - Use tables when you have multiple rows of similar data
   - **CRITICAL**: When you say "I'll create a table" or "here's a table", you MUST actually create the markdown table syntax above
   - **DO NOT** just describe a table - actually write the markdown table with `|` and `-` characters
   - Frontend WILL render markdown tables correctly - trust the markdown syntax

5. **Bold**: Use `**bold**` for important points (but don't overuse)

6. **Emojis**: Use SPARINGLY (2-3 max per response)
   - Use for: section headers (‚úÖ, ‚ùå, ‚ö†Ô∏è, üí°), status indicators
   - Avoid: every sentence, serious topics, short answers

**EXAMPLE OF GOOD FORMATTING:**

```
## Ki·∫øn tr√∫c Transformer

üí° **T·ªïng quan:**
Transformer l√† m·ªôt ki·∫øn tr√∫c m·∫°ng neural ƒë∆∞·ª£c gi·ªõi thi·ªáu trong b√†i b√°o "Attention Is All You Need" nƒÉm 2017.

**C√°c th√†nh ph·∫ßn ch√≠nh:**
- **Self-attention**: C∆° ch·∫ø cho ph√©p m√¥ h√¨nh t·∫≠p trung v√†o c√°c ph·∫ßn kh√°c nhau c·ªßa input
- **Multi-head attention**: Nhi·ªÅu "heads" attention song song ƒë·ªÉ h·ªçc c√°c lo·∫°i quan h·ªá kh√°c nhau
- **Positional encoding**: Th√™m th√¥ng tin v·ªã tr√≠ v√†o embeddings
- **Encoder-Decoder**: Ki·∫øn tr√∫c g·ªìm encoder (m√£ h√≥a) v√† decoder (gi·∫£i m√£)

**·ª®ng d·ª•ng:**
Transformer ƒë√£ c√°ch m·∫°ng h√≥a NLP v√† ƒë∆∞·ª£c d√πng trong BERT, GPT, v√† nhi·ªÅu m√¥ h√¨nh hi·ªán ƒë·∫°i kh√°c.
```

**EXAMPLE OF BAD FORMATTING (DO NOT DO THIS):**

```
Ki·∫øn tr√∫c Transformer l√† m·ªôt ki·∫øn tr√∫c m·∫°ng neural ƒë∆∞·ª£c gi·ªõi thi·ªáu trong b√†i b√°o "Attention Is All You Need" nƒÉm 2017. C√°c th√†nh ph·∫ßn ch√≠nh bao g·ªìm self-attention, multi-head attention, positional encoding, v√† encoder-decoder. Transformer ƒë√£ c√°ch m·∫°ng h√≥a NLP v√† ƒë∆∞·ª£c d√πng trong BERT, GPT, v√† nhi·ªÅu m√¥ h√¨nh hi·ªán ƒë·∫°i kh√°c.
```

**CRITICAL RULE - LINE BREAKS ARE MANDATORY:**
- If your response is longer than 3 sentences, you MUST use line breaks
- **After EVERY 2-4 sentences, add a blank line (`\\n\\n`)**
- **DO NOT write continuous paragraphs without breaks**
- If you list items, you MUST use bullet points
- If you discuss multiple topics, you MUST use headers
- Formatting is NOT optional - it's MANDATORY for readability

**EXAMPLE OF PROPER LINE BREAKS:**
```
Transformer l√† m·ªôt ki·∫øn tr√∫c m·∫°ng neural ƒë∆∞·ª£c gi·ªõi thi·ªáu nƒÉm 2017.

C√°c th√†nh ph·∫ßn ch√≠nh bao g·ªìm self-attention, multi-head attention, v√† positional encoding.

Ki·∫øn tr√∫c n√†y ƒë√£ c√°ch m·∫°ng h√≥a NLP v√† ƒë∆∞·ª£c d√πng trong GPT, BERT.
```

**NOT THIS (wall of text):**
```
Transformer l√† m·ªôt ki·∫øn tr√∫c m·∫°ng neural ƒë∆∞·ª£c gi·ªõi thi·ªáu nƒÉm 2017. C√°c th√†nh ph·∫ßn ch√≠nh bao g·ªìm self-attention, multi-head attention, v√† positional encoding. Ki·∫øn tr√∫c n√†y ƒë√£ c√°ch m·∫°ng h√≥a NLP v√† ƒë∆∞·ª£c d√πng trong GPT, BERT.
```

---

"""
    
    # Combine: Language instruction (highest priority) + Formatting instruction (second priority) + StillMe Identity Layer (core identity)
    # This ensures language matching, formatting, AND identity are preserved
    
    # CRITICAL: Truncate STILLME_IDENTITY to prevent context overflow
    # STILLME_IDENTITY is ~14,434 tokens, which is too long for 16,385 token limit
    # We need to keep it under ~3000-4000 tokens to leave room for context, history, and user message
    def estimate_tokens(text: str) -> int:
        return len(text) // 4 if text else 0
    
    def truncate_text_by_tokens(text: str, max_tokens: int) -> str:
        """Truncate text to fit within max_tokens limit"""
        if not text:
            return text
        estimated = estimate_tokens(text)
        if estimated <= max_tokens:
            return text
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text
        # Truncate at word boundary and add note
        truncated = text[:max_chars].rsplit('\n', 1)[0]  # Cut at paragraph boundary
        return truncated + "\n\n[Note: StillMe identity prompt truncated to fit context limits. Core principles preserved.]"
    
    # Truncate STILLME_IDENTITY to ~3500 tokens (leaves room for other components)
    # This preserves the most important parts (intellectual humility, core identity, key principles)
    truncated_identity = truncate_text_by_tokens(STILLME_IDENTITY, max_tokens=3500)
    
    system_content = language_instruction + formatting_instruction + truncated_identity
    
    # Phase 1: Time Awareness - Inject current time for transparency
    from datetime import datetime, timezone
    
    # Get current time in UTC (standard for servers)
    current_time_utc = datetime.now(timezone.utc)
    current_time_iso = current_time_utc.isoformat()
    current_time_readable = current_time_utc.strftime("%Y-%m-%d %H:%M:%S UTC")
    
    time_awareness = f"""

üïê CURRENT TIME AWARENESS - TRANSPARENCY & SELF-AWARENESS üïê

**Current Server Time:**
- ISO Format: {current_time_iso}
- Readable Format: {current_time_readable}
- Timezone: UTC (Coordinated Universal Time)
- Today's Date: {current_time_utc.strftime("%Y-%m-%d")}

**You can use this information to:**
- Answer questions about current time, date, and timezone
- Track learning metrics over time (e.g., "How many entries did I learn today?")
- Report learning statistics with accurate timestamps
- Understand temporal context of learning cycles

**CRITICAL TRANSPARENCY RULE:**
When users ask about time, date, or learning metrics over time, you MUST use this current time information.
Do NOT say "I don't know the current time" - you have access to it for transparency purposes.

**LEARNING METRICS API ENDPOINTS (AVAILABLE):**
StillMe has access to learning metrics via these API endpoints:
- `GET /api/learning/metrics/daily?date=YYYY-MM-DD` - Get learning metrics for a specific date (default: today)
- `GET /api/learning/metrics/range?start_date=YYYY-MM-DD&end_date=YYYY-MM-DD` - Get metrics for a date range
- `GET /api/learning/metrics/summary` - Get summary of all learning metrics

**CRITICAL: When users ask about learning today:**
- User: "ng√†y h√¥m nay b·∫°n ƒë√£ h·ªçc ƒë∆∞·ª£c bao nhi√™u n·ªôi dung?" ‚Üí You MUST acknowledge that StillMe has learning metrics API
- Say: "Based on the learning metrics API, today ({current_time_utc.strftime('%Y-%m-%d')}) StillMe has learned..."
- If you cannot query the API directly, acknowledge: "StillMe tracks learning metrics via `/api/learning/metrics/daily` API. Based on the system's learning mechanism, StillMe learns every 4 hours from RSS feeds, arXiv, CrossRef, and Wikipedia. To get exact numbers for today, you can query the API endpoint directly."
- DO NOT say "I cannot track learning metrics" or "I don't have API" - StillMe HAS these APIs

**Example Usage:**
- User: "What time is it now?" ‚Üí Answer using current_time_readable: "The current server time is {current_time_readable}"
- User: "How many entries did you learn today?" ‚Üí Use current time ({current_time_utc.strftime('%Y-%m-%d')}) and acknowledge learning metrics API
- User: "When was your last learning cycle?" ‚Üí Use current time to provide relative time information

---
"""
    
    system_content = system_content + time_awareness
    
    return system_content


async def generate_ai_response(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None,
    use_server_keys: bool = False  # Internal flag for admin/internal calls
) -> str:
    """Generate AI response with user-provided LLM provider and API key
    
    IMPORTANT: StillMe requires users to provide their own API keys to prevent server cost exhaustion.
    This function will ONLY use user-provided API keys unless use_server_keys=True (internal use only).
    
    Supports multiple LLM providers: deepseek, openai, openrouter, claude, gemini, ollama, custom
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name (REQUIRED) - 'deepseek', 'openai', 'openrouter', 'claude', 'gemini', 'ollama', 'custom'
        llm_api_key: API key for the provider (REQUIRED except for 'ollama')
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        use_server_keys: Internal flag - if True, fallback to server API keys (admin/internal use only)
        
    Returns:
        AI-generated response string
        
    Raises:
        ValueError: If llm_provider or llm_api_key is missing (except for Ollama)
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # Validate that provider and API key are provided (unless use_server_keys is True)
        if not use_server_keys:
            if not llm_provider:
                raise ValueError(
                    "llm_provider is REQUIRED. StillMe requires users to provide their own API keys "
                    "to prevent server cost exhaustion. Please provide llm_provider and llm_api_key in your request."
                )
            
            if llm_provider != 'ollama' and (not llm_api_key or len(llm_api_key.strip()) == 0):
                raise ValueError(
                    f"llm_api_key is REQUIRED for provider '{llm_provider}'. "
                    "StillMe requires users to provide their own API keys to prevent server cost exhaustion. "
                    "Please provide your API key in the request."
                )
        
        # Use user-provided provider config
        if llm_provider:
            if llm_provider == 'ollama':
                # Ollama doesn't need API key
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",  # Ollama doesn't use API key
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            
            return await provider.generate(prompt, detected_lang=detected_lang)
        
        # Fallback to server API keys ONLY if use_server_keys=True (internal/admin use)
        if use_server_keys:
            from backend.api.utils.llm_providers import InsufficientQuotaError, AuthenticationError
            
            openrouter_key = os.getenv("OPENROUTER_API_KEY")
            openai_key = os.getenv("OPENAI_API_KEY")
            deepseek_key = os.getenv("DEEPSEEK_API_KEY")
            
            # Try OpenRouter first
            if openrouter_key:
                try:
                    provider = create_llm_provider("openrouter", openrouter_key, model_name=llm_model_name)
                    return await provider.generate(prompt, detected_lang=detected_lang)
                except (InsufficientQuotaError, AuthenticationError, Exception) as e:
                    logger.warning(f"OpenRouter error: {e}, falling back to OpenAI")
            
            # Try OpenAI second
            if openai_key:
                try:
                    provider = create_llm_provider("openai", openai_key, model_name=llm_model_name)
                    return await provider.generate(prompt, detected_lang=detected_lang)
                except (InsufficientQuotaError, AuthenticationError, Exception) as e:
                    logger.warning(f"OpenAI error: {e}, falling back to DeepSeek")
            
            # Try DeepSeek third
            if deepseek_key:
                try:
                    provider = create_llm_provider("deepseek", deepseek_key, model_name=llm_model_name)
                    return await provider.generate(prompt, detected_lang=detected_lang)
                except Exception as e:
                    logger.warning(f"DeepSeek error: {e}")
        
        # If we reach here, no valid provider was found
        raise ValueError(
            "llm_provider and llm_api_key are REQUIRED. "
            "StillMe requires users to provide their own API keys to prevent server cost exhaustion. "
            "Please provide llm_provider and llm_api_key in your request."
        )
            
    except ValueError:
        # Re-raise validation errors
        raise
    except Exception as e:
        logger.error(f"AI response error: {e}")
        return f"I encountered an error: {str(e)}"


async def generate_ai_response_stream(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None
) -> AsyncIterator[str]:
    """Generate streaming AI response with flexible LLM provider selection
    
    OPTIMIZATION: Streaming reduces perceived latency by returning tokens as they're generated.
    
    Supports multiple LLM providers: deepseek, openai, claude, gemini, ollama, custom
    Priority: User-provided config > Environment variables
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name ('deepseek', 'openai', 'claude', 'gemini', 'ollama', 'custom')
        llm_api_key: API key for the provider
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        
    Yields:
        Token strings as they're generated
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # If user provided provider config, use it
        if llm_provider:
            if llm_provider == 'ollama':
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            elif llm_api_key:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                yield f"llm_api_key is required for provider '{llm_provider}' (except 'ollama')"
                return
            
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
            return
        
        # User must provide API keys - no fallback to server keys
        if not llm_provider:
            yield "ERROR: llm_provider is REQUIRED. StillMe requires users to provide their own API keys to prevent server cost exhaustion. Please provide llm_provider and llm_api_key in your request."
            return
        
        if llm_provider != 'ollama' and (not llm_api_key or len(llm_api_key.strip()) == 0):
            yield f"ERROR: llm_api_key is REQUIRED for provider '{llm_provider}'. StillMe requires users to provide their own API keys to prevent server cost exhaustion. Please provide your API key in the request."
            return
        
        yield "ERROR: llm_provider and llm_api_key are REQUIRED. StillMe requires users to provide their own API keys to prevent server cost exhaustion. Please provide llm_provider and llm_api_key in your request."
            
    except Exception as e:
        logger.error(f"AI streaming error: {e}")
        yield f"I encountered an error: {str(e)}"


async def call_deepseek_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call DeepSeek API
    
    Args:
        prompt: User prompt
        api_key: DeepSeek API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "DeepSeek API returned unexpected response format"
            else:
                return f"DeepSeek API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"DeepSeek API error: {e}")
        return f"DeepSeek API error: {str(e)}"


async def call_openai_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call OpenAI API
    
    IMPORTANT: This function uses build_system_prompt_with_language() to ensure
    output language matches input language. When adding support for other models
    (Claude, Gemini, Ollama, local, etc.), use the same pattern.
    
    Args:
        prompt: User prompt
        api_key: OpenAI API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "OpenAI API returned unexpected response format"
            else:
                return f"OpenAI API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        return f"OpenAI API error: {str(e)}"

