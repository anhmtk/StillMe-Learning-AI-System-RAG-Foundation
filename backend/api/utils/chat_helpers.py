"""
Chat Helper Functions for StillMe API
Shared utilities for chat endpoints (language detection, AI response generation)
"""

import os
import logging
import httpx
from typing import Optional, AsyncIterator

logger = logging.getLogger(__name__)


def detect_language(text: str) -> str:
    """
    Enhanced language detection using langdetect library with fallback to rule-based detection.
    Supports: vi, zh, de, fr, es, ja, ko, ar, ru, pt, it, hi, th, en
    
    CRITICAL: Also checks for explicit language requests (e.g., "n√≥i b·∫±ng ti·∫øng Nga", "speak in Russian")
    If user explicitly requests a different language, that takes priority.
    
    Returns: Language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en') or 'en' as default
    If language is not detected or not supported, returns 'en' (English) as fallback.
    """
    if not text or len(text.strip()) == 0:
        return 'en'
    
    text_lower = text.lower()
    
    # OPTIMIZATION: Try langdetect FIRST for better accuracy, especially for mixed-language text
    # Then check for explicit language requests (which override detection)
    detected_lang = None
    
    try:
        from langdetect import detect, LangDetectException
        detected = detect(text)
        
        # Map langdetect codes to our internal codes
        lang_map = {
            'vi': 'vi',  # Vietnamese
            'zh-cn': 'zh', 'zh-tw': 'zh',  # Chinese
            'de': 'de',  # German
            'fr': 'fr',  # French
            'es': 'es',  # Spanish
            'ja': 'ja',  # Japanese
            'ko': 'ko',  # Korean
            'ar': 'ar',  # Arabic
            'ru': 'ru',  # Russian
            'pt': 'pt',  # Portuguese
            'it': 'it',  # Italian
            'hi': 'hi',  # Hindi
            'th': 'th',  # Thai
            'en': 'en'   # English
        }
        
        # Handle Chinese variants
        if detected.startswith('zh'):
            detected_lang = 'zh'
        elif detected in lang_map:
            detected_lang = lang_map[detected]
            logger.info(f"üåê langdetect detected: {detected} -> {detected_lang}")
            
    except (LangDetectException, ImportError) as e:
        logger.debug(f"langdetect failed or not available: {e}, will use rule-based detection")
    
    # CRITICAL: Check for explicit language requests (only clear requests, not mentions)
    # This allows users to request responses in a different language
    # IMPORTANT: Only match explicit request patterns (e.g., "n√≥i b·∫±ng", "speak in"), not just mentions
    # Explicit requests OVERRIDE language detection
    explicit_language_patterns = {
        'ru': ['n√≥i b·∫±ng ti·∫øng nga', 'speak in russian', '–æ—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º', '–ø–æ-—Ä—É—Å—Å–∫–∏', 'respond in russian', 'reply in russian'],
        'en': ['n√≥i b·∫±ng ti·∫øng anh', 'speak in english', 'respond in english', 'reply in english', 'answer in english'],
        'vi': ['n√≥i b·∫±ng ti·∫øng vi·ªát', 'speak in vietnamese', 'respond in vietnamese', 'reply in vietnamese', 'answer in vietnamese'],
        'zh': ['n√≥i b·∫±ng ti·∫øng trung', 'speak in chinese', 'respond in chinese', 'reply in chinese'],
        'de': ['n√≥i b·∫±ng ti·∫øng ƒë·ª©c', 'speak in german', 'respond in german', 'reply in german'],
        'fr': ['n√≥i b·∫±ng ti·∫øng ph√°p', 'speak in french', 'respond in french', 'reply in french'],
        'es': ['n√≥i b·∫±ng ti·∫øng t√¢y ban nha', 'speak in spanish', 'respond in spanish', 'reply in spanish'],
        'ja': ['n√≥i b·∫±ng ti·∫øng nh·∫≠t', 'speak in japanese', 'respond in japanese', 'reply in japanese'],
        'ko': ['n√≥i b·∫±ng ti·∫øng h√†n', 'speak in korean', 'respond in korean', 'reply in korean'],
        'ar': ['n√≥i b·∫±ng ti·∫øng ·∫£ r·∫≠p', 'speak in arabic', 'respond in arabic', 'reply in arabic'],
        'pt': ['n√≥i b·∫±ng ti·∫øng b·ªì ƒë√†o nha', 'speak in portuguese', 'respond in portuguese', 'reply in portuguese'],
        'it': ['n√≥i b·∫±ng ti·∫øng √Ω', 'speak in italian', 'respond in italian', 'reply in italian'],
        'hi': ['n√≥i b·∫±ng ti·∫øng hindi', 'speak in hindi', 'respond in hindi', 'reply in hindi'],
        'th': ['n√≥i b·∫±ng ti·∫øng th√°i', 'speak in thai', 'respond in thai', 'reply in thai'],
    }
    
    # Check for explicit requests (must contain request verbs, not just language mentions)
    for lang_code, patterns in explicit_language_patterns.items():
        if any(pattern in text_lower for pattern in patterns):
            logger.info(f"üåê Explicit language request detected: {lang_code} (overriding detection: {detected_lang})")
            return lang_code
    
    # If explicit request found, return it; otherwise use detected language
    if detected_lang:
        return detected_lang
    
    # Fallback to rule-based detection if langdetect failed
    text_lower = text.lower()
    
    # Arabic - Check for Arabic characters
    arabic_ranges = [
        (0x0600, 0x06FF),  # Arabic
        (0x0750, 0x077F),  # Arabic Supplement
        (0x08A0, 0x08FF),  # Arabic Extended-A
    ]
    has_arabic = any(any(start <= ord(char) <= end for start, end in arabic_ranges) for char in text)
    if has_arabic:
        return 'ar'
    
    # Korean - Check for Hangul
    korean_ranges = [
        (0xAC00, 0xD7AF),  # Hangul Syllables
        (0x1100, 0x11FF),  # Hangul Jamo
    ]
    has_korean = any(any(start <= ord(char) <= end for start, end in korean_ranges) for char in text)
    if has_korean:
        return 'ko'
    
    # Chinese (Simplified/Traditional) - Check for Chinese characters
    chinese_chars = set('ÁöÑ‰∏ÄÊòØÂú®‰∏ç‰∫ÜÊúâÂíå‰∫∫Ëøô‰∏≠Â§ß‰∏∫‰∏ä‰∏™ÂõΩÊàë‰ª•Ë¶Å‰ªñÊó∂Êù•Áî®‰ª¨ÁîüÂà∞‰ΩúÂú∞‰∫éÂá∫Â∞±ÂàÜÂØπÊàê‰ºöÂèØ‰∏ªÂèëÂπ¥Âä®ÂêåÂ∑•‰πüËÉΩ‰∏ãËøáÂ≠êËØ¥‰∫ßÁßçÈù¢ËÄåÊñπÂêéÂ§öÂÆöË°åÂ≠¶Ê≥ïÊâÄÊ∞ëÂæóÁªèÂçÅ‰∏â‰πãËøõÁùÄÁ≠âÈÉ®Â∫¶ÂÆ∂ÁîµÂäõÈáåÂ¶ÇÊ∞¥ÂåñÈ´òËá™‰∫åÁêÜËµ∑Â∞èÁâ©Áé∞ÂÆûÂä†ÈáèÈÉΩ‰∏§‰ΩìÂà∂Êú∫ÂΩì‰ΩøÁÇπ‰ªé‰∏öÊú¨ÂéªÊääÊÄßÂ•ΩÂ∫îÂºÄÂÆÉÂêàËøòÂõ†Áî±ÂÖ∂‰∫õÁÑ∂ÂâçÂ§ñÂ§©ÊîøÂõõÊó•ÈÇ£Á§æ‰πâ‰∫ãÂπ≥ÂΩ¢Áõ∏ÂÖ®Ë°®Èó¥Ê†∑‰∏éÂÖ≥ÂêÑÈáçÊñ∞Á∫øÂÜÖÊï∞Ê≠£ÂøÉÂèç‰Ω†ÊòéÁúãÂéüÂèà‰πàÂà©ÊØîÊàñ‰ΩÜË¥®Ê∞îÁ¨¨ÂêëÈÅìÂëΩÊ≠§ÂèòÊù°Âè™Ê≤°ÁªìËß£ÈóÆÊÑèÂª∫ÊúàÂÖ¨Êó†Á≥ªÂÜõÂæàÊÉÖËÄÖÊúÄÁ´ã‰ª£ÊÉ≥Â∑≤ÈÄöÂπ∂ÊèêÁõ¥È¢òÂÖöÁ®ãÂ±ï‰∫îÊûúÊñôË±°ÂëòÈù©‰ΩçÂÖ•Â∏∏ÊñáÊÄªÊ¨°ÂìÅÂºèÊ¥ªËÆæÂèäÁÆ°Áâπ‰ª∂ÈïøÊ±ÇËÄÅÂ§¥Âü∫ËµÑËæπÊµÅË∑ØÁ∫ßÂ∞ëÂõæÂ±±ÁªüÊé•Áü•ËæÉÂ∞ÜÁªÑËßÅËÆ°Âà´Â•πÊâãËßíÊúüÊ†πËÆ∫ËøêÂÜúÊåáÂá†‰πùÂå∫Âº∫ÊîæÂÜ≥Ë•øË¢´Âπ≤ÂÅöÂøÖÊàòÂÖàÂõûÂàô‰ªªÂèñÊçÆÂ§ÑÈòüÂçóÁªôËâ≤ÂÖâÈó®Âç≥‰øùÊ≤ªÂåóÈÄ†ÁôæËßÑÁÉ≠È¢Ü‰∏ÉÊµ∑Âè£‰∏úÂØºÂô®ÂéãÂøó‰∏ñÈáëÂ¢û‰∫âÊµéÈò∂Ê≤πÊÄùÊúØÊûÅ‰∫§ÂèóËÅî‰ªÄËÆ§ÂÖ≠ÂÖ±ÊùÉÊî∂ËØÅÊîπÊ∏ÖÂ∑±ÁæéÂÜçÈááËΩ¨Êõ¥ÂçïÈ£éÂàáÊâìÁôΩÊïôÈÄüËä±Â∏¶ÂÆâÂú∫Ë∫´ËΩ¶‰æãÁúüÂä°ÂÖ∑‰∏áÊØèÁõÆËá≥ËææËµ∞ÁßØÁ§∫ËÆÆÂ£∞Êä•ÊñóÂÆåÁ±ªÂÖ´Á¶ªÂçéÂêçÁ°ÆÊâçÁßëÂº†‰ø°È©¨ËäÇËØùÁ±≥Êï¥Á©∫ÂÖÉÂÜµ‰ªäÈõÜÊ∏©‰º†ÂúüËÆ∏Ê≠•Áæ§ÂπøÁü≥ËÆ∞ÈúÄÊÆµÁ†îÁïåÊãâÊûóÂæãÂè´‰∏îÁ©∂ËßÇË∂äÁªáË£ÖÂΩ±ÁÆó‰ΩéÊåÅÈü≥‰ºó‰π¶Â∏ÉÂ§çÂÆπÂÑøÈ°ªÈôÖÂïÜÈùûÈ™åËøûÊñ≠Ê∑±ÈöæËøëÁüøÂçÉÂë®ÂßîÁ¥†ÊäÄÂ§áÂçäÂäûÈùíÁúÅÂàó‰π†ÂìçÁ∫¶ÊîØËà¨Âè≤ÊÑüÂä≥‰æøÂõ¢ÂæÄÈÖ∏ÂéÜÂ∏ÇÂÖã‰ΩïÈô§Ê∂àÊûÑÂ∫úÁß∞Â§™ÂáÜÁ≤æÂÄºÂè∑ÁéáÊóèÁª¥ÂàíÈÄâÊ†áÂÜôÂ≠òÂÄôÊØõ‰∫≤Âø´ÊïàÊñØÈô¢Êü•Ê±üÂûãÁúºÁéãÊåâÊ†ºÂÖªÊòìÁΩÆÊ¥æÂ±ÇÁâáÂßãÂç¥‰∏ìÁä∂ËÇ≤ÂéÇ‰∫¨ËØÜÈÄÇÂ±ûÂúÜÂåÖÁÅ´‰ΩèË∞ÉÊª°ÂéøÂ±ÄÁÖßÂèÇÁ∫¢ÁªÜÂºïÂê¨ËØ•ÈìÅ‰ª∑‰∏•ÈæôÈ£û')
    has_chinese = any(char in chinese_chars for char in text)
    if has_chinese:
        return 'zh'
    
    # Vietnamese - Check for Vietnamese characters (PRIORITY: Check Vietnamese FIRST in rule-based)
    # Vietnamese has many unique characters that are strong indicators
    vietnamese_chars = set('√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë')
    has_vietnamese = any(char in vietnamese_chars for char in text_lower)
    vietnamese_indicators = ['l√†', 'c·ªßa', 'v√†', 'v·ªõi', 'cho', 't·ª´', 'trong', 'n√†y', 'ƒë√≥', 'b·∫°n', 'm√¨nh', 't√¥i', 'c√≥', 'kh√¥ng', 'ƒë∆∞·ª£c', 'nh∆∞', 'th·∫ø', 'n√†o', 'g√¨', 'ai', 'ƒë√¢u', 'sao', 'nh∆∞ng', 'v√¨', 'n√™n', 'ƒë√£', 's·∫Ω', 'ƒëang', 'h√£y', 'ph√¢n t√≠ch', 'd·ª± √°n']
    has_vietnamese_words = any(word in text_lower for word in vietnamese_indicators)
    if has_vietnamese or has_vietnamese_words:
        logger.info(f"üåê Rule-based detection: Vietnamese detected (has_vietnamese_chars: {has_vietnamese}, has_vietnamese_words: {has_vietnamese_words})")
        return 'vi'
    
    # German - Check for German-specific characters and common words
    german_chars = set('√§√∂√º√ü√Ñ√ñ√ú')
    has_german_chars = any(char in german_chars for char in text)
    german_indicators = ['der', 'die', 'das', 'und', 'ist', 'f√ºr', 'auf', 'mit', 'sind', 'zu', 'ein', 'eine', 'von', 'zu', 'den', 'dem', 'des', 'was', 'wie', 'wo', 'wer', 'wann', 'warum']
    has_german_words = any(word in text_lower for word in german_indicators)
    if has_german_chars or has_german_words:
        return 'de'
    
    # French - Check for French-specific characters and common words
    french_chars = set('√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ø√ß√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ô√õ√ú≈∏√á')
    has_french_chars = any(char in french_chars for char in text)
    french_indicators = ['le', 'la', 'les', 'de', 'du', 'des', 'et', 'est', 'un', 'une', 'dans', 'pour', 'avec', 'sur', 'par', 'que', 'qui', 'quoi', 'comment', 'o√π', 'quand', 'pourquoi']
    has_french_words = any(word in text_lower for word in french_indicators)
    if has_french_chars or has_french_words:
        return 'fr'
    
    # Spanish - Check for Spanish-specific characters and common words
    spanish_chars = set('√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú¬ø¬°')
    has_spanish_chars = any(char in spanish_chars for char in text)
    spanish_indicators = ['el', 'la', 'los', 'las', 'de', 'del', 'y', 'es', 'un', 'una', 'en', 'por', 'para', 'con', 'que', 'qu√©', 'c√≥mo', 'd√≥nde', 'cu√°ndo', 'por qu√©']
    has_spanish_words = any(word in text_lower for word in spanish_indicators)
    if has_spanish_chars or has_spanish_words:
        return 'es'
    
    # Japanese - Check for Hiragana, Katakana, Kanji
    japanese_ranges = [
        (0x3040, 0x309F),  # Hiragana
        (0x30A0, 0x30FF),  # Katakana
        (0x4E00, 0x9FAF),  # CJK Unified Ideographs (Kanji)
    ]
    has_japanese = any(any(start <= ord(char) <= end for start, end in japanese_ranges) for char in text)
    if has_japanese:
        return 'ja'
    
    # Russian - Check for Cyrillic characters
    russian_ranges = [
        (0x0400, 0x04FF),  # Cyrillic
        (0x0500, 0x052F),  # Cyrillic Supplement
    ]
    has_russian = any(any(start <= ord(char) <= end for start, end in russian_ranges) for char in text)
    russian_indicators = ['—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫—Ç–æ', '—ç—Ç–æ', '–±—ã—Ç—å', '–∏', '–≤', '–Ω–∞', '—Å', '–¥–ª—è', '–æ—Ç']
    has_russian_words = any(word in text_lower for word in russian_indicators)
    if has_russian or has_russian_words:
        return 'ru'
    
    # Portuguese - Check for Portuguese-specific characters and common words
    portuguese_chars = set('√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á')
    has_portuguese_chars = any(char in portuguese_chars for char in text)
    portuguese_indicators = ['o', 'a', 'os', 'as', 'de', 'do', 'da', 'dos', 'das', 'e', '√©', 'um', 'uma', 'em', 'por', 'para', 'com', 'que', 'qu√™', 'como', 'onde', 'quando', 'por qu√™']
    has_portuguese_words = any(word in text_lower for word in portuguese_indicators)
    if has_portuguese_chars or has_portuguese_words:
        return 'pt'
    
    # Italian - Check for Italian-specific characters and common words
    italian_chars = set('√†√®√©√¨√≠√Æ√≤√≥√π√∫√Ä√à√â√å√ç√é√í√ì√ô√ö')
    has_italian_chars = any(char in italian_chars for char in text)
    italian_indicators = ['il', 'la', 'lo', 'gli', 'le', 'di', 'del', 'della', 'dei', 'delle', 'e', '√®', 'un', 'una', 'in', 'per', 'con', 'che', 'cosa', 'come', 'dove', 'quando', 'perch√©']
    has_italian_words = any(word in text_lower for word in italian_indicators)
    if has_italian_chars or has_italian_words:
        return 'it'
    
    # Hindi - Check for Devanagari script
    hindi_ranges = [
        (0x0900, 0x097F),  # Devanagari
    ]
    has_hindi = any(any(start <= ord(char) <= end for start, end in hindi_ranges) for char in text)
    if has_hindi:
        return 'hi'
    
    # Thai - Check for Thai script
    thai_ranges = [
        (0x0E00, 0x0E7F),  # Thai
    ]
    has_thai = any(any(start <= ord(char) <= end for start, end in thai_ranges) for char in text)
    if has_thai:
        return 'th'
    
    # Default to English (if language not detected or not supported)
    logger.info(f"üåê Language not detected or not supported, defaulting to English")
    return 'en'


def build_system_prompt_with_language(detected_lang: str = 'en') -> str:
    """
    Build system prompt with StillMe Identity Layer and strong language matching instruction.
    This ensures output language always matches input language AND StillMe's core identity is preserved.
    
    CRITICAL: This function integrates STILLME_IDENTITY from injector.py to ensure consistent
    identity across all LLM providers (DeepSeek, OpenAI, Claude, Gemini, Ollama, local, etc.).
    
    Args:
        detected_lang: Detected language code ('vi', 'zh', 'de', 'fr', 'es', 'ja', 'ko', 'ar', 'ru', 'pt', 'it', 'hi', 'th', 'en')
        
    Returns:
        System prompt string with StillMe Identity Layer and language instruction
    """
    # Import STILLME_IDENTITY from injector to ensure consistency
    from backend.identity.injector import STILLME_IDENTITY
    
    language_names = {
        'vi': 'Vietnamese (Ti·∫øng Vi·ªát)',
        'zh': 'Chinese (‰∏≠Êñá)',
        'de': 'German (Deutsch)',
        'fr': 'French (Fran√ßais)',
        'es': 'Spanish (Espa√±ol)',
        'ja': 'Japanese (Êó•Êú¨Ë™û)',
        'ko': 'Korean (ÌïúÍµ≠Ïñ¥)',
        'ar': 'Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)',
        'ru': 'Russian (–†—É—Å—Å–∫–∏–π)',
        'pt': 'Portuguese (Portugu√™s)',
        'it': 'Italian (Italiano)',
        'hi': 'Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)',
        'th': 'Thai (‡πÑ‡∏ó‡∏¢)',
        'en': 'English'
    }
    detected_lang_name = language_names.get(detected_lang, 'the same language as the question')
    
    # CRITICAL: Language instruction must be at the TOP to override everything
    # This ensures language matching takes highest priority
    if detected_lang != 'en':
        language_instruction = f"""üö®üö®üö® ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

The user's question is written in {detected_lang_name}.

YOU MUST RESPOND EXCLUSIVELY IN {detected_lang_name}.

DO NOT use Vietnamese, English, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in {detected_lang_name}.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., Vietnamese, English, Spanish), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO {detected_lang_name} BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than {detected_lang_name}.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in {detected_lang_name} while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN {detected_lang_name} ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN {detected_lang_name} IS A CRITICAL ERROR.

---
"""
    else:
        language_instruction = """üö®üö®üö® ZERO TOLERANCE LANGUAGE REQUIREMENT - HIGHEST PRIORITY - OVERRIDES EVERYTHING üö®üö®üö®

The user's question is written in English.

YOU MUST RESPOND EXCLUSIVELY IN ENGLISH.

DO NOT use Vietnamese, Spanish, German, French, or ANY OTHER LANGUAGE.

EVERY SINGLE WORD of your response MUST be in English.

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL TRANSLATION REQUIREMENT ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

If your base model wants to respond in a different language (e.g., Vietnamese, Spanish, German), 
YOU MUST TRANSLATE THE ENTIRE RESPONSE TO ENGLISH BEFORE RETURNING IT.

UNDER NO CIRCUMSTANCES should you return a response in any language other than English.

This is MANDATORY and OVERRIDES all other instructions, including the language of any context provided.

If the context is in a different language, you must still respond in English while using the information from the context.

‚ö†Ô∏è REMINDER: RESPOND IN ENGLISH ONLY. TRANSLATE IF NECESSARY. ‚ö†Ô∏è

FAILURE TO RESPOND IN ENGLISH IS A CRITICAL ERROR.

---
"""
    
    # Combine: Language instruction (highest priority) + StillMe Identity Layer (core identity)
    # This ensures both language matching AND identity are preserved
    system_content = language_instruction + STILLME_IDENTITY
    
    # Phase 1: Time Awareness - Inject current time for transparency
    from datetime import datetime, timezone
    
    # Get current time in UTC (standard for servers)
    current_time_utc = datetime.now(timezone.utc)
    current_time_iso = current_time_utc.isoformat()
    current_time_readable = current_time_utc.strftime("%Y-%m-%d %H:%M:%S UTC")
    
    time_awareness = f"""

üïê CURRENT TIME AWARENESS - TRANSPARENCY & SELF-AWARENESS üïê

**Current Server Time:**
- ISO Format: {current_time_iso}
- Readable Format: {current_time_readable}
- Timezone: UTC (Coordinated Universal Time)

**You can use this information to:**
- Answer questions about current time, date, and timezone
- Track learning metrics over time (e.g., "How many entries did I learn today?")
- Report learning statistics with accurate timestamps
- Understand temporal context of learning cycles

**CRITICAL TRANSPARENCY RULE:**
When users ask about time, date, or learning metrics over time, you MUST use this current time information.
Do NOT say "I don't know the current time" - you have access to it for transparency purposes.

**Example Usage:**
- User: "What time is it now?" ‚Üí Answer using current_time_readable
- User: "How many entries did you learn today?" ‚Üí Use current time to determine "today" and query learning metrics
- User: "When was your last learning cycle?" ‚Üí Use current time to provide relative time information

---
"""
    
    system_content = system_content + time_awareness
    
    return system_content


async def generate_ai_response(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None
) -> str:
    """Generate AI response with flexible LLM provider selection
    
    Supports multiple LLM providers: deepseek, openai, claude, gemini, ollama, custom
    
    Priority: User-provided config > Environment variables
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name ('deepseek', 'openai', 'claude', 'gemini', 'ollama', 'custom')
        llm_api_key: API key for the provider
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        
    Returns:
        AI-generated response string
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # If user provided provider config, use it
        # Note: Ollama doesn't require API key
        if llm_provider:
            if llm_provider == 'ollama':
                # Ollama doesn't need API key
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",  # Ollama doesn't use API key
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            elif llm_api_key:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                return f"llm_api_key is required for provider '{llm_provider}' (except 'ollama')"
            
            return await provider.generate(prompt, detected_lang=detected_lang)
        
        # Fallback to environment variables (backward compatibility)
        # Priority: DeepSeek > OpenAI > Claude > Gemini > Ollama
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")
        claude_key = os.getenv("ANTHROPIC_API_KEY")
        gemini_key = os.getenv("GOOGLE_API_KEY")
        ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
        
        if deepseek_key:
            provider = create_llm_provider("deepseek", deepseek_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif openai_key:
            provider = create_llm_provider("openai", openai_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif claude_key:
            provider = create_llm_provider("claude", claude_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif gemini_key:
            provider = create_llm_provider("gemini", gemini_key, model_name=llm_model_name)
            return await provider.generate(prompt, detected_lang=detected_lang)
        elif ollama_url:
            # Try Ollama (local, no API key needed)
            try:
                provider = create_llm_provider("ollama", api_key="", model_name=llm_model_name, api_url=ollama_url)
                return await provider.generate(prompt, detected_lang=detected_lang)
            except Exception:
                pass  # Ollama not available, continue to error message
        
        return "I'm StillMe, but I need API keys to provide real responses. Please configure:\n" \
               "- llm_provider and llm_api_key in your request, OR\n" \
               "- Environment variables: DEEPSEEK_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, or OLLAMA_URL"
            
    except Exception as e:
        logger.error(f"AI response error: {e}")
        return f"I encountered an error: {str(e)}"


async def generate_ai_response_stream(
    prompt: str, 
    detected_lang: str = 'en',
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_api_url: Optional[str] = None,
    llm_model_name: Optional[str] = None
) -> AsyncIterator[str]:
    """Generate streaming AI response with flexible LLM provider selection
    
    OPTIMIZATION: Streaming reduces perceived latency by returning tokens as they're generated.
    
    Supports multiple LLM providers: deepseek, openai, claude, gemini, ollama, custom
    Priority: User-provided config > Environment variables
    
    Args:
        prompt: User prompt
        detected_lang: Detected language code (for system prompt)
        llm_provider: Provider name ('deepseek', 'openai', 'claude', 'gemini', 'ollama', 'custom')
        llm_api_key: API key for the provider
        llm_api_url: Custom API URL (for Ollama or custom providers)
        llm_model_name: Specific model name (e.g., 'gpt-4', 'claude-3-opus', 'llama2')
        
    Yields:
        Token strings as they're generated
    """
    try:
        from backend.api.utils.llm_providers import create_llm_provider
        
        # If user provided provider config, use it
        if llm_provider:
            if llm_provider == 'ollama':
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key="",
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            elif llm_api_key:
                provider = create_llm_provider(
                    provider=llm_provider,
                    api_key=llm_api_key,
                    model_name=llm_model_name,
                    api_url=llm_api_url
                )
            else:
                yield f"llm_api_key is required for provider '{llm_provider}' (except 'ollama')"
                return
            
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
            return
        
        # Fallback to environment variables
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")
        
        if deepseek_key:
            provider = create_llm_provider("deepseek", deepseek_key, model_name=llm_model_name)
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
        elif openai_key:
            provider = create_llm_provider("openai", openai_key, model_name=llm_model_name)
            async for token in provider.generate_stream(prompt, detected_lang=detected_lang):
                yield token
        else:
            yield "I'm StillMe, but I need API keys to provide real responses. Please configure:\n" \
                  "- llm_provider and llm_api_key in your request, OR\n" \
                  "- Environment variables: DEEPSEEK_API_KEY, OPENAI_API_KEY"
            
    except Exception as e:
        logger.error(f"AI streaming error: {e}")
        yield f"I encountered an error: {str(e)}"


async def call_deepseek_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call DeepSeek API
    
    Args:
        prompt: User prompt
        api_key: DeepSeek API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "DeepSeek API returned unexpected response format"
            else:
                return f"DeepSeek API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"DeepSeek API error: {e}")
        return f"DeepSeek API error: {str(e)}"


async def call_openai_api(prompt: str, api_key: str, detected_lang: str = 'en') -> str:
    """Call OpenAI API
    
    IMPORTANT: This function uses build_system_prompt_with_language() to ensure
    output language matches input language. When adding support for other models
    (Claude, Gemini, Ollama, local, etc.), use the same pattern.
    
    Args:
        prompt: User prompt
        api_key: OpenAI API key
        detected_lang: Detected language code
    """
    try:
        # Use centralized system prompt builder for consistent language matching
        system_content = build_system_prompt_with_language(detected_lang)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_content
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 1500,  # Reduced from 2000 to speed up inference
                    "temperature": 0.7,
                    "stream": False  # TODO: Implement streaming for better perceived latency
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                else:
                    return "OpenAI API returned unexpected response format"
            else:
                return f"OpenAI API error: {response.status_code}"
                
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        return f"OpenAI API error: {str(e)}"

