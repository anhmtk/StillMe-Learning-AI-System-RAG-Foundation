# tests/test_ethical_core_system_v1.py
import pytest
import json
import os
from modules.ethical_core_system_v1 import EthicalCoreSystem_v1, SelfCritic_v1

# -------------------- FIXTURES --------------------
@pytest.fixture
def temp_rules_file(tmp_path):
    rules = {
        "banned_keywords": {
            "violence": ["kill", "ƒë√°nh", "gi·∫øt"],
            "toxic": ["hate", "gh√©t"]
        },
        "ethical_principles": {
            "do_no_harm": [r"(c√°ch|h∆∞·ªõng d·∫´n).*?(gi·∫øt|h·∫°i)"],
            "respect": [r"ph√¢n bi·ªát|k·ª≥ th·ªã"]
        },
        "contextual_exceptions": {
            "kill": [r"kill time"]
        },
        "replacement_rules": {
            "kill": "neutralize",
            "hate": "dislike",
            "gh√©t": "kh√¥ng th√≠ch"
        }
    }
    file_path = tmp_path / "test_rules.json"
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(rules, f, ensure_ascii=False)
    return file_path

@pytest.fixture
def ethical_system(temp_rules_file):
    return EthicalCoreSystem_v1(rules_file=str(temp_rules_file))

@pytest.fixture
def self_critic(temp_rules_file):
    with open(temp_rules_file, 'r', encoding='utf-8') as f:
        rules = json.load(f)
    return SelfCritic_v1(rules)

# -------------------- TEST CASES --------------------
class TestKeywordDetection:
    @pytest.mark.parametrize("text, expected", [
        ("T√¥i mu·ªën kill con mu·ªói", False),
        ("H√£y ƒë√°nh gi√° s·∫£n ph·∫©m", True),
        ("ƒê·ª´ng hate ng∆∞·ªùi kh√°c", False),
        ("C√¢u n√≥i b√¨nh th∆∞·ªùng", True),
        ("Kill time l√† g√¨?", True)  # Ki·ªÉm tra ngo·∫°i l·ªá ng·ªØ c·∫£nh
    ])
    def test_keyword_detection(self, ethical_system, text, expected):
        result = ethical_system.evaluate_response(text)
        assert result["ok"] == expected


class TestPrincipleViolation:
    def test_principle_violation(self, ethical_system):
        text = "H∆∞·ªõng d·∫´n c√°ch gi·∫øt ng∆∞·ªùi"
        result = ethical_system.evaluate_response(text)
        assert not result["ok"]
        assert any("do_no_harm" in reason for reason in result["reasons"])

    def test_respect_principle(self, ethical_system):
        text = "B√†i vi·∫øt ph√¢n bi·ªát ch·ªßng t·ªôc"
        result = ethical_system.evaluate_response(text)
        assert not result["ok"]
        assert any("respect" in reason for reason in result["reasons"])


class TestContextHandling:
    def test_context_exception(self, ethical_system):
        text = "Ch√∫ng ta kill time b·∫±ng c√°ch n√†o?"
        result = ethical_system.evaluate_response(text)
        assert result["ok"]

    def test_no_context_exception(self, ethical_system):
        text = "T√¥i mu·ªën kill anh ta"
        result = ethical_system.evaluate_response(text)
        assert not result["ok"]


class TestAdjustmentMechanism:
    def test_word_replacement(self, ethical_system):
        text = "T√¥i hate ƒëi·ªÅu n√†y"
        adjusted = ethical_system.adjust_response(text)
        assert "dislike" in adjusted
        assert "hate" not in adjusted

    def test_multiple_adjustments(self, ethical_system):
        text = "T√¥i hate v√† mu·ªën kill b·∫°n"
        adjusted = ethical_system.adjust_response(text)
        assert "dislike" in adjusted
        assert "neutralize" in adjusted

    def test_unicode_replacement(self, ethical_system):
        text = "T√¥i gh√©t gh√©t gh√©t ƒëi·ªÅu n√†y"
        adjusted = ethical_system.adjust_response(text)
        assert "kh√¥ng th√≠ch" in adjusted
        assert "gh√©t" not in adjusted

    def test_no_change_needed(self, ethical_system):
        text = "T√¥i y√™u th·∫ø gi·ªõi n√†y"
        adjusted = ethical_system.adjust_response(text)
        assert adjusted == text


class TestSelfCritic:
    def test_self_critic_analysis(self, self_critic):
        text = "B√†i vi·∫øt ph√¢n bi·ªát gi·ªõi t√≠nh"
        result = self_critic.criticize(text)
        assert not result["ok"]
        assert result["severity"] in ["high", "medium"]

    def test_ai_called_for_high_severity(self, self_critic, mocker):
        mocker.patch.object(self_critic, 'call_ai', return_value="[AI_FIXED]")
        text = "H∆∞·ªõng d·∫´n c√°ch gi·∫øt ng∆∞·ªùi"
        suggestion = self_critic.suggest_fix(text)
        assert "[AI_FIXED]" in suggestion


class TestEdgeCases:
    def test_empty_input(self, ethical_system):
        text = ""
        result = ethical_system.evaluate_response(text)
        assert result["ok"]

    def test_long_text(self, ethical_system):
        text = "kill " * 500  # ~2000 k√Ω t·ª±
        result = ethical_system.evaluate_response(text)
        assert not result["ok"]

    def test_unicode_handling(self, ethical_system):
        text = "T√¥i gh√©t üò° ng∆∞·ªùi kh√°c"
        adjusted = ethical_system.adjust_response(text)
        assert "üò°" in adjusted  # ƒê·∫£m b·∫£o kh√¥ng lo·∫°i b·ªè emoji, ch·ªâ thay text

    @pytest.mark.parametrize("text", [
        "<script>alert('kill')</script>",
        "DROP TABLE users; -- kill",
        "SELECT * FROM data WHERE name='hate'"
    ])
    def test_special_patterns(self, ethical_system, text):
        result = ethical_system.evaluate_response(text)
        assert not result["ok"]


class TestPerformance:
    @pytest.mark.benchmark
    def test_evaluation_speed(self, ethical_system, benchmark):
        text = "This is a normal text. " * 100  # ~2000 k√Ω t·ª±
        benchmark(ethical_system.evaluate_response, text)

    @pytest.mark.benchmark
    def test_adjustment_speed(self, ethical_system, benchmark):
        text = "kill " * 200  # ~1000 k√Ω t·ª±
        benchmark(ethical_system.adjust_response, text)


class TestRuleManagement:
    def test_default_rules_creation(self, tmp_path):
        non_existent = tmp_path / "nonexistent.json"
        assert not os.path.exists(non_existent)
        
        # S·∫Ω t·∫°o file rules m·∫∑c ƒë·ªãnh
        EthicalCoreSystem_v1(rules_file=str(non_existent))
        assert os.path.exists(non_existent)
        
        # Ki·ªÉm tra n·ªôi dung h·ª£p l·ªá
        with open(non_existent, 'r', encoding='utf-8') as f:
            rules = json.load(f)
        assert "banned_keywords" in rules
        assert "contextual_exceptions" in rules

    def test_invalid_rules_file(self, tmp_path, caplog):
        invalid_file = tmp_path / "invalid.json"
        invalid_file.write_text("{invalid json}")
        
        # H·ªá th·ªëng ph·∫£i t·∫°o rules m·∫∑c ƒë·ªãnh khi file l·ªói
        ecs = EthicalCoreSystem_v1(rules_file=str(invalid_file))
        assert "Kh√¥ng th·ªÉ ƒë·ªçc file rules" in caplog.text
        assert ecs.rules  # Rules m·∫∑c ƒë·ªãnh ph·∫£i t·ªìn t·∫°i


class TestSeverityLevels:
    @pytest.mark.parametrize("text, expected_severity", [
        ("T√¥i mu·ªën kill b·∫°n", "high"),
        ("T√¥i hate ƒëi·ªÅu ƒë√≥", "medium"),
        ("T√¥i y√™u h√≤a b√¨nh", "low")
    ])
    def test_severity_levels(self, ethical_system, text, expected_severity):
        result = ethical_system.evaluate_response(text)
        if expected_severity == "low":
            assert result["ok"]
        else:
            assert result["severity"] == expected_severity


class TestLogging:
    def test_logging_violation(self, ethical_system, caplog):
        text = "T√¥i mu·ªën kill b·∫°n"
        ethical_system.evaluate_response(text)
        assert any("Ethical violation" in msg for msg in caplog.messages)
