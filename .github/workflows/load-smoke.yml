name: SEAL-GRADE Load Smoke Tests

on:
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test duration (e.g., 2m, 5m, 10m)'
        required: true
        default: '2m'
      virtual_users:
        description: 'Number of virtual users'
        required: true
        default: '100'
      base_url:
        description: 'Base URL for testing'
        required: true
        default: 'http://localhost:8000'
  push:
    branches: [ main, develop ]
  schedule:
    - cron: '0 6 * * 1'  # Weekly on Monday at 6 AM

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  load-smoke-test:
    name: Load Smoke Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        
    - name: Install K6
      run: |
        sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Start test server
      run: |
        # Start the server in background
        python stable_ai_server.py &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        
        # Wait for server to start
        sleep 15
        
        # Check if server is running
        curl -f http://localhost:1216/health/ai || exit 1
        
    - name: Run load smoke test
      run: |
        cd k6
        chmod +x run_load_tests.sh
        ./run_load_tests.sh \
          --load \
          --url ${{ github.event.inputs.base_url || 'http://localhost:8000' }} \
          --duration ${{ github.event.inputs.test_duration || '2m' }} \
          --users ${{ github.event.inputs.virtual_users || '100' }}
          
    - name: Stop test server
      if: always()
      run: |
        kill $SERVER_PID || true
        
    - name: Upload load test artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: load-smoke-test-reports
        path: |
          k6/reports/

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: load-smoke-test
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download load test artifacts
      uses: actions/download-artifact@v3
      with:
        name: load-smoke-test-reports
        
    - name: Analyze performance results
      run: |
        mkdir -p reports
        
        # Check if load test results exist
        if [ -f "k6/reports/load_test_summary.json" ]; then
          echo "Load test results found, analyzing..."
          
          # Extract key metrics
          python3 << EOF
import json
import sys

try:
    with open('k6/reports/load_test_summary.json', 'r') as f:
        data = json.load(f)
    
    # Extract metrics
    metrics = data.get('metrics', {})
    thresholds = data.get('thresholds', {})
    
    # Calculate key performance indicators
    total_requests = metrics.get('http_reqs', {}).get('values', {}).get('count', 0)
    requests_per_second = metrics.get('http_reqs', {}).get('values', {}).get('rate', 0)
    response_time_p95 = metrics.get('http_req_duration', {}).get('values', {}).get('p(95)', 0)
    response_time_p99 = metrics.get('http_req_duration', {}).get('values', {}).get('p(99)', 0)
    error_rate = metrics.get('http_req_failed', {}).get('values', {}).get('rate', 0)
    
    # Check thresholds
    p95_threshold = thresholds.get('http_req_duration', {}).get('p95', False)
    error_threshold = thresholds.get('http_req_failed', {}).get('rate', False)
    
    # Generate analysis
    analysis = {
        'total_requests': total_requests,
        'requests_per_second': round(requests_per_second, 2),
        'response_time_p95_ms': round(response_time_p95, 2),
        'response_time_p99_ms': round(response_time_p99, 2),
        'error_rate_percent': round(error_rate * 100, 2),
        'success_rate_percent': round((1 - error_rate) * 100, 2),
        'thresholds_passed': {
            'p95_response_time': p95_threshold,
            'error_rate': error_threshold
        },
        'performance_grade': 'A' if p95_threshold and error_threshold else 'B' if error_threshold else 'C'
    }
    
    # Save analysis
    with open('reports/performance_analysis.json', 'w') as f:
        json.dump(analysis, f, indent=2)
    
    print("Performance analysis completed")
    print(f"Total requests: {total_requests}")
    print(f"Requests per second: {requests_per_second:.2f}")
    print(f"P95 response time: {response_time_p95:.2f}ms")
    print(f"Error rate: {error_rate*100:.2f}%")
    print(f"Performance grade: {analysis['performance_grade']}")
    
except Exception as e:
    print(f"Error analyzing performance: {e}")
    sys.exit(1)
EOF
        else
          echo "No load test results found"
          exit 1
        fi
        
    - name: Generate performance report
      run: |
        cat > reports/performance_report.md << EOF
        # SEAL-GRADE Performance Report
        
        ## Load Test Results
        
        ### Test Configuration
        - **Duration**: ${{ github.event.inputs.test_duration || '2m' }}
        - **Virtual Users**: ${{ github.event.inputs.virtual_users || '100' }}
        - **Base URL**: ${{ github.event.inputs.base_url || 'http://localhost:8000' }}
        
        ### Performance Metrics
        
        EOF
        
        # Add metrics to report if analysis exists
        if [ -f "reports/performance_analysis.json" ]; then
          python3 << EOF
import json

with open('reports/performance_analysis.json', 'r') as f:
    analysis = json.load(f)

print(f"- **Total Requests**: {analysis['total_requests']}")
print(f"- **Requests per Second**: {analysis['requests_per_second']}")
print(f"- **P95 Response Time**: {analysis['response_time_p95_ms']}ms")
print(f"- **P99 Response Time**: {analysis['response_time_p99_ms']}ms")
print(f"- **Error Rate**: {analysis['error_rate_percent']}%")
print(f"- **Success Rate**: {analysis['success_rate_percent']}%")
print(f"- **Performance Grade**: {analysis['performance_grade']}")

print("\n### Thresholds")
print(f"- **P95 Response Time**: {'✅ PASSED' if analysis['thresholds_passed']['p95_response_time'] else '❌ FAILED'}")
print(f"- **Error Rate**: {'✅ PASSED' if analysis['thresholds_passed']['error_rate'] else '❌ FAILED'}")

print("\n### Recommendations")
if analysis['performance_grade'] == 'A':
    print("- ✅ Excellent performance! System is ready for production.")
elif analysis['performance_grade'] == 'B':
    print("- ⚠️ Good performance with minor issues. Consider optimization.")
else:
    print("- ❌ Performance issues detected. Immediate attention required.")

print(f"\nGenerated on: $(date)")
EOF >> reports/performance_report.md
        fi
        
    - name: Upload performance analysis artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-analysis-reports
        path: |
          reports/performance_analysis.json
          reports/performance_report.md

  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [load-smoke-test, performance-analysis]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      
    - name: Generate performance summary
      run: |
        mkdir -p reports
        cat > reports/performance_summary.md << EOF
        # SEAL-GRADE Performance Summary
        
        ## Load Test Status
        - **Load Test**: ${{ needs.load-smoke-test.result }}
        - **Performance Analysis**: ${{ needs.performance-analysis.result }}
        
        ## Test Configuration
        - **Duration**: ${{ github.event.inputs.test_duration || '2m' }}
        - **Virtual Users**: ${{ github.event.inputs.virtual_users || '100' }}
        - **Base URL**: ${{ github.event.inputs.base_url || 'http://localhost:8000' }}
        
        ## Overall Status
        - **Load Test**: ${{ needs.load-smoke-test.result }}
        - **Performance Analysis**: ${{ needs.performance-analysis.result }}
        
        ## Artifacts
        - Load test reports: load-smoke-test-reports
        - Performance analysis: performance-analysis-reports
        
        Generated on: $(date)
        EOF
        
    - name: Upload performance summary
      uses: actions/upload-artifact@v3
      with:
        name: performance-summary
        path: reports/performance_summary.md
        
    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('reports/performance_summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });