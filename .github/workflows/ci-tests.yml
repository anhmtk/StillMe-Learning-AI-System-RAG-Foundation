name: CI Tests

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11, 3.12]
        test-type: [unit, integration, security]

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html pytest-xdist
        pip install bandit safety semgrep
    
    - name: Lint with ruff
      run: |
        pip install ruff
        ruff check . --fix --force-exclude
    
    - name: Type check with pyright
      run: |
        pip install pyright
        pyright --stats --ignoreexternal stillme_core/ tests/
    
    - name: Run unit tests
      if: matrix.test-type == 'unit'
      run: |
        pytest -m "unit" --cov=stillme_core --cov-report=xml --cov-report=html --junitxml=test-results-unit.xml --html=test-report-unit.html --self-contained-html
    
    - name: Run integration tests
      if: matrix.test-type == 'integration'
      run: |
        pytest -m "integration" --cov=stillme_core --cov-report=xml --cov-report=html --junitxml=test-results-integration.xml --html=test-report-integration.html --self-contained-html
    
    - name: Run security tests
      if: matrix.test-type == 'security'
      run: |
        pytest -m "security" --junitxml=test-results-security.xml --html=test-report-security.html --self-contained-html
    
    - name: Upload coverage to Codecov
      if: matrix.test-type == 'unit'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.test-type }}
        path: |
          test-results-*.xml
          test-report-*.html
          coverage.xml
          htmlcov/

  reflex-engine-specific:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html
    
    - name: Run Reflex Engine specific tests
      run: |
        pytest tests/test_reflex_engine_shadow.py tests/test_pattern_matcher.py tests/test_reflex_policy.py tests/test_reflex_safety.py tests/test_action_sandbox.py tests/test_habit_store.py tests/test_observability.py tests/test_reflex_engine_integration.py --cov=stillme_core.middleware --cov-report=xml --cov-report=html --junitxml=test-results-reflex.xml --html=test-report-reflex.html --self-contained-html
    
    - name: Generate shadow evaluation report
      run: |
        python scripts/generate_shadow_report.py --output docs/REFLEX_SHADOW_EVAL.md
    
    - name: Upload Reflex Engine test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: reflex-engine-test-results
        path: |
          test-results-reflex.xml
          test-report-reflex.html
          coverage.xml
          htmlcov/
          docs/REFLEX_SHADOW_EVAL.md

  performance:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark
    
    - name: Run performance benchmarks
      run: |
        pytest tests/ -k "benchmark" --benchmark-only --benchmark-save=reflex-engine-benchmark --benchmark-save-data
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: |
          .benchmarks/