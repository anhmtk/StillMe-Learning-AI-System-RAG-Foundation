#!/usr/bin/env python3
"""
ğŸš€ STILLME AI - Phi-4-mini Setup Script
Má»¥c tiÃªu: KÃ©o model Phi-4-mini chÃ­nh thá»©c tá»« Ollama vÃ  warmup
"""

import subprocess
import sys
import time
import requests
import json
from typing import Dict, Any, Optional

def check_ollama_version() -> tuple[bool, str]:
    """Kiá»ƒm tra phiÃªn báº£n Ollama"""
    try:
        result = subprocess.run(['ollama', '-v'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            version_str = result.stdout.strip()
            print(f"   Raw version output: {version_str}")
            
            # Extract version number - handle "ollama version is 0.11.8"
            import re
            version_match = re.search(r'(\d+\.\d+\.\d+)', version_str)
            if version_match:
                version = version_match.group(1)
                major, minor, patch = map(int, version.split('.'))
                required_major, required_minor, required_patch = 0, 5, 13
                
                if (major > required_major or 
                    (major == required_major and minor > required_minor) or
                    (major == required_major and minor == required_minor and patch >= required_patch)):
                    return True, version
                else:
                    return False, version
            else:
                return False, "Version format not recognized"
        return False, "Unknown"
    except Exception as e:
        print(f"âŒ Lá»—i kiá»ƒm tra phiÃªn báº£n Ollama: {e}")
        return False, "Error"

def pull_phi4_mini() -> bool:
    """Pull model phi4-mini tá»« Ollama"""
    print("ğŸ”„ Äang pull model phi4-mini...")
    try:
        # Sá»­ dá»¥ng encoding utf-8 vÃ  tÄƒng timeout
        result = subprocess.run(['ollama', 'pull', 'phi4-mini'], 
                              capture_output=True, text=True, 
                              encoding='utf-8', errors='ignore',
                              timeout=600)  # 10 phÃºt timeout
        
        if result.returncode == 0:
            print("âœ… Pull phi4-mini thÃ nh cÃ´ng!")
            return True
        else:
            print(f"âŒ Lá»—i pull phi4-mini: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("â° Timeout khi pull phi4-mini (model cÃ³ thá»ƒ ráº¥t lá»›n)")
        print("   Vui lÃ²ng cháº¡y thá»§ cÃ´ng: ollama pull phi4-mini")
        return False
    except Exception as e:
        print(f"âŒ Lá»—i pull phi4-mini: {e}")
        return False

def warmup_phi4_mini() -> bool:
    """Warmup model phi4-mini vá»›i 8 tokens"""
    print("ğŸ”¥ Äang warmup phi4-mini...")
    
    warmup_payload = {
        "model": "phi4-mini",
        "prompt": "Hello, how are you?",
        "stream": False,
        "options": {
            "num_predict": 8,
            "temperature": 0.1,
            "keep_alive": "1h"
        }
    }
    
    try:
        response = requests.post("http://localhost:11434/api/generate", 
                               json=warmup_payload, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… Warmup thÃ nh cÃ´ng! Response: {result.get('response', '')[:50]}...")
            return True
        else:
            print(f"âŒ Lá»—i warmup: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Lá»—i warmup phi4-mini: {e}")
        return False

def check_model_ready() -> Dict[str, Any]:
    """Kiá»ƒm tra model cÃ³ sáºµn sÃ ng trong /api/ps"""
    try:
        response = requests.get("http://localhost:11434/api/ps", timeout=10)
        if response.status_code == 200:
            models = response.json()
            for model in models.get('models', []):
                if model.get('name', '').startswith('phi4-mini'):
                    return {
                        'ready': True,
                        'name': model.get('name'),
                        'size': model.get('size'),
                        'expires_at': model.get('expires_at')
                    }
            return {'ready': False, 'reason': 'Model not found in /api/ps'}
        else:
            return {'ready': False, 'reason': f'HTTP {response.status_code}'}
    except Exception as e:
        return {'ready': False, 'reason': str(e)}

def get_model_info() -> Optional[Dict[str, Any]]:
    """Láº¥y thÃ´ng tin chi tiáº¿t vá» model"""
    try:
        response = requests.post("http://localhost:11434/api/show", 
                               json={"name": "phi4-mini"}, timeout=10)
        if response.status_code == 200:
            return response.json()
        return None
    except Exception as e:
        print(f"âš ï¸ KhÃ´ng thá»ƒ láº¥y model info: {e}")
        return None

def main():
    print("ğŸš€ STILLME AI - Phi-4-mini Setup")
    print("=" * 50)
    
    # 1. Kiá»ƒm tra phiÃªn báº£n Ollama
    print("1ï¸âƒ£ Kiá»ƒm tra phiÃªn báº£n Ollama...")
    version_ok, version = check_ollama_version()
    print(f"   PhiÃªn báº£n Ollama: {version}")
    
    if not version_ok:
        print("âš ï¸ WARNING: Phi-4-mini requires Ollama â‰¥ 0.5.13")
        print("   Vui lÃ²ng nÃ¢ng cáº¥p Ollama trÆ°á»›c khi tiáº¿p tá»¥c!")
        return False
    else:
        print("âœ… PhiÃªn báº£n Ollama phÃ¹ há»£p!")
    
    # 2. Pull model
    print("\n2ï¸âƒ£ Pull model phi4-mini...")
    if not pull_phi4_mini():
        return False
    
    # 3. Warmup model
    print("\n3ï¸âƒ£ Warmup model phi4-mini...")
    if not warmup_phi4_mini():
        print("âš ï¸ Warmup tháº¥t báº¡i, nhÆ°ng model Ä‘Ã£ Ä‘Æ°á»£c pull")
    
    # 4. Kiá»ƒm tra model ready
    print("\n4ï¸âƒ£ Kiá»ƒm tra model ready...")
    model_status = check_model_ready()
    if model_status['ready']:
        print(f"âœ… Model ready: {model_status['name']}")
        if model_status.get('size'):
            size_mb = model_status['size'] / (1024 * 1024)
            print(f"   KÃ­ch thÆ°á»›c: {size_mb:.1f} MB")
    else:
        print(f"âŒ Model chÆ°a ready: {model_status['reason']}")
    
    # 5. Láº¥y thÃ´ng tin model
    print("\n5ï¸âƒ£ ThÃ´ng tin model...")
    model_info = get_model_info()
    if model_info:
        print(f"   Model: {model_info.get('modelfile', 'N/A')[:100]}...")
        if model_info.get('size'):
            size_gb = model_info['size'] / (1024 * 1024 * 1024)
            print(f"   Dung lÆ°á»£ng Æ°á»›c tÃ­nh: {size_gb:.2f} GB")
            
            # Cáº£nh bÃ¡o RAM/VRAM
            if size_gb > 8:
                print("âš ï¸ Cáº¢NH BÃO: Model lá»›n (>8GB), cáº§n RAM/VRAM Ä‘á»§")
            elif size_gb > 4:
                print("âš ï¸ LÆ°u Ã½: Model trung bÃ¬nh (4-8GB), kiá»ƒm tra RAM/VRAM")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ DONE P1 - Phi-4-mini Setup Complete!")
    print(f"   âœ… Ollama version: {version}")
    print(f"   âœ… Model ready: {model_status['ready']}")
    print(f"   âœ… Keep-alive: 1h")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
