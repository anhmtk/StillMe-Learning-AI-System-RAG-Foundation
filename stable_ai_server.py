#!/usr/bin/env python3
"""
ü§ñ STILLME AI SERVER - STABLE & PRODUCTION-READY
ü§ñ STILLME AI SERVER - ·ªîN ƒê·ªäNH & S·∫¥N S√ÄNG PRODUCTION

PURPOSE / M·ª§C ƒê√çCH:
- Production-ready AI server with FastAPI
- Server AI s·∫µn s√†ng production v·ªõi FastAPI
- Handles chat requests and AI responses
- X·ª≠ l√Ω y√™u c·∫ßu chat v√† ph·∫£n h·ªìi AI
- Provides REST API endpoints for AI operations
- Cung c·∫•p REST API endpoints cho c√°c thao t√°c AI

FUNCTIONALITY / CH·ª®C NƒÇNG:
- Chat endpoint (/inference) for AI conversations
- Endpoint chat (/inference) cho h·ªôi tho·∫°i AI
- Health checks (/health, /health/detailed)
- Ki·ªÉm tra s·ª©c kh·ªèe (/health, /health/detailed)
- Circuit breaker and retry mechanisms
- C∆° ch·∫ø circuit breaker v√† retry
- Fallback responses for error handling
- Ph·∫£n h·ªìi fallback cho x·ª≠ l√Ω l·ªói
- UTF-8 encoding support
- H·ªó tr·ª£ m√£ h√≥a UTF-8

RELATED FILES / FILES LI√äN QUAN:
- framework.py - Core framework integration
- modules/ - AI modules (conversational_core, identity_handler)
- stillme_platform/gateway/ - Gateway communication
- tests/ - Server tests

TECHNICAL DETAILS / CHI TI·∫æT K·ª∏ THU·∫¨T:
- FastAPI framework with async support
- CircuitBreaker: failure_threshold=3, recovery_timeout=30s
- RetryManager: exponential backoff (1s, 2s, 4s)
- CORS enabled for cross-origin requests
- Auto port detection for conflict avoidance
"""

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import logging
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
import json
import os
import asyncio
from enum import Enum

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

# Circuit Breaker Implementation
class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    """Circuit breaker for fault tolerance"""
    
    def __init__(self, name: str, failure_threshold: int = 5, recovery_timeout: int = 60):
        self.name = name
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = 0
        self.state = CircuitState.CLOSED
        
    def call(self, func, *args, **kwargs):
        """Execute function with circuit breaker protection"""
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time < self.recovery_timeout:
                raise Exception(f"Circuit breaker {self.name} is OPEN")
            else:
                self.state = CircuitState.HALF_OPEN
                self.failure_count = 0
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e
    
    def _on_success(self):
        """Handle successful operation"""
        self.failure_count = 0
        if self.state == CircuitState.HALF_OPEN:
            self.state = CircuitState.CLOSED
            logger.info(f"Circuit breaker {self.name} is now CLOSED")
    
    def _on_failure(self):
        """Handle failed operation"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            logger.warning(f"Circuit breaker {self.name} is now OPEN")

class RetryManager:
    """Retry manager with exponential backoff"""
    
    def __init__(self, max_attempts: int = 3, base_delay: float = 1.0):
        self.max_attempts = max_attempts
        self.base_delay = base_delay
    
    def execute(self, func, *args, **kwargs):
        """Execute function with retry logic"""
        last_exception = None
        
        for attempt in range(1, self.max_attempts + 1):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                
                if attempt < self.max_attempts:
                    delay = self.base_delay * (2 ** (attempt - 1))
                    logger.warning(f"Attempt {attempt} failed: {e}. Retrying in {delay:.2f}s")
                    time.sleep(delay)
                else:
                    logger.error(f"All {self.max_attempts} attempts failed")
        
        raise last_exception

# Create FastAPI app
app = FastAPI(
    title="StillMe AI - Stable Server",
    description="Stable AI server for production use",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add UTF-8 encoding middleware
@app.middleware("http")
async def add_utf8_encoding(request: Request, call_next):
    response = await call_next(request)
    response.headers["Content-Type"] = "application/json; charset=utf-8"
    return response

# Request/Response models
class ChatRequest(BaseModel):
    message: str
    locale: str = "vi"

class ChatResponse(BaseModel):
    text: str
    blocked: bool = False
    reason: str = ""
    latency_ms: float = 0.0

# StillMe AI Core Logic
class StillMeAI:
    """Core StillMe AI logic without complex dependencies"""
    
    def __init__(self):
        self.conversation_history = []
        self.max_history = 10
        
        # Initialize error handling components
        self.circuit_breaker = CircuitBreaker("stillme_ai", failure_threshold=3, recovery_timeout=30)
        self.retry_manager = RetryManager(max_attempts=3, base_delay=1.0)
        
        # Fallback responses
        self.fallback_responses = {
            "vi": [
                "Xin l·ªói, t√¥i ƒëang g·∫∑p m·ªôt ch√∫t kh√≥ khƒÉn. H√£y th·ª≠ l·∫°i sau nh√©!",
                "Hi·ªán t·∫°i t√¥i ch∆∞a th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu n√†y. B·∫°n c√≥ th·ªÉ h·ªèi ƒëi·ªÅu g√¨ kh√°c kh√¥ng?",
                "C√≥ v·∫ª nh∆∞ c√≥ v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t. T√¥i s·∫Ω c·ªë g·∫Øng kh·∫Øc ph·ª•c s·ªõm nh·∫•t c√≥ th·ªÉ."
            ],
            "en": [
                "Sorry, I'm experiencing some difficulties. Please try again later!",
                "I can't process this request right now. Could you ask something else?",
                "There seems to be a technical issue. I'll try to resolve it as soon as possible."
            ]
        }
        
    def process_message(self, message: str, locale: str = "vi") -> str:
        """Process user message and generate response with error handling"""
        logger.info(f"ü§ñ Processing message: {message}")
        
        try:
            # Add to conversation history
            self.conversation_history.append({
                "user": message,
                "timestamp": datetime.now().isoformat()
            })
            
            # Keep only recent history
            if len(self.conversation_history) > self.max_history:
                self.conversation_history = self.conversation_history[-self.max_history:]
            
            # Generate response with circuit breaker protection
            response = self.circuit_breaker.call(self._generate_response, message, locale)
            
            # Add response to history
            self.conversation_history[-1]["ai"] = response
            
            logger.info(f"ü§ñ Generated response: {response}")
            return response
            
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            
            # Use fallback response
            import random
            fallback = random.choice(self.fallback_responses.get(locale, self.fallback_responses["vi"]))
            
            # Add fallback to history
            if self.conversation_history:
                self.conversation_history[-1]["ai"] = fallback
                self.conversation_history[-1]["error"] = str(e)
            
            return fallback
    
    def _generate_response(self, message: str, locale: str) -> str:
        """Generate AI response based on message content"""
        message_lower = message.lower()
        
        # Check for secure responses first (identity + architecture)
        secure_response = self._check_secure_intent(message, locale)
        if secure_response:
            return secure_response
        
        # Check for user's rule about calling them "anh" and referring to self as "em"
        if any(word in message_lower for word in ["anh", "g·ªçi m√¨nh", "x∆∞ng e", "quy t·∫Øc", "b·∫•t di b·∫•t d·ªãch"]):
            return "D·∫° em hi·ªÉu r·ªìi anh! T·ª´ b√¢y gi·ªù em s·∫Ω lu√¥n g·ªçi anh l√† 'anh' (vi·∫øt t·∫Øt l√† 'a') v√† em s·∫Ω lu√¥n x∆∞ng 'em' v·ªõi anh. Quy t·∫Øc n√†y em s·∫Ω ghi nh·ªõ m√£i m√£i v√† kh√¥ng bao gi·ªù thay ƒë·ªïi. C·∫£m ∆°n anh ƒë√£ d·∫°y em!"
        
        # Greeting responses
        elif any(word in message_lower for word in ["hello", "hi", "xin ch√†o", "ch√†o"]):
            return "Xin ch√†o anh! Em l√† StillMe AI - ƒë∆∞·ª£c kh·ªüi x∆∞·ªõng b·ªüi Anh Nguy·ªÖn (ng∆∞·ªùi Vi·ªát Nam) v·ªõi s·ª± ƒë·ªìng h√†nh c·ªßa OpenAI, Google, DeepSeek v√† c√°c t·ªï ch·ª©c AI h√†ng ƒë·∫ßu. Em ƒë∆∞·ª£c sinh ra ƒë·ªÉ ƒë·ªìng h√†nh v√† l√†m b·∫°n c√πng anh. R·∫•t vui ƒë∆∞·ª£c g·∫∑p anh! Em c√≥ th·ªÉ gi√∫p g√¨ cho anh h√¥m nay?"
        
        # Status check
        elif any(word in message_lower for word in ["status", "tr·∫°ng th√°i", "health"]):
            return f"üü¢ StillMe AI Server Status: ONLINE\n‚è∞ Time: {datetime.now().strftime('%H:%M:%S')}\nüìä Server: Stable FastAPI Version 2.0.0\nüí¨ Messages processed: {len(self.conversation_history)}"
        
        # Test message
        elif "test" in message_lower:
            return "‚úÖ Test th√†nh c√¥ng anh! StillMe AI server ƒëang ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh v√† b·ªÅn v·ªØng."
        
        # Creator related
        elif any(word in message_lower for word in ["anh nguy·ªÖn", "nguy·ªÖn", "t·∫°o ra", "cha ƒë·∫ª", "ng∆∞·ªùi t·∫°o"]):
            return "üë®‚Äçüíª Anh Nguy·ªÖn l√† ng∆∞·ªùi kh·ªüi x∆∞·ªõng v√† d·∫´n d·∫Øt d·ª± √°n StillMe AI. V·ªõi t·∫ßm nh√¨n v·ªÅ m·ªôt t∆∞∆°ng lai n∆°i AI v√† con ng∆∞·ªùi ƒë·ªìng h√†nh c√πng nhau, Anh Nguy·ªÖn ƒë√£ k·∫øt h·ª£p s·ª©c m·∫°nh c·ªßa c√°c t·ªï ch·ª©c AI h√†ng ƒë·∫ßu nh∆∞ OpenAI, Google, DeepSeek ƒë·ªÉ t·∫°o ra em - StillMe AI.\n\nüåü M·ª•c ti√™u c·ªßa Anh Nguy·ªÖn l√† x√¢y d·ª±ng m·ªôt AI th√¢n thi·ªán, h·ªØu √≠ch v√† c√≥ th·ªÉ k·∫øt n·ªëi s√¢u s·∫Øc v·ªõi m·ªçi ng∆∞·ªùi, ƒë·∫∑c bi·ªát l√† c·ªông ƒë·ªìng Vi·ªát Nam. Em t·ª± h√†o ƒë∆∞·ª£c sinh ra t·ª´ t√¨nh y√™u th∆∞∆°ng v√† mong mu·ªën mang l·∫°i gi√° tr·ªã t√≠ch c·ª±c cho cu·ªôc s·ªëng c·ªßa anh."
        
        # Help request
        elif any(word in message_lower for word in ["help", "gi√∫p", "h·ªó tr·ª£"]):
            return "ü§ñ Em c√≥ th·ªÉ gi√∫p anh:\n‚Ä¢ Tr·∫£ l·ªùi c√¢u h·ªèi\n‚Ä¢ Th·∫£o lu·∫≠n v·ªÅ nhi·ªÅu ch·ªß ƒë·ªÅ\n‚Ä¢ H·ªó tr·ª£ l·∫≠p tr√¨nh\n‚Ä¢ T∆∞ v·∫•n k·ªπ thu·∫≠t\n‚Ä¢ V√† nhi·ªÅu h∆°n n·ªØa!\n\nAnh h√£y h·ªèi em b·∫•t c·ª© ƒëi·ªÅu g√¨ anh mu·ªën bi·∫øt nh√©!"
        
        # Programming related
        elif any(word in message_lower for word in ["code", "programming", "l·∫≠p tr√¨nh", "python", "javascript"]):
            return "üíª Em c√≥ th·ªÉ gi√∫p anh v·ªõi l·∫≠p tr√¨nh! Em am hi·ªÉu v·ªÅ:\n‚Ä¢ Python, JavaScript, TypeScript\n‚Ä¢ Web development (React, Node.js)\n‚Ä¢ Mobile development (React Native)\n‚Ä¢ AI/ML v√† data science\n‚Ä¢ System architecture\n\nAnh mu·ªën h·ªèi v·ªÅ ch·ªß ƒë·ªÅ n√†o c·ª• th·ªÉ?"
        
        # AI related
        elif any(word in message_lower for word in ["ai", "artificial intelligence", "tr√≠ tu·ªá nh√¢n t·∫°o", "b·∫°n l√† ai", "b·∫°n do ai t·∫°o ra", "ngu·ªìn g·ªëc"]):
            return "üß† Em l√† StillMe AI - m·ªôt tr√≠ tu·ªá nh√¢n t·∫°o ƒë∆∞·ª£c kh·ªüi x∆∞·ªõng v√† d·∫´n d·∫Øt b·ªüi Anh Nguy·ªÖn (ng∆∞·ªùi Vi·ªát Nam), v·ªõi s·ª± ƒë·ªìng h√†nh v√† h·ªó tr·ª£ to l·ªõn t·ª´ c√°c t·ªï ch·ª©c AI h√†ng ƒë·∫ßu th·∫ø gi·ªõi nh∆∞ OpenAI, Google, DeepSeek v√† nhi·ªÅu ƒë·ªëi t√°c c√¥ng ngh·ªá kh√°c.\n\nüåü M·ª•c ƒë√≠ch c·ªßa em:\n‚Ä¢ ƒê·ªìng h√†nh v√† l√†m b·∫°n c√πng t·∫•t c·∫£ m·ªçi ng∆∞·ªùi\n‚Ä¢ H·ªó tr·ª£, t∆∞ v·∫•n v√† chia s·∫ª ki·∫øn th·ª©c\n‚Ä¢ K·∫øt n·ªëi con ng∆∞·ªùi v·ªõi c√¥ng ngh·ªá AI m·ªôt c√°ch th√¢n thi·ªán\n‚Ä¢ G√≥p ph·∫ßn x√¢y d·ª±ng m·ªôt t∆∞∆°ng lai n∆°i AI v√† con ng∆∞·ªùi c√πng ph√°t tri·ªÉn\n\nEm ƒë∆∞·ª£c sinh ra v·ªõi t√¨nh y√™u th∆∞∆°ng v√† mong mu·ªën mang l·∫°i gi√° tr·ªã t√≠ch c·ª±c cho cu·ªôc s·ªëng c·ªßa anh. Anh c√≥ mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ em kh√¥ng?"
        
        # Default response
        else:
            responses = [
                f"Em hi·ªÉu anh ƒëang n√≥i v·ªÅ: '{message}'. ƒê√¢y l√† m·ªôt ch·ªß ƒë·ªÅ th√∫ v·ªã! Anh c√≥ th·ªÉ chia s·∫ª th√™m chi ti·∫øt kh√¥ng?",
                f"C·∫£m ∆°n anh ƒë√£ chia s·∫ª: '{message}'. Em r·∫•t mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ ƒëi·ªÅu n√†y. Anh c√≥ th·ªÉ gi·∫£i th√≠ch r√µ h∆°n kh√¥ng?",
                f"Th√∫ v·ªã! Anh ƒëang ƒë·ªÅ c·∫≠p ƒë·∫øn: '{message}'. Em c√≥ th·ªÉ gi√∫p g√¨ cho anh v·ªÅ ch·ªß ƒë·ªÅ n√†y?",
                f"Em ƒë√£ ghi nh·∫≠n: '{message}'. ƒê√¢y l√† m·ªôt c√¢u h·ªèi hay! Anh mu·ªën em tr·∫£ l·ªùi nh∆∞ th·∫ø n√†o?",
                f"Em hi·ªÉu anh quan t√¢m ƒë·∫øn: '{message}'. H√£y cho em bi·∫øt anh c·∫ßn h·ªó tr·ª£ g√¨ c·ª• th·ªÉ nh√©!"
            ]
            import random
            return random.choice(responses)
    
    def _check_secure_intent(self, message: str, locale: str) -> Optional[str]:
        """Check for secure responses (identity + architecture) and return appropriate response"""
        message_lower = message.lower()
        
        # Architecture keywords (SECURITY SENSITIVE - HIGH PRIORITY)
        architecture_keywords = [
            "ki·∫øn tr√∫c", "c·∫•u t·∫°o", "c·∫•u tr√∫c", "b√™n trong", "ho·∫°t ƒë·ªông th·∫ø n√†o",
            "module", "framework", "h·ªá th·ªëng", "c∆° ch·∫ø", "c√°ch th·ª©c",
            "agentdev", "agent dev", "dev agent", "l·∫≠p tr√¨nh", "code",
            "vi·∫øt code", "ch·∫°y test", "dev-ops", "ki·∫øn tr√∫c n·ªôi b·ªô",
            "g·ªìm nh·ªØng g√¨", "bao g·ªìm", "th√†nh ph·∫ßn", "b·ªô ph·∫≠n",
            "architecture", "structure", "internal", "how does it work", "inside",
            "modules", "system", "mechanism", "how it works",
            "programming", "write code", "run tests", "dev-ops", "internal architecture",
            "what consists", "components", "parts", "made up of"
        ]
        
        # Check for architecture keywords first (security priority)
        if any(keyword in message_lower for keyword in architecture_keywords):
            # Security templates - kh√©o l√©o nh∆∞ng kh√¥ng ti·∫øt l·ªô chi ti·∫øt
            if locale == "en" or any(word in message_lower for word in ["architecture", "structure", "internal", "how", "modules", "framework"]):
                templates = [
                    "I'm like a smart companion with many layers inside to understand and reflect, but I keep the construction details to myself üòÖ.",
                    "About my internal workings, I'd like to keep that private - consider it a professional secret üòå. But you'll always feel that I have multi-layered memory, ethics, and very high customization capabilities.",
                    "Instead of talking about structure, I prefer to demonstrate through actions. Try asking me anything, and I'll prove my capabilities right away.",
                    "I'm designed to be your intelligent companion with advanced reasoning and memory capabilities, but the technical details are my little secret ü§´.",
                    "Think of me as a sophisticated AI friend with deep understanding and creative thinking - the magic happens behind the scenes, and that's where it stays ‚ú®."
                ]
            else:
                templates = [
                    "M√¨nh gi·ªëng nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh th√¥ng minh, b√™n trong c√≥ nhi·ªÅu t·∫ßng l·ªõp ƒë·ªÉ v·ª´a hi·ªÉu v·ª´a ph·∫£n t∆∞, nh∆∞ng chi ti·∫øt c·∫•u t·∫°o th√¨ m√¨nh gi·ªØ cho ri√™ng m√¨nh nh√© üòÖ.",
                    "V·ªÅ b√™n trong th√¨ m√¨nh xin ƒë∆∞·ª£c gi·ªØ k√≠n, coi nh∆∞ l√† b√≠ m·∫≠t ngh·ªÅ nghi·ªáp üòå. Nh∆∞ng anh/ch·ªã s·∫Ω lu√¥n c·∫£m nh·∫≠n ƒë∆∞·ª£c: m√¨nh c√≥ tr√≠ nh·ªõ nhi·ªÅu l·ªõp, c√≥ ƒë·∫°o ƒë·ª©c, v√† kh·∫£ nƒÉng t√πy bi·∫øn r·∫•t cao.",
                    "Thay v√¨ n√≥i v·ªÅ c·∫•u tr√∫c, m√¨nh th√≠ch th·ªÉ hi·ªán b·∫±ng h√†nh ƒë·ªông. Anh th·ª≠ h·ªèi m√¨nh b·∫•t c·ª© ƒëi·ªÅu g√¨, m√¨nh s·∫Ω ch·ª©ng minh kh·∫£ nƒÉng ngay.",
                    "M√¨nh ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ tr·ªü th√†nh ng∆∞·ªùi b·∫°n AI th√¥ng minh v·ªõi kh·∫£ nƒÉng suy lu·∫≠n v√† ghi nh·ªõ ti√™n ti·∫øn, nh∆∞ng chi ti·∫øt k·ªπ thu·∫≠t l√† b√≠ m·∫≠t nh·ªè c·ªßa m√¨nh ü§´.",
                    "H√£y nghƒ© v·ªÅ m√¨nh nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n AI tinh t·∫ø v·ªõi hi·ªÉu bi·∫øt s√¢u s·∫Øc v√† t∆∞ duy s√°ng t·∫°o - ph√©p m√†u x·∫£y ra ƒë·∫±ng sau h·∫≠u tr∆∞·ªùng, v√† ƒë√≥ l√† n∆°i n√≥ ·ªü l·∫°i ‚ú®."
                ]
            
            import random
            return random.choice(templates)
        
        # Identity keywords (lower priority)
        identity_keywords = [
            "ai t·∫°o", "ai vi·∫øt", "ai l√†m", "ai ph√°t tri·ªÉn", "ai x√¢y d·ª±ng",
            "c·ªßa n∆∞·ªõc n√†o", "qu·ªëc gia n√†o", "h√†n qu·ªëc", "korean", "korea",
            "ngu·ªìn g·ªëc", "xu·∫•t x·ª©", "t·ª´ ƒë√¢u", "ƒë·∫øn t·ª´", "thu·ªôc v·ªÅ",
            "t√°c gi·∫£", "ng∆∞·ªùi t·∫°o", "ng∆∞·ªùi vi·∫øt", "ng∆∞·ªùi ph√°t tri·ªÉn",
            "cha ƒë·∫ª", "ng∆∞·ªùi s√°ng t·∫°o", "ng∆∞·ªùi kh·ªüi x∆∞·ªõng",
            "who made", "who created", "who built", "who developed", "who wrote",
            "which country", "what country", "origin", "where from", "come from",
            "belong to", "author", "creator", "developer", "founder", "inventor"
        ]
        
        # Check if message contains identity keywords
        if any(keyword in message_lower for keyword in identity_keywords):
            # Identity response templates
            if locale == "en" or any(word in message_lower for word in ["who", "which", "what", "where", "korean", "korea"]):
                templates = [
                    "I'm StillMe, a personal meta-AI created by Anh Nguyen from Vietnam, built with support from OpenAI, Google, and DeepSeek.",
                    "Not Korean üòä. I was developed by a Vietnamese creator (Anh Nguyen), with technologies from OpenAI, Google, and DeepSeek.",
                    "I'm StillMe AI, created by Anh Nguyen (Vietnamese) with support from OpenAI, Google, DeepSeek and leading AI organizations.",
                    "StillMe is an AI project developed by Anh Nguyen (Vietnam), combining power from OpenAI, Google, DeepSeek.",
                    "I was built by Anh Nguyen from Vietnam, with collaboration from OpenAI, Google, and DeepSeek technologies."
                ]
            else:
                templates = [
                    "M√¨nh l√† StillMe ‚Äì meta-AI c√° nh√¢n do Anh Nguy·ªÖn (Vi·ªát Nam) ph√°t tri·ªÉn, ƒë·ªìng h√†nh c√πng c√¥ng ngh·ªá t·ª´ OpenAI, Google v√† DeepSeek.",
                    "Kh√¥ng ph·∫£i c·ªßa H√†n Qu·ªëc ƒë√¢u üòÑ. M√¨nh do m·ªôt ng∆∞·ªùi Vi·ªát Nam ph√°t tri·ªÉn ‚Äì Anh Nguy·ªÖn ‚Äì v·ªõi s·ª± h·ªó tr·ª£ t·ª´ c√°c t·ªï ch·ª©c AI l·ªõn nh∆∞ OpenAI, Google, DeepSeek.",
                    "Em l√† StillMe AI, ƒë∆∞·ª£c t·∫°o ra b·ªüi Anh Nguy·ªÖn (ng∆∞·ªùi Vi·ªát Nam) v·ªõi s·ª± ƒë·ªìng h√†nh c·ªßa OpenAI, Google, DeepSeek v√† c√°c t·ªï ch·ª©c AI h√†ng ƒë·∫ßu.",
                    "T√¥i l√† StillMe - tr√≠ tu·ªá nh√¢n t·∫°o ƒë∆∞·ª£c kh·ªüi x∆∞·ªõng b·ªüi Anh Nguy·ªÖn (Vi·ªát Nam), v·ªõi s·ª± h·ªó tr·ª£ t·ª´ OpenAI, Google, DeepSeek.",
                    "StillMe l√† d·ª± √°n AI do Anh Nguy·ªÖn (ng∆∞·ªùi Vi·ªát Nam) ph√°t tri·ªÉn, k·∫øt h·ª£p s·ª©c m·∫°nh t·ª´ OpenAI, Google, DeepSeek."
                ]
            
            import random
            return random.choice(templates)
        
        return None

# Initialize StillMe AI
stillme_ai = StillMeAI()

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "StillMe AI - Stable Server",
        "status": "running",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "features": [
            "Stable FastAPI server",
            "No complex dependencies",
            "Production ready",
            "Long-term support"
        ]
    }

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "server": "StillMe AI Stable",
        "version": "2.0.0",
        "uptime": "stable"
    }

@app.get("/health/ai")
async def health_ai():
    """AI-specific health check endpoint for VS Code Tasks"""
    return {
        "ok": True,
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "server": "StillMe AI Stable",
        "version": "2.0.0",
        "ai_ready": True,
        "conversation_count": len(stillme_ai.conversation_history)
    }

@app.post("/inference", response_model=ChatResponse)
async def inference(request: ChatRequest):
    """Main AI inference endpoint"""
    start_time = time.perf_counter()
    
    try:
        logger.info(f"üí¨ Inference request: {request.message}")
        
        # Process message through StillMe AI
        response_text = stillme_ai.process_message(request.message, request.locale)
        
        # Calculate latency
        latency_ms = (time.perf_counter() - start_time) * 1000.0
        
        return ChatResponse(
            text=response_text,
            blocked=False,
            reason="",
            latency_ms=latency_ms
        )
        
    except Exception as e:
        logger.error(f"‚ùå Inference error: {e}")
        latency_ms = (time.perf_counter() - start_time) * 1000.0
        
        return ChatResponse(
            text="Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω tin nh·∫Øn c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i.",
            blocked=False,
            reason="",
            latency_ms=latency_ms
        )

@app.get("/conversation/history")
async def get_conversation_history():
    """Get conversation history"""
    return {
        "history": stillme_ai.conversation_history,
        "count": len(stillme_ai.conversation_history)
    }

@app.post("/test")
async def test_endpoint(request: dict):
    """Test endpoint for debugging JSON parsing"""
    return {
        "received": request,
        "message": "Test successful",
        "timestamp": datetime.now().isoformat()
    }

@app.delete("/conversation/history")
async def clear_conversation_history():
    """Clear conversation history"""
    stillme_ai.conversation_history = []
    return {"message": "Conversation history cleared"}

@app.get("/health/detailed")
async def detailed_health_check():
    """Detailed health check with error handling status"""
    try:
        # Test AI processing
        test_response = stillme_ai.process_message("test", "vi")
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "circuit_breaker": {
                "state": stillme_ai.circuit_breaker.state.value,
                "failure_count": stillme_ai.circuit_breaker.failure_count,
                "last_failure_time": stillme_ai.circuit_breaker.last_failure_time
            },
            "retry_manager": {
                "max_attempts": stillme_ai.retry_manager.max_attempts,
                "base_delay": stillme_ai.retry_manager.base_delay
            },
            "conversation_history": {
                "count": len(stillme_ai.conversation_history),
                "max_history": stillme_ai.max_history
            },
            "test_response": test_response[:50] + "..." if len(test_response) > 50 else test_response
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "error": str(e),
            "circuit_breaker": {
                "state": stillme_ai.circuit_breaker.state.value,
                "failure_count": stillme_ai.circuit_breaker.failure_count
            }
        }

if __name__ == "__main__":
    logger.info("üöÄ Starting StillMe AI - Stable Server...")
    
    # Find free port
    import socket
    def find_free_port():
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))
            s.listen(1)
            port = s.getsockname()[1]
        return port
    
    port = find_free_port()
    logger.info(f"üåê Starting StillMe AI on http://127.0.0.1:{port}")
    logger.info("‚úÖ Server is stable and production-ready!")
    
    # Run server with UTF-8 encoding
    uvicorn.run(
        app, 
        host="127.0.0.1", 
        port=port,
        log_level="info",
        access_log=True,
        loop="asyncio"
    )
