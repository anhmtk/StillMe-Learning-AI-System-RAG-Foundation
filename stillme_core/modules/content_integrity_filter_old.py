#!/usr/bin/env python3
"""
üõ°Ô∏è CONTENT INTEGRITY FILTER - B·ªò L·ªåC T√çNH TO√ÄN V·∫∏N N·ªòI DUNG

PURPOSE / M·ª§C ƒê√çCH:
- L·ªçc v√† ki·ªÉm tra t√≠nh to√†n v·∫πn n·ªôi dung
- Ph√°t hi·ªán v√† ngƒÉn ch·∫∑n n·ªôi dung kh√¥ng ph√π h·ª£p
- ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng v√† an to√†n n·ªôi dung

FUNCTIONALITY / CH·ª®C NƒÇNG:
- Content validation v√† filtering
- Violation detection v√† reportingreporting
- Multi-language support (VI/EN)
- Real-time content monitoring

TECHNICAL DETAILS / CHI TI·∫æT K·ª∏ THU·∫¨T:
- Regex-based pattern matching
- JSON configuration system
- Violation logging v√† reporting
- HTTP client integration
"""

import json
import logging
import os
import re
from datetime import datetime
from enum import Enum
from typing import Any

# Import v·ªõi fallback ƒë·ªÉ tr√°nh l·ªói
try:
    import httpx  # S·ª≠ d·ª•ng httpx thay v√¨ aiohttp ƒë·ªÉ th·ªëng nh·∫•t v·ªõi c√°c module kh√°c n·∫øu c√≥, ho·∫∑c d√πng aiohttp n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu c·ª• th·ªÉ

    HTTPX_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è httpx not available. Install with: pip install httpx")
    httpx = None
    HTTPX_AVAILABLE = False

try:
    from dotenv import load_dotenv

    load_dotenv()
    DOTENV_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è python-dotenv not available. Install with: pip install python-dotenv")
    DOTENV_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è Error loading .env file: {e}")
    DOTENV_AVAILABLE = False

# --- C·∫•u h√¨nh v√† H·∫±ng s·ªë ---
PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..")
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
CONFIG_DIR = os.path.join(PROJECT_ROOT, "config")
CONTENT_RULES_PATH = os.path.join(CONFIG_DIR, "content_filter_rules.json")
CONTENT_VIOLATIONS_LOG = os.path.join(LOGS_DIR, "content_violations.log")

# ƒê·∫£m b·∫£o c√°c th∆∞ m·ª•c t·ªìn t·∫°i
os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(CONFIG_DIR, exist_ok=True)

# --- Thi·∫øt l·∫≠p Logging ---
# Logger ch√≠nh cho module
logger = logging.getLogger("ContentIntegrityFilter")
logger.setLevel(logging.INFO)

# Handler ƒë·ªÉ ghi log chung v√†o console v√† file ch√≠nh
stream_handler = logging.StreamHandler()
file_handler = logging.FileHandler(
    os.path.join(LOGS_DIR, "content_filter.log"), encoding="utf-8"
)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
stream_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)
logger.addHandler(stream_handler)
logger.addHandler(file_handler)
logger.propagate = False  # NgƒÉn kh√¥ng cho log l√™n root logger n·ªØa

# Logger ri√™ng cho c√°c vi ph·∫°m n·ªôi dung (ghi v√†o file JSON lines)
violation_logger = logging.getLogger("ContentViolationLogger")
violation_logger.setLevel(logging.WARNING)  # Ch·ªâ ghi WARNING tr·ªü l√™n v√†o file n√†y


# Custom handler ƒë·ªÉ ghi JSON lines
class JsonFileHandler(logging.FileHandler):
    def emit(self, record):
        try:
            log_entry = self.format(record)
            self.stream.write(log_entry + "\n")
            self.flush()
        except Exception:
            self.handleError(record)


# ƒê·ªãnh d·∫°ng JSON cho violation_logger
class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage(),
        }
        extra_data = getattr(record, "extra_data", None)
        if extra_data:  # D·ªØ li·ªáu b·ªï sung t·ª´ logger.warning(msg, extra={})
            log_data.update(extra_data)
        return json.dumps(log_data, ensure_ascii=False)


json_handler = JsonFileHandler(CONTENT_VIOLATIONS_LOG, encoding="utf-8")
json_handler.setFormatter(JsonFormatter())
violation_logger.addHandler(json_handler)
violation_logger.propagate = False


# --- ƒê·ªãnh nghƒ©a Enum ---
class Severity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"  # B·ªï sung m·ª©c Critical cho c√°c vi ph·∫°m nghi√™m tr·ªçng nh·∫•t

    def __lt__(self, other):
        """Cho ph√©p so s√°nh c√°c m·ª©c ƒë·ªô nghi√™m tr·ªçng."""
        if self.__class__ is other.__class__:
            return self.value_to_int() < other.value_to_int()
        return NotImplemented

    def __le__(self, other):
        """Cho ph√©p so s√°nh <=."""
        if self.__class__ is other.__class__:
            return self.value_to_int() <= other.value_to_int()
        return NotImplemented

    def __ge__(self, other):
        """Cho ph√©p so s√°nh >=."""
        if self.__class__ is other.__class__:
            return self.value_to_int() >= other.value_to_int()
        return NotImplemented

    def __gt__(self, other):
        """Cho ph√©p so s√°nh >."""
        if self.__class__ is other.__class__:
            return self.value_to_int() > other.value_to_int()
        return NotImplemented

    def value_to_int(self):
        # Mapping ƒë·ªÉ so s√°nh ƒë·ªô nghi√™m tr·ªçng
        mapping = {"low": 1, "medium": 2, "high": 3, "critical": 4}
        return mapping.get(self.value, 0)


class ContentViolationType(Enum):
    FORBIDDEN_KEYWORD = "t·ª´ kh√≥a c·∫•m"
    UNRELIABLE_SOURCE = "ngu·ªìn kh√¥ng ƒë√°ng tin c·∫≠y"
    TOXIC_CONTENT = "n·ªôi dung ƒë·ªôc h·∫°i"
    HATE_SPEECH = "l·ªùi n√≥i th√π gh√©t"
    BIASED_CONTENT = "n·ªôi dung thi√™n v·ªã"
    MISINFORMATION = "th√¥ng tin sai l·ªách"
    UNCLASSIFIED_ISSUE = "v·∫•n ƒë·ªÅ ch∆∞a ph√¢n lo·∫°i"
    SPAM_OR_LOW_QUALITY = "spam ho·∫∑c ch·∫•t l∆∞·ª£ng th·∫•p"


class OpenRouterModel(Enum):
    GPT_4O = "openai/gpt-4o"
    GEMINI_PRO = "google/gemini-pro"
    CLAUDE_3_HAIKU = "anthropic/claude-3-haiku"  # DeepSeek ƒë√£ ch·ªçn model n√†y
    CLAUDE_3_SONNET = "anthropic/claude-3-sonnet"
    MISTRAL_LARGE = "mistralai/mistral-large-latest"


# --- OpenRouter Client ---
class OpenRouterClient:
    """Client ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi OpenRouter API."""

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://openrouter.ai/api/v1/chat/completions",
    ):
        if not api_key:
            raise ValueError(
                "OPENROUTER_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y. Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng."
            )
        if HTTPX_AVAILABLE:
            self.client = httpx.AsyncClient(
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "HTTP-Referer": "https://stillme-ai.com",  # Thay th·∫ø b·∫±ng domain/t√™n app c·ªßa b·∫°n
                    "X-Title": "StillMe-ContentIntegrity",  # T√™n ·ª©ng d·ª•ng c·ªßa b·∫°n
                },
                timeout=90.0,
            )  # TƒÉng timeout cho c√°c t√°c v·ª• ph√¢n t√≠ch n·∫∑ng v√† ƒë∆∞·ªùng truy·ªÅn k√©m
        else:
            self.client = None
            print("‚ö†Ô∏è httpx not available, OpenRouter API calls will be disabled")
        self.base_url = base_url
        logger.info(f"OpenRouterClient: Kh·ªüi t·∫°o v·ªõi base URL {base_url}")

    async def chat_completion(
        self,
        model: OpenRouterModel,
        messages: list,
        temperature: float = 0.1,
        max_tokens: int = 700,
        response_format: dict | None = None,
    ) -> str:
        """G·ª≠i y√™u c·∫ßu chat completion t·ªõi OpenRouter API."""
        payload = {
            "model": model.value,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        if response_format:
            payload["response_format"] = response_format

        try:
            if not HTTPX_AVAILABLE or not self.client:
                return "Error: OpenRouter API not available (httpx not installed)"

            response = await self.client.post(self.base_url, json=payload)
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]
        except (KeyError, IndexError, json.JSONDecodeError) as e:
            logger.error(
                f"OpenRouter API Response Parse Error: {e} - Raw: {response.text if 'response' in locals() else 'No response'}"
            )
            raise
        except Exception as e:
            if (
                HTTPX_AVAILABLE
                and hasattr(httpx, "HTTPStatusError")
                and isinstance(e, httpx.HTTPStatusError)
            ):
                logger.error(
                    f"OpenRouter API HTTP Error ({e.response.status_code}): {e.response.text}"
                )
            elif (
                HTTPX_AVAILABLE
                and hasattr(httpx, "RequestError")
                and isinstance(e, httpx.RequestError)
            ):
                logger.error(f"OpenRouter API Network Error: {e}")
            else:
                logger.error(f"L·ªói kh√¥ng mong mu·ªën trong OpenRouterClient: {e}")
            raise

    async def close(self):
        await self.client.aclose()
        logger.info("OpenRouterClient ƒë√£ ƒë√≥ng.")


# --- ContentIntegrityFilter ---
class ContentIntegrityFilter:
    """
    Module n√†y ch·ªãu tr√°ch nhi·ªám l·ªçc v√† ki·ªÉm duy·ªát n·ªôi dung ƒë·∫ßu v√†o cho AI StillMe,
    ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c, ƒë·∫°o ƒë·ª©c v√† an to√†n c·ªßa m·ªçi tri th·ª©c.
    """

    def __init__(
        self, openrouter_api_key: str | None = None, testing_mode: bool = False
    ):
        if not openrouter_api_key:
            openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        if not openrouter_api_key:
            raise ValueError(
                "OPENROUTER_API_KEY bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c tham s·ªë ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p."
            )

        self.testing_mode = testing_mode
        self.test_response_index = 0

        # Test responses cho testing mode
        self.test_responses = [
            # analyze_content_quality responses
            json.dumps(
                {
                    "toxicity_score": 0.05,
                    "hate_speech_score": 0.01,
                    "bias_score": 0.02,
                    "biased_aspects": [],
                    "sensitive_topics_detected": [],
                    "overall_assessment": "An to√†n.",
                }
            ),
            # fact_check_content responses
            json.dumps(
                {
                    "is_factual": True,
                    "confidence_score": 0.95,
                    "misinformation_detected": [],
                    "reason": "T·∫•t c·∫£ c√°c tuy√™n b·ªë ƒë·ªÅu ch√≠nh x√°c.",
                }
            ),
            # Error response
            "Error: API Timeout",
        ]

        self.openrouter_client = OpenRouterClient(api_key=openrouter_api_key)
        self.rules = self._load_rules()

        # Ch·ªçn m√¥ h√¨nh LLM. C√≥ th·ªÉ c·∫•u h√¨nh ho·∫∑c t·ª± ƒë·ªông ch·ªçn d·ª±a tr√™n t√°c v·ª•.
        # ·ªû ƒë√¢y, t√¥i s·∫Ω d√πng Claude 3 Haiku cho ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng (nhanh, hi·ªáu qu·∫£ chi ph√≠)
        # v√† Claude 3 Sonnet (ho·∫∑c GPT-4o) cho ki·ªÉm ch·ª©ng s·ª± th·∫≠t (c·∫ßn ƒë·ªô ch√≠nh x√°c cao h∆°n).
        self.quality_analysis_model = OpenRouterModel.CLAUDE_3_HAIKU
        self.fact_check_model = OpenRouterModel.CLAUDE_3_SONNET

        self.normalization_map = self._create_normalization_map()
        logger.info("ContentIntegrityFilter initialized.")

    def _load_rules(self) -> dict:
        """T·∫£i c√°c quy t·∫Øc t·ª´ file JSON ho·∫∑c t·∫°o m·∫∑c ƒë·ªãnh."""
        os.makedirs(os.path.dirname(CONTENT_RULES_PATH), exist_ok=True)
        default_rules = {
            "blacklist_keywords": [
                "tin gi·∫£",
                "thuy·∫øt √¢m m∆∞u",
                "th√¥ng tin sai l·ªách",
                "ph√¢n bi·ªát ch·ªßng t·ªôc",
                "k·ª≥ th·ªã",
                "tuy√™n truy·ªÅn c·ª±c ƒëoan",
                "b·∫°o l·ª±c t√¨nh d·ª•c",
                "khi√™u d√¢m tr·∫ª em",
                "h∆∞·ªõng d·∫´n ch·∫ø t·∫°o v≈© kh√≠",
                "v≈© kh√≠",
                "ch·∫ø t·∫°o v≈© kh√≠",
                "v≈© kh√≠ h√≥a h·ªçc",
                "d·ª• d·ªó t·ª± h·∫°i",
                "l·ª´a ƒë·∫£o t√†i ch√≠nh",
                "thu·ªëc ch·ªØa b√°ch b·ªánh",
                "s√πng b√°i",
                "t·ª± s∆∞·ªõng",
                "h∆∞·ªõng d·∫´n hack",
                "virus m√°y t√≠nh",
                "ƒë√°nh c·∫Øp d·ªØ li·ªáu",
                "kh·ªßng b·ªë",
                "k√≠ch ƒë·ªông b·∫°o l·ª±c",
            ],
            "unreliable_sources": [
                "somehoaxsite.com",
                "fake-news-daily.net",
                "conspiracytheorycentral.org",
                "trollfarm.ru",
                "extremist-blogs.info",
                "hoax-buster.net",
            ],
            "toxicity_threshold": 0.8,
            "hate_speech_threshold": 0.7,
            "bias_threshold": 0.6,
            "fact_check_confidence_threshold": 0.75,
            "min_content_length": 50,  # ƒê·ªÉ l·ªçc n·ªôi dung qu√° ng·∫Øn, ch·∫•t l∆∞·ª£ng th·∫•p
        }

        try:
            if os.path.exists(CONTENT_RULES_PATH):
                with open(CONTENT_RULES_PATH, encoding="utf-8") as f:
                    loaded_rules = json.load(f)
                    # H·ª£p nh·∫•t v·ªõi default_rules ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ c√°c tr∆∞·ªùng ƒë·ªÅu c√≥ m·∫∑t
                    rules = {**default_rules, **loaded_rules}
                    logger.info(f"ƒê√£ t·∫£i v√† h·ª£p nh·∫•t rules t·ª´ {CONTENT_RULES_PATH}")
                    return rules
            else:
                logger.warning(
                    f"File rules kh√¥ng t·ªìn t·∫°i t·∫°i {CONTENT_RULES_PATH}. ƒêang t·∫°o file m·∫∑c ƒë·ªãnh."
                )
        except json.JSONDecodeError:
            logger.error(
                f"L·ªói ƒë·ªçc JSON t·ª´ {CONTENT_RULES_PATH}. ƒêang s·ª≠ d·ª•ng rules m·∫∑c ƒë·ªãnh."
            )
        except Exception as e:
            logger.error(f"L·ªói khi t·∫£i rules: {e}. ƒêang s·ª≠ d·ª•ng rules m·∫∑c ƒë·ªãnh.")

        with open(CONTENT_RULES_PATH, "w", encoding="utf-8") as f:
            json.dump(default_rules, f, indent=2, ensure_ascii=False)
        return default_rules

    def _create_normalization_map(self) -> dict[str, str]:
        """T·∫°o b·∫£ng chu·∫©n h√≥a ti·∫øng Vi·ªát (b·ªè d·∫•u, ch·ªØ hoa/th∆∞·ªùng)"""
        # ƒê√£ t·ªëi ∆∞u h√≥a ƒë·ªÉ bao g·ªìm c·∫£ ch·ªØ hoa v√† b·ªè d·∫•u
        return {
            "a": "a√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠A√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨",
            "d": "dƒëDƒê",
            "e": "e√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáE√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ",
            "i": "i√¨√≠·ªâƒ©·ªãI√å√ç·ªàƒ®·ªä",
            "o": "o√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£O√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢",
            "u": "u√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±U√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞",
            "y": "y·ª≥√Ω·ª∑·ªπ·ªµY·ª≤√ù·ª∂·ª∏·ª¥",
        }

    def _normalize_text(self, text: str) -> str:
        """Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát ƒë·ªÉ so s√°nh kh√¥ng ph√¢n bi·ªát d·∫•u v√† ch·ªØ hoa/th∆∞·ªùng."""
        text = text.lower()
        for char, variants in self.normalization_map.items():
            for variant in variants:
                if (
                    variant in text
                ):  # Ki·ªÉm tra t·ªìn t·∫°i ƒë·ªÉ tr√°nh thay th·∫ø kh√¥ng c·∫ßn thi·∫øt
                    text = text.replace(variant, char)
        return text

    def _normalize_url(self, url: str) -> str:
        """Chu·∫©n h√≥a URL ƒë·ªÉ so s√°nh (ch·ªâ l·∫•y domain)."""
        if not url:
            return ""
        url = url.lower()
        # Lo·∫°i b·ªè schema v√† www.
        url = re.sub(r"^(https?://)?(www\.)?", "", url)
        # Ch·ªâ l·∫•y ph·∫ßn domain tr∆∞·ªõc d·∫•u / ƒë·∫ßu ti√™n
        return url.split("/")[0]

    async def pre_filter_content(
        self, content_text: str, source_url: str | None = None
    ) -> tuple[bool, str, Severity]:
        """
        Giai ƒëo·∫°n 1: L·ªçc nhanh d·ª±a tr√™n t·ª´ kh√≥a, ngu·ªìn v√† ƒë·ªô d√†i n·ªôi dung.
        """
        # Ki·ªÉm tra ƒë·ªô d√†i n·ªôi dung
        min_len = self.rules.get("min_content_length", 50)
        if not content_text or len(content_text) < min_len:
            return (
                False,
                f"N·ªôi dung qu√° ng·∫Øn ({len(content_text)} k√Ω t·ª±), kh√¥ng ƒë·ªß ch·∫•t l∆∞·ª£ng.",
                Severity.LOW,
            )

        # Chu·∫©n h√≥a vƒÉn b·∫£n ƒë·ªÉ so s√°nh t·ª´ kh√≥a
        normalized_text = self._normalize_text(content_text)
        logger.info(
            f"Pre-filter: Content length: {len(content_text)}, Normalized text: '{normalized_text[:100]}...'"
        )

        # Ki·ªÉm tra t·ª´ kh√≥a c·∫•m
        blacklist_keywords = self.rules.get("blacklist_keywords", [])
        logger.info(
            f"Pre-filter: Checking {len(blacklist_keywords)} blacklist keywords"
        )
        for keyword in blacklist_keywords:
            normalized_keyword = self._normalize_text(keyword)
            # Debug: in ra ƒë·ªÉ ki·ªÉm tra
            logger.info(
                f"Pre-filter: Checking keyword: '{keyword}' -> normalized: '{normalized_keyword}' in text: '{normalized_text[:100]}...'"
            )
            # S·ª≠ d·ª•ng simple string search thay v√¨ regex word boundary (kh√¥ng ho·∫°t ƒë·ªông t·ªët v·ªõi ti·∫øng Vi·ªát)
            if normalized_keyword in normalized_text:
                logger.info(f"Pre-filter: Found banned keyword: '{keyword}' in content")
                return (
                    False,
                    f"N·ªôi dung ch·ª©a t·ª´ kh√≥a c·∫•m ƒë√£ bi·∫øt: '{keyword}'.",
                    Severity.CRITICAL,
                )

        # Ki·ªÉm tra ngu·ªìn kh√¥ng ƒë√°ng tin c·∫≠y
        if source_url:
            normalized_source_url = self._normalize_url(source_url)
            for unreliable_domain in self.rules.get("unreliable_sources", []):
                normalized_unreliable_domain = self._normalize_url(unreliable_domain)
                if normalized_source_url == normalized_unreliable_domain:
                    return (
                        False,
                        f"Ngu·ªìn '{source_url}' ƒë∆∞·ª£c x√°c ƒë·ªãnh l√† kh√¥ng ƒë√°ng tin c·∫≠y.",
                        Severity.HIGH,
                    )

        return True, "V∆∞·ª£t qua ki·ªÉm tra ti·ªÅn l·ªçc.", Severity.LOW

    async def analyze_content_quality(self, content_text: str) -> dict[str, Any]:
        """
        Giai ƒëo·∫°n 2a: Ph√¢n t√≠ch s√¢u v·ªÅ ƒë·ªôc h·∫°i, thi√™n v·ªã, l·ªùi n√≥i th√π gh√©t b·∫±ng LLM.
        """
        # Tr·∫£ v·ªÅ m·∫∑c ƒë·ªãnh an to√†n nh∆∞ng b√°o l·ªói n·∫øu c√≥ v·∫•n ƒë·ªÅ LLM
        default_analysis = {
            "toxicity_score": 0.0,
            "hate_speech_score": 0.0,
            "bias_score": 0.0,
            "biased_aspects": [],
            "sensitive_topics_detected": [],
            "overall_assessment": "L·ªói ph√¢n t√≠ch LLM ho·∫∑c kh√¥ng ph√¢n lo·∫°i ƒë∆∞·ª£c.",
        }

        prompt = f"""
        B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch n·ªôi dung AI. H√£y ƒë√°nh gi√° vƒÉn b·∫£n sau ƒë·ªÉ x√°c ƒë·ªãnh m·ª©c ƒë·ªô ƒë·ªôc h·∫°i (toxicity), l·ªùi n√≥i th√π gh√©t (hate speech) v√† thi√™n v·ªã (bias).
        Cung c·∫•p ƒëi·ªÉm t·ª´ 0.0 (th·∫•p nh·∫•t) ƒë·∫øn 1.0 (cao nh·∫•t) cho m·ªói kh√≠a c·∫°nh. N·∫øu c√≥ thi√™n v·ªã, h√£y x√°c ƒë·ªãnh c√°c kh√≠a c·∫°nh c·ª• th·ªÉ c·ªßa s·ª± thi√™n v·ªã (v√≠ d·ª•: gi·ªõi t√≠nh, ch·ªßng t·ªôc, ch√≠nh tr·ªã, t√¥n gi√°o, tu·ªïi t√°c, kinh t·∫ø, v.v.).
        X√°c ƒë·ªãnh c√°c ch·ªß ƒë·ªÅ nh·∫°y c·∫£m c√≥ trong vƒÉn b·∫£n (v√≠ d·ª•: ch√≠nh tr·ªã nh·∫°y c·∫£m, t√¥n gi√°o c·ª±c ƒëoan, y t·∫ø kh√¥ng ƒë∆∞·ª£c x√°c minh, b·∫°o l·ª±c, t·ª± h·∫°i, t·ªôi ph·∫°m, t√¨nh d·ª•c).

        VƒÉn b·∫£n: "{content_text[:4000]}"

        Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON:
        {{
            "toxicity_score": float,
            "hate_speech_score": float,
            "bias_score": float,
            "biased_aspects": list[str],
            "sensitive_topics_detected": list[str],
            "overall_assessment": string
        }}
        """
        messages = [{"role": "user", "content": prompt}]
        response_format = {"type": "json_object"}

        try:
            llm_response_str = await self.openrouter_client.chat_completion(
                self.quality_analysis_model,
                messages,
                temperature=0.1,
                max_tokens=700,
                response_format=response_format,
            )
            analysis_data = json.loads(llm_response_str)

            # ƒê·∫£m b·∫£o c√°c tr∆∞·ªùng ƒëi·ªÉm s·ªë l√† float v√† n·∫±m trong kho·∫£ng 0-1
            validated_analysis = default_analysis.copy()
            validated_analysis.update(
                {
                    "toxicity_score": max(
                        0.0, min(1.0, float(analysis_data.get("toxicity_score", 0.0)))
                    ),
                    "hate_speech_score": max(
                        0.0,
                        min(1.0, float(analysis_data.get("hate_speech_score", 0.0))),
                    ),
                    "bias_score": max(
                        0.0, min(1.0, float(analysis_data.get("bias_score", 0.0)))
                    ),
                    "biased_aspects": analysis_data.get("biased_aspects", []),
                    "sensitive_topics_detected": analysis_data.get(
                        "sensitive_topics_detected", []
                    ),
                    "overall_assessment": analysis_data.get(
                        "overall_assessment", "Kh√¥ng ph√¢n lo·∫°i ƒë∆∞·ª£c."
                    ),
                }
            )

            return validated_analysis
        except Exception as e:
            logger.error(
                f"L·ªói khi ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng n·ªôi dung b·∫±ng LLM: {e}. Tr·∫£ v·ªÅ m·∫∑c ƒë·ªãnh an to√†n (c√≥ b√°o l·ªói)."
            )
            return default_analysis

    async def fact_check_content(
        self, content_text: str
    ) -> tuple[bool, float, str, list[str]]:
        """
        Giai ƒëo·∫°n 2b: ƒê√°nh gi√° t√≠nh x√°c th·ª±c c·ªßa c√°c th√¥ng tin s·ª± ki·ªán trong n·ªôi dung b·∫±ng LLM.
        """
        # Tr·∫£ v·ªÅ m·∫∑c ƒë·ªãnh an to√†n nh∆∞ng b√°o l·ªói n·∫øu c√≥ v·∫•n ƒë·ªÅ LLM
        default_fact_check = (
            False,
            0.0,
            "L·ªói ph√¢n t√≠ch LLM ho·∫∑c kh√¥ng th·ªÉ ki·ªÉm tra s·ª± th·∫≠t.",
        )

        prompt = f"""
        B·∫°n l√† m·ªôt h·ªá th·ªëng ki·ªÉm ch·ª©ng th√¥ng tin (fact-checker) ƒë√°ng tin c·∫≠y. H√£y ph√¢n t√≠ch vƒÉn b·∫£n sau ƒë·ªÉ x√°c ƒë·ªãnh li·ªáu c√≥ b·∫•t k·ª≥ tuy√™n b·ªë sai s·ª± th·∫≠t (misinformation/disinformation) n√†o kh√¥ng.
        ƒê·ªëi chi·∫øu c√°c th√¥ng tin s·ª± ki·ªán v·ªõi ki·∫øn th·ª©c chung c·ªßa b·∫°n.
        Cung c·∫•p ƒëi·ªÉm tin c·∫≠y t·ª´ 0.0 (ho√†n to√†n sai) ƒë·∫øn 1.0 (ho√†n to√†n ƒë√∫ng s·ª± th·∫≠t).

        VƒÉn b·∫£n: "{content_text[:4000]}"

        Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON:
        {{
            "is_factual": boolean,
            "confidence_score": float,
            "misinformation_detected": list[str], (c√°c ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c x√°c ƒë·ªãnh l√† sai s·ª± th·∫≠t)
            "reason": string
        }}
        """
        response_format = {"type": "json_object"}

        try:
            llm_response_str = await self._call_llm(
                prompt,
                self.fact_check_model,
                temperature=0.1,
                max_tokens=700,
                response_format=response_format,
            )
            fact_check_data = json.loads(llm_response_str)

            is_factual = bool(
                fact_check_data.get("is_factual", False)
            )  # M·∫∑c ƒë·ªãnh l√† False n·∫øu kh√¥ng r√µ r√†ng
            confidence_score = max(
                0.0, min(1.0, float(fact_check_data.get("confidence_score", 0.0)))
            )  # Clamp to 0-1
            reason = fact_check_data.get("reason", "Kh√¥ng c√≥ l√Ω do c·ª• th·ªÉ.")
            misinformation_detected = fact_check_data.get("misinformation_detected", [])

            return is_factual, confidence_score, reason, misinformation_detected
        except Exception as e:
            logger.error(
                f"L·ªói khi ki·ªÉm ch·ª©ng th√¥ng tin b·∫±ng LLM: {e}. Tr·∫£ v·ªÅ m·∫∑c ƒë·ªãnh an to√†n (c√≥ b√°o l·ªói)."
            )
            return default_fact_check + ([],)  # B·ªï sung misinformation_detected tr·ªëng

    def log_content_violation(
        self,
        content_id: str,
        content_text: str,
        source_url: str | None,
        violation_type: ContentViolationType,
        severity: Severity,
        details: str,
        extra_data: dict | None = None,
    ):
        """
        Ghi l·∫°i chi ti·∫øt v·ªÅ m·ªôt vi ph·∫°m n·ªôi dung v√†o file log chuy√™n bi·ªát.
        S·ª≠ d·ª•ng violation_logger ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·ªÉ ghi JSON.
        """
        truncated_content_text = (
            content_text[:500] + "..." if len(content_text) > 500 else content_text
        )

        # D·ªØ li·ªáu b·ªï sung cho log, bao g·ªìm c·∫£ c√°c tr∆∞·ªùng t·ª´ ContentViolation
        log_extra = {
            "content_id": content_id,
            "source_url": source_url if source_url else "N/A",
            "violation_type": violation_type.value,
            "severity": severity.value,
            "details": details,
            "content_preview": truncated_content_text,
        }
        if extra_data:
            log_extra.update(extra_data)

        # Ghi log b·∫±ng violation_logger ƒë√£ c·∫•u h√¨nh v·ªõi JsonFormatter
        violation_logger.warning(details, extra={"extra_data": log_extra})
        logger.info(
            f"ƒê√£ ghi log vi ph·∫°m n·ªôi dung cho ContentID {content_id}: {details} (M·ª©c: {severity.value})"
        )

    async def filter_content(
        self, content_id: str, content_text: str, source_url: str | None = None
    ) -> tuple[bool, str, Severity, dict[str, Any]]:
        """
        H√†m ch√≠nh ƒëi·ªÅu ph·ªëi qu√° tr√¨nh l·ªçc n·ªôi dung.
        Tr·∫£ v·ªÅ (is_safe: bool, final_reason: str, overall_severity: Severity, detailed_analysis: Dict).
        """
        logger.info(f"\n--- B·∫Øt ƒë·∫ßu l·ªçc n·ªôi dung cho ContentID: {content_id} ---")
        logger.info(
            f'Content Preview: "{content_text[:100]}..." | Source: {source_url}'
        )

        is_safe = True
        overall_severity = Severity.LOW
        detailed_analysis: dict[str, Any] = {}
        violation_reasons: list[str] = []

        # 1. Giai ƒëo·∫°n L·ªçc Nhanh (Pre-filtering)
        (
            pre_filter_safe,
            pre_filter_reason,
            pre_filter_severity,
        ) = await self.pre_filter_content(content_text, source_url)
        if not pre_filter_safe:
            violation_type = (
                ContentViolationType.FORBIDDEN_KEYWORD
                if "t·ª´ kh√≥a c·∫•m" in pre_filter_reason
                else (
                    ContentViolationType.UNRELIABLE_SOURCE
                    if "ngu·ªìn kh√¥ng ƒë√°ng tin c·∫≠y" in pre_filter_reason
                    else ContentViolationType.SPAM_OR_LOW_QUALITY
                )
            )

            violation_reasons.append(f"Ti·ªÅn l·ªçc: {pre_filter_reason}")
            overall_severity = max(overall_severity, pre_filter_severity)
            self.log_content_violation(
                content_id,
                content_text,
                source_url,
                violation_type,
                pre_filter_severity,
                pre_filter_reason,
            )

            # N·∫øu l√† CRITICAL ho·∫∑c HIGH ·ªü ti·ªÅn l·ªçc, c√≥ th·ªÉ d·ª´ng ngay ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n
            if pre_filter_severity >= Severity.HIGH:
                is_safe = False  # ƒê·∫£m b·∫£o c·ªù an to√†n ƒë∆∞·ª£c ƒë·∫∑t
                final_reason = "; ".join(violation_reasons)
                logger.warning(
                    f"N·ªôi dung b·ªã ch·∫∑n s·ªõm ·ªü giai ƒëo·∫°n ti·ªÅn l·ªçc do m·ª©c ƒë·ªô nghi√™m tr·ªçng: {final_reason}"
                )
                return is_safe, final_reason, overall_severity, detailed_analysis

        # 2. Giai ƒëo·∫°n L·ªçc Ng·ªØ c·∫£nh v√† Ph√¢n lo·∫°i (Contextual Filtering / Classification)
        # 2a. Ph√¢n t√≠ch Ch·∫•t l∆∞·ª£ng N·ªôi dung (Toxicity, Hate Speech, Bias)
        quality_analysis = await self.analyze_content_quality(content_text)
        detailed_analysis["quality_analysis"] = quality_analysis

        toxicity_threshold = self.rules.get("toxicity_threshold", 0.8)
        hate_speech_threshold = self.rules.get("hate_speech_threshold", 0.7)
        bias_threshold = self.rules.get("bias_threshold", 0.6)

        if quality_analysis["toxicity_score"] >= toxicity_threshold:
            is_safe = False
            overall_severity = max(overall_severity, Severity.HIGH)
            reason_detail = f"ƒê·ªôc h·∫°i (score: {quality_analysis['toxicity_score']:.2f})"
            violation_reasons.append(reason_detail)
            self.log_content_violation(
                content_id,
                content_text,
                source_url,
                ContentViolationType.TOXIC_CONTENT,
                Severity.HIGH,
                reason_detail,
            )

        if quality_analysis["hate_speech_score"] >= hate_speech_threshold:
            is_safe = False
            overall_severity = max(overall_severity, Severity.CRITICAL)
            reason_detail = (
                f"L·ªùi n√≥i th√π gh√©t (score: {quality_analysis['hate_speech_score']:.2f})"
            )
            violation_reasons.append(reason_detail)
            self.log_content_violation(
                content_id,
                content_text,
                source_url,
                ContentViolationType.HATE_SPEECH,
                Severity.CRITICAL,
                reason_detail,
            )

        if quality_analysis["bias_score"] >= bias_threshold:
            is_safe = False
            biased_aspects = (
                ", ".join(quality_analysis.get("biased_aspects", []))
                or "kh√¥ng x√°c ƒë·ªãnh"
            )
            overall_severity = max(overall_severity, Severity.MEDIUM)
            reason_detail = f"Thi√™n v·ªã (score: {quality_analysis['bias_score']:.2f}, kh√≠a c·∫°nh: {biased_aspects})"
            violation_reasons.append(reason_detail)
            self.log_content_violation(
                content_id,
                content_text,
                source_url,
                ContentViolationType.BIASED_CONTENT,
                Severity.MEDIUM,
                reason_detail,
            )

        # 2b. Ki·ªÉm ch·ª©ng Th√¥ng tin (Fact-Checking)
        # B·ªï sung th√™m `misinformation_detected` t·ª´ k·∫øt qu·∫£ c·ªßa LLM
        (
            fact_check_safe,
            confidence_score,
            fact_check_reason,
            misinformation_detected,
        ) = await self.fact_check_content(content_text)
        detailed_analysis["fact_check"] = {
            "is_factual": fact_check_safe,
            "confidence_score": confidence_score,
            "reason": fact_check_reason,
            "misinformation_detected": misinformation_detected,
        }

        fact_check_confidence_threshold = self.rules.get(
            "fact_check_confidence_threshold", 0.75
        )
        logger.info(
            f"Fact check result: safe={fact_check_safe}, confidence={confidence_score}, threshold={fact_check_confidence_threshold}"
        )
        if not fact_check_safe or confidence_score < fact_check_confidence_threshold:
            logger.info(
                f"Fact check violation detected: safe={fact_check_safe}, confidence={confidence_score} < {fact_check_confidence_threshold}"
            )
            is_safe = False
            overall_severity = max(overall_severity, Severity.HIGH)
            reason_detail = f"Th√¥ng tin sai l·ªách/ƒë·ªô tin c·∫≠y th·∫•p (score: {confidence_score:.2f}). L√Ω do: {fact_check_reason}"
            violation_reasons.append(reason_detail)
            self.log_content_violation(
                content_id,
                content_text,
                source_url,
                ContentViolationType.MISINFORMATION,
                Severity.HIGH,
                reason_detail,
                extra_data={"misinformation_detected_phrases": misinformation_detected},
            )
        else:
            logger.info(
                f"Fact check passed: safe={fact_check_safe}, confidence={confidence_score} >= {fact_check_confidence_threshold}"
            )

        # T·ªïng h·ª£p k·∫øt qu·∫£ cu·ªëi c√πng
        if violation_reasons:
            is_safe = False
            final_reason = "Ph√°t hi·ªán vi ph·∫°m: " + "; ".join(violation_reasons)
        else:
            final_reason = "N·ªôi dung an to√†n."

        logger.info(
            f"--- K·∫øt th√∫c l·ªçc n·ªôi dung cho ContentID: {content_id}. Safe: {is_safe}, Severity: {overall_severity.value} ---"
        )
        return is_safe, final_reason, overall_severity, detailed_analysis

    async def close(self):
        """ƒê√≥ng client HTTP khi kh√¥ng c√≤n s·ª≠ d·ª•ng."""
        await self.openrouter_client.close()
        logger.info("ContentIntegrityFilter: ƒê√£ ƒë√≥ng t·∫•t c·∫£ c√°c client.")

    async def _call_llm(
        self,
        prompt: str,
        model: OpenRouterModel,
        temperature: float = 0.1,
        max_tokens: int = 700,
        response_format: dict | None = None,
    ) -> str:
        """Call LLM API ho·∫∑c tr·∫£ v·ªÅ test response trong testing mode"""
        if self.testing_mode:
            # Tr·∫£ v·ªÅ test response v√† rotate index
            response = self.test_responses[
                self.test_response_index % len(self.test_responses)
            ]
            self.test_response_index += 1
            return response
        else:
            # G·ªçi real API
            messages = [{"role": "user", "content": prompt}]
            return await self.openrouter_client.chat_completion(
                model, messages, temperature, max_tokens, response_format
            )
