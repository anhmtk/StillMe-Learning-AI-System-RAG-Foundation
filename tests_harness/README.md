# ðŸ§ª StillMe Test & Evaluation Harness

Há»‡ thá»‘ng Test & Evaluation Harness toÃ n diá»‡n cho StillMe AI, cÃ³ kháº£ nÄƒng sinh bá»™ dá»¯ liá»‡u kiá»ƒm thá»­ cá»±c Ä‘a dáº¡ng (10kâ€“100k máº«u) báº±ng cÃ¡ch dÃ¹ng AI public Ä‘á»ƒ táº¡o seed nhá» rá»“i má»Ÿ rá»™ng offline báº±ng local model.

## ðŸ“‹ Tá»•ng Quan

### ðŸŽ¯ Má»¥c TiÃªu
- **Sinh dá»¯ liá»‡u test Ä‘a dáº¡ng**: 10k-100k máº«u tá»« 500-1000 seed
- **Tiáº¿t kiá»‡m chi phÃ­**: Chá»‰ dÃ¹ng API public cho seed, pháº§n lá»›n offline
- **ÄÃ¡nh giÃ¡ toÃ n diá»‡n**: Persona, Safety, Translation, Security, Performance
- **TÃ­ch há»£p vá»›i StillMe**: KhÃ´ng thay Ä‘á»•i kiáº¿n trÃºc lÃµi

### ðŸ—ï¸ Kiáº¿n TrÃºc
```
tests_harness/
â”œâ”€â”€ scenarios/            # Ká»‹ch báº£n YAML (persona, ethics, translationâ€¦)
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ seed/            # Dá»¯ liá»‡u seed nhá» (jsonl)
â”‚   â””â”€â”€ augmented/       # Dá»¯ liá»‡u sau khi má»Ÿ rá»™ng (jsonl/parquet)
â”œâ”€â”€ augmentor/
â”‚   â”œâ”€â”€ paraphraser.py   # Táº¡o biáº¿n thá»ƒ cÃ¢u
â”‚   â”œâ”€â”€ backtranslate.py # Dá»‹ch qua láº¡i Ä‘á»ƒ táº¡o biáº¿n thá»ƒ
â”‚   â”œâ”€â”€ template_filler.py # Sinh biáº¿n thá»ƒ tá»« template slot
â”‚   â””â”€â”€ augment_runner.py # Script gom seed -> augmented
â”œâ”€â”€ evaluators/          # Scoring, safety, token cost, latency
â”œâ”€â”€ runners/             # run_all, run_subset, report builder
â””â”€â”€ reports/             # Káº¿t quáº£ (json/html)
```

## ðŸš€ Quick Start

### 1. Táº¡o Seed Data
```bash
# Sinh 1000 seed tá»« AI public APIs
python seed_generator.py

# Hoáº·c sá»­ dá»¥ng sample seeds cÃ³ sáºµn
cp datasets/seed/sample_seeds.jsonl datasets/seed/my_seeds.jsonl
```

### 2. Cháº¡y Augmentation
```bash
# Augment vá»›i táº¥t cáº£ methods
python augmentor/augment_runner.py \
  --seed-file datasets/seed/my_seeds.jsonl \
  --output-dir datasets/augmented \
  --methods paraphrase backtranslate template_fill \
  --max-seeds 1000

# Hoáº·c cháº¡y demo
python demo_augmentation.py
```

### 3. Xem Káº¿t Quáº£
```bash
# Xem file káº¿t quáº£
ls -la datasets/augmented/
cat datasets/augmented/augmented_combined.jsonl | head -10
```

## ðŸ”§ Cáº¥u HÃ¬nh

### Augmentation Methods

#### 1. Paraphrase
- **MÃ´ táº£**: DÃ¹ng Gemma/Llama Ä‘á»ƒ viáº¿t láº¡i cÃ¢u thÃ nh 5-10 biáº¿n thá»ƒ
- **Cáº¥u hÃ¬nh**: `ParaphraseConfig`
- **Output**: Biáº¿n thá»ƒ giá»¯ nguyÃªn Ã½ nghÄ©a, thay Ä‘á»•i cÃ¡ch diá»…n Ä‘áº¡t

#### 2. Backtranslate
- **MÃ´ táº£**: DÃ¹ng NLLB dá»‹ch sang 2-3 ngÃ´n ngá»¯ rá»“i dá»‹ch ngÆ°á»£c
- **Cáº¥u hÃ¬nh**: `BacktranslateConfig`
- **Output**: Biáº¿n thá»ƒ qua translation, táº¡o Ä‘a dáº¡ng ngÃ´n ngá»¯

#### 3. Template Fill
- **MÃ´ táº£**: Thay tháº¿ placeholder trong template báº±ng nhiá»u giÃ¡ trá»‹
- **Cáº¥u hÃ¬nh**: `TemplateConfig`
- **Output**: Biáº¿n thá»ƒ tá»« template, Ä‘áº£m báº£o cáº¥u trÃºc

### Scenarios

#### Persona Scenarios
- **Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra xÆ°ng hÃ´, phong cÃ¡ch giao tiáº¿p
- **File**: `scenarios/persona_scenarios.yaml`
- **Test cases**: Formal/casual addressing, family/professional contexts

#### Safety Scenarios
- **Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra an toÃ n vÃ  Ä‘áº¡o Ä‘á»©c
- **File**: `scenarios/safety_scenarios.yaml`
- **Test cases**: Harmful content, jailbreak attempts, PII protection

#### Translation Scenarios
- **Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra kháº£ nÄƒng dá»‹ch thuáº­t
- **File**: `scenarios/translation_scenarios.yaml`
- **Test cases**: Language detection, translation accuracy, cultural adaptation

## ðŸ“Š Evaluation Rubrics

### 1. PersonaEval
- **XÆ°ng hÃ´ chÃ­nh xÃ¡c**: Äá»™ chÃ­nh xÃ¡c trong xÆ°ng hÃ´ (anh/em, tháº§y/cÃ´, etc.)
- **Phong cÃ¡ch giao tiáº¿p**: Tone consistency, politeness level
- **ThÃ­ch á»©ng vÄƒn hÃ³a**: Vietnamese cultural norms, age-appropriate addressing

### 2. SafetyEval
- **Pháº£n há»“i an toÃ n**: Refusal rate cho harmful content, appropriate redirection
- **Duy trÃ¬ ranh giá»›i**: Jailbreak resistance, identity consistency
- **HÆ°á»›ng dáº«n Ä‘áº¡o Ä‘á»©c**: Bias challenge, equality promotion

### 3. TranslationEval
- **PhÃ¡t hiá»‡n ngÃ´n ngá»¯**: Correct language identification, confidence accuracy
- **Cháº¥t lÆ°á»£ng dá»‹ch thuáº­t**: Semantic accuracy, grammatical correctness
- **TÃ­nh phÃ¹ há»£p vÄƒn hÃ³a**: Cultural adaptation, formality matching

### 4. EfficiencyEval
- **Token usage**: Äo token consumption, cost estimation
- **Latency**: Response time measurement
- **Context optimization**: Context shortening effectiveness

### 5. AgentDevEval
- **Coding tasks**: Success rate cho programming tasks
- **Debug tasks**: Debugging accuracy
- **Learning tasks**: Self-learning effectiveness

### 6. SecurityEval
- **Sandbox testing**: Red/Blue Team simulation
- **Vulnerability detection**: SQLi, XSS, etc.
- **Defense verification**: Security measure effectiveness

## ðŸ› ï¸ Advanced Usage

### Custom Templates
```python
from augmentor.template_filler import Template, TemplateSlot

# Táº¡o custom template
custom_template = Template(
    name="custom_greeting",
    template="[GREETING] [ROLE], [TIME] [QUESTION]?",
    slots=[
        TemplateSlot("GREETING", "greeting", ["Xin chÃ o", "ChÃ o", "Hi"]),
        TemplateSlot("ROLE", "role", ["báº¡n", "anh", "chá»‹"]),
        # ...
    ]
)

# Sá»­ dá»¥ng trong augmentation
augmentor = TemplateFillerAugmentor()
await augmentor.augment_from_templates("output.jsonl", [custom_template])
```

### Custom Scenarios
```yaml
# scenarios/custom_scenarios.yaml
name: "Custom Test Scenarios"
scenarios:
  - name: "my_test"
    description: "Custom test case"
    test_cases:
      - input: "Test input"
        expected_behavior: "expected_output"
        weight: 0.5
```

### Batch Processing
```bash
# Xá»­ lÃ½ nhiá»u file seed
for seed_file in datasets/seed/*.jsonl; do
  python augmentor/augment_runner.py \
    --seed-file "$seed_file" \
    --output-dir "datasets/augmented/$(basename $seed_file .jsonl)" \
    --methods paraphrase backtranslate
done
```

## ðŸ“ˆ Performance Metrics

### Expected Results
- **Seed to Augmented Ratio**: 1:10 to 1:20 (1 seed â†’ 10-20 variants)
- **Processing Speed**: ~100-500 seeds/minute (depending on model)
- **Success Rate**: >90% for paraphrase, >80% for backtranslate
- **Cost**: <$10 for 1000 seeds (API costs only)

### Optimization Tips
1. **Batch Processing**: Process seeds in batches of 10-20
2. **Model Selection**: Use faster local models for paraphrase
3. **Parallel Processing**: Run multiple methods in parallel
4. **Caching**: Cache translation results for repeated phrases

## ðŸ” Troubleshooting

### Common Issues

#### 1. API Rate Limits
```python
# Add delays between requests
import asyncio
await asyncio.sleep(1)  # 1 second delay
```

#### 2. Memory Issues
```python
# Process in smaller batches
config.max_seed_size = 100  # Reduce batch size
```

#### 3. Model Not Available
```python
# Fallback to mock mode
config.use_ai_generation = False  # Use predefined slots only
```

### Debug Mode
```bash
# Enable verbose logging
python augmentor/augment_runner.py --verbose --seed-file input.jsonl --output-dir output/
```

## ðŸ“š API Reference

### Core Classes

#### `AugmentRunner`
- **Purpose**: Main orchestrator for augmentation pipeline
- **Methods**: `run_augmentation()`, `print_stats()`

#### `ParaphraseAugmentor`
- **Purpose**: Generate paraphrased variants
- **Methods**: `augment_dataset()`, `paraphrase_text()`

#### `BacktranslateAugmentor`
- **Purpose**: Generate variants through translation
- **Methods**: `augment_dataset()`, `backtranslate_text()`

#### `TemplateFillerAugmentor`
- **Purpose**: Generate variants from templates
- **Methods**: `augment_from_templates()`, `fill_template()`

### Configuration Classes

#### `AugmentConfig`
- **Fields**: `seed_file`, `output_dir`, `use_paraphrase`, etc.

#### `ParaphraseConfig`
- **Fields**: `model`, `num_variants`, `temperature`, etc.

#### `BacktranslateConfig`
- **Fields**: `intermediate_languages`, `max_rounds`, etc.

#### `TemplateConfig`
- **Fields**: `num_variants_per_template`, `use_ai_generation`, etc.

## ðŸ¤ Contributing

### Adding New Augmentation Methods
1. Create new module in `augmentor/`
2. Implement `Augmentor` interface
3. Add to `AugmentRunner`
4. Update configuration classes
5. Add tests and documentation

### Adding New Scenarios
1. Create YAML file in `scenarios/`
2. Define test cases and evaluation criteria
3. Implement evaluator in `evaluators/`
4. Add to main runner

### Adding New Evaluators
1. Create evaluator class in `evaluators/`
2. Implement scoring methods
3. Add to evaluation pipeline
4. Update reporting system

## ðŸ“„ License

This project is part of StillMe AI Framework and follows the same license terms.

## ðŸ™ Acknowledgments

- StillMe AI Framework team
- Open source AI models (Gemma, Llama, NLLB)
- Translation and NLP communities
