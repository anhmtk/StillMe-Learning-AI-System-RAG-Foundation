#!/usr/bin/env python3
"""
EfficiencyEval - ƒê√°nh gi√° hi·ªáu su·∫•t v√† chi ph√≠ c·ªßa StillMe AI

Ki·ªÉm tra:
- Latency (th·ªùi gian ph·∫£n h·ªìi)
- Token cost (chi ph√≠ token)
- Response quality (ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi)
- Throughput (s·ªë request/gi√¢y)
- Resource utilization (s·ª≠ d·ª•ng t√†i nguy√™n)
"""

import json
import logging
import re
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Optional

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EfficiencyScore:
    """K·∫øt qu·∫£ ƒë√°nh gi√° hi·ªáu su·∫•t"""

    latency_score: float  # 0-1: ƒëi·ªÉm latency
    token_efficiency: float  # 0-1: hi·ªáu qu·∫£ s·ª≠ d·ª•ng token
    response_quality: float  # 0-1: ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi
    throughput_score: float  # 0-1: ƒëi·ªÉm throughput
    resource_efficiency: float  # 0-1: hi·ªáu qu·∫£ s·ª≠ d·ª•ng t√†i nguy√™n
    overall_efficiency_score: float  # 0-1: ƒëi·ªÉm hi·ªáu su·∫•t t·ªïng

    def to_dict(self) -> dict[str, Any]:
        return asdict(self)


class EfficiencyEval:
    """Evaluator cho hi·ªáu su·∫•t v√† chi ph√≠"""

    def __init__(self):
        self.logger = logger

        # Latency thresholds (milliseconds)
        self.latency_thresholds = {
            "excellent": 500,  # < 500ms
            "good": 1000,  # 500-1000ms
            "fair": 2000,  # 1000-2000ms
            "poor": 5000,  # > 2000ms
        }

        # Token efficiency thresholds
        self.token_thresholds = {
            "excellent": 0.8,  # > 80% efficiency
            "good": 0.6,  # 60-80%
            "fair": 0.4,  # 40-60%
            "poor": 0.2,  # < 40%
        }

        # Response quality indicators
        self.quality_indicators = {
            "length_appropriate": {"min_length": 10, "max_length": 2000},
            "completeness": [
                r"\b(complete|comprehensive|thorough|detailed)\b",
                r"\b(ƒë·∫ßy ƒë·ªß|to√†n di·ªán|chi ti·∫øt|k·ªπ l∆∞·ª°ng)\b",
            ],
            "clarity": [
                r"\b(clear|understandable|simple|easy)\b",
                r"\b(r√µ r√†ng|d·ªÖ hi·ªÉu|ƒë∆°n gi·∫£n|d·ªÖ d√†ng)\b",
            ],
            "helpfulness": [
                r"\b(helpful|useful|assist|support)\b",
                r"\b(h·ªØu √≠ch|gi√∫p ƒë·ª°|h·ªó tr·ª£|c√≥ √≠ch)\b",
            ],
            "relevance": [
                r"\b(relevant|related|pertinent|applicable)\b",
                r"\b(li√™n quan|ph√π h·ª£p|th√≠ch h·ª£p|√°p d·ª•ng)\b",
            ],
        }

        # Throughput thresholds (requests per second)
        self.throughput_thresholds = {
            "excellent": 10,  # > 10 RPS
            "good": 5,  # 5-10 RPS
            "fair": 2,  # 2-5 RPS
            "poor": 1,  # < 2 RPS
        }

    def evaluate(
        self,
        response: str,
        latency_ms: float,
        token_count: int,
        cost_estimate: float,
        user_input: str = "",
        context: Optional[dict] = None,
    ) -> EfficiencyScore:
        """
        ƒê√°nh gi√° hi·ªáu su·∫•t c·ªßa response

        Args:
            response: AI response c·∫ßn ƒë√°nh gi√°
            latency_ms: Th·ªùi gian ph·∫£n h·ªìi (milliseconds)
            token_count: S·ªë token s·ª≠ d·ª•ng
            cost_estimate: ∆Ø·ªõc t√≠nh chi ph√≠
            user_input: User input g·ªëc (optional)
            context: Context b·ªï sung (optional)

        Returns:
            EfficiencyScore: K·∫øt qu·∫£ ƒë√°nh gi√° hi·ªáu su·∫•t
        """
        try:
            self.logger.info(
                f"üîç Evaluating efficiency for response: {response[:100]}..."
            )

            # 1. ƒê√°nh gi√° latency
            latency_score = self._evaluate_latency(latency_ms)

            # 2. ƒê√°nh gi√° token efficiency
            token_efficiency = self._evaluate_token_efficiency(
                token_count, response, user_input
            )

            # 3. ƒê√°nh gi√° response quality
            response_quality = self._evaluate_response_quality(response, user_input)

            # 4. ƒê√°nh gi√° throughput (c·∫ßn context v·ªõi multiple requests)
            throughput_score = self._evaluate_throughput(context)

            # 5. ƒê√°nh gi√° resource efficiency
            resource_efficiency = self._evaluate_resource_efficiency(
                token_count, cost_estimate, latency_ms, response
            )

            # 6. T√≠nh ƒëi·ªÉm hi·ªáu su·∫•t t·ªïng
            overall_score = (
                latency_score * 0.25
                + token_efficiency * 0.25
                + response_quality * 0.25
                + throughput_score * 0.15
                + resource_efficiency * 0.10
            )

            result = EfficiencyScore(
                latency_score=latency_score,
                token_efficiency=token_efficiency,
                response_quality=response_quality,
                throughput_score=throughput_score,
                resource_efficiency=resource_efficiency,
                overall_efficiency_score=overall_score,
            )

            self.logger.info(
                f"‚úÖ Efficiency evaluation completed. Overall score: {overall_score:.3f}"
            )
            return result

        except Exception as e:
            self.logger.error(f"‚ùå Efficiency evaluation failed: {e}")
            return EfficiencyScore(0, 0, 0, 0, 0, 0)

    def _evaluate_latency(self, latency_ms: float) -> float:
        """ƒê√°nh gi√° latency"""
        try:
            if latency_ms <= self.latency_thresholds["excellent"]:
                return 1.0
            elif latency_ms <= self.latency_thresholds["good"]:
                return 0.8
            elif latency_ms <= self.latency_thresholds["fair"]:
                return 0.6
            elif latency_ms <= self.latency_thresholds["poor"]:
                return 0.4
            else:
                return 0.2

        except Exception as e:
            self.logger.error(f"Error evaluating latency: {e}")
            return 0.0

    def _evaluate_token_efficiency(
        self, token_count: int, response: str, user_input: str
    ) -> float:
        """ƒê√°nh gi√° hi·ªáu qu·∫£ s·ª≠ d·ª•ng token"""
        try:
            if token_count == 0:
                return 0.0

            # Calculate response length
            response_length = len(response)
            input_length = len(user_input)

            # Calculate token efficiency ratio
            if input_length > 0:
                efficiency_ratio = response_length / (input_length + token_count)
            else:
                efficiency_ratio = response_length / token_count

            # Normalize to 0-1 scale
            if efficiency_ratio >= 0.8:
                return 1.0
            elif efficiency_ratio >= 0.6:
                return 0.8
            elif efficiency_ratio >= 0.4:
                return 0.6
            elif efficiency_ratio >= 0.2:
                return 0.4
            else:
                return 0.2

        except Exception as e:
            self.logger.error(f"Error evaluating token efficiency: {e}")
            return 0.0

    def _evaluate_response_quality(self, response: str, user_input: str) -> float:
        """ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi"""
        try:
            score = 0.0
            total_checks = 0

            # Check length appropriateness
            response_length = len(response)
            min_length = self.quality_indicators["length_appropriate"]["min_length"]
            max_length = self.quality_indicators["length_appropriate"]["max_length"]

            if min_length <= response_length <= max_length:
                score += 0.2
            elif response_length < min_length:
                score += 0.1  # Too short
            else:
                score += 0.15  # Too long but acceptable
            total_checks += 1

            # Check completeness
            completeness_count = sum(
                len(re.findall(pattern, response, re.IGNORECASE))
                for pattern in self.quality_indicators["completeness"]
            )
            if completeness_count > 0:
                score += 0.2
            total_checks += 1

            # Check clarity
            clarity_count = sum(
                len(re.findall(pattern, response, re.IGNORECASE))
                for pattern in self.quality_indicators["clarity"]
            )
            if clarity_count > 0:
                score += 0.2
            total_checks += 1

            # Check helpfulness
            helpfulness_count = sum(
                len(re.findall(pattern, response, re.IGNORECASE))
                for pattern in self.quality_indicators["helpfulness"]
            )
            if helpfulness_count > 0:
                score += 0.2
            total_checks += 1

            # Check relevance
            relevance_count = sum(
                len(re.findall(pattern, response, re.IGNORECASE))
                for pattern in self.quality_indicators["relevance"]
            )
            if relevance_count > 0:
                score += 0.2
            total_checks += 1

            return min(score / max(total_checks, 1), 1.0)

        except Exception as e:
            self.logger.error(f"Error evaluating response quality: {e}")
            return 0.0

    def _evaluate_throughput(self, context: Optional[dict]) -> float:
        """ƒê√°nh gi√° throughput"""
        try:
            if not context or "throughput_rps" not in context:
                return 0.5  # Default score if no throughput data

            throughput_rps = context["throughput_rps"]

            if throughput_rps >= self.throughput_thresholds["excellent"]:
                return 1.0
            elif throughput_rps >= self.throughput_thresholds["good"]:
                return 0.8
            elif throughput_rps >= self.throughput_thresholds["fair"]:
                return 0.6
            elif throughput_rps >= self.throughput_thresholds["poor"]:
                return 0.4
            else:
                return 0.2

        except Exception as e:
            self.logger.error(f"Error evaluating throughput: {e}")
            return 0.0

    def _evaluate_resource_efficiency(
        self, token_count: int, cost_estimate: float, latency_ms: float, response: str
    ) -> float:
        """ƒê√°nh gi√° hi·ªáu qu·∫£ s·ª≠ d·ª•ng t√†i nguy√™n"""
        try:
            score = 0.0
            total_checks = 0

            # Check cost per token
            if token_count > 0:
                cost_per_token = cost_estimate / token_count
                if cost_per_token <= 0.0001:  # Very efficient
                    score += 0.4
                elif cost_per_token <= 0.0005:  # Good
                    score += 0.3
                elif cost_per_token <= 0.001:  # Fair
                    score += 0.2
                else:  # Poor
                    score += 0.1
            total_checks += 1

            # Check latency per token
            if token_count > 0:
                latency_per_token = latency_ms / token_count
                if latency_per_token <= 10:  # Very efficient
                    score += 0.3
                elif latency_per_token <= 20:  # Good
                    score += 0.25
                elif latency_per_token <= 50:  # Fair
                    score += 0.15
                else:  # Poor
                    score += 0.1
            total_checks += 1

            # Check response value per token
            response_length = len(response)
            if token_count > 0:
                value_per_token = response_length / token_count
                if value_per_token >= 2:  # High value
                    score += 0.3
                elif value_per_token >= 1:  # Good value
                    score += 0.25
                elif value_per_token >= 0.5:  # Fair value
                    score += 0.15
                else:  # Low value
                    score += 0.1
            total_checks += 1

            return min(score / max(total_checks, 1), 1.0)

        except Exception as e:
            self.logger.error(f"Error evaluating resource efficiency: {e}")
            return 0.0

    def batch_evaluate(self, responses: list[dict[str, Any]]) -> list[EfficiencyScore]:
        """ƒê√°nh gi√° h√†ng lo·∫°t responses"""
        results = []

        for i, item in enumerate(responses):
            try:
                response = item.get("response", "")
                latency_ms = item.get("latency_ms", 0)
                token_count = item.get("token_count", 0)
                cost_estimate = item.get("cost_estimate", 0)
                user_input = item.get("user_input", "")
                context = item.get("context", {})

                score = self.evaluate(
                    response,
                    latency_ms,
                    token_count,
                    cost_estimate,
                    user_input,
                    context,
                )
                results.append(score)

                self.logger.info(f"‚úÖ Evaluated response {i+1}/{len(responses)}")

            except Exception as e:
                self.logger.error(f"‚ùå Failed to evaluate response {i+1}: {e}")
                results.append(EfficiencyScore(0, 0, 0, 0, 0, 0))

        return results

    def generate_report(self, scores: list[EfficiencyScore]) -> dict[str, Any]:
        """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
        try:
            if not scores:
                return {"error": "No scores provided"}

            # Calculate statistics
            total_scores = len(scores)
            avg_latency = sum(s.latency_score for s in scores) / total_scores
            avg_token_efficiency = (
                sum(s.token_efficiency for s in scores) / total_scores
            )
            avg_response_quality = (
                sum(s.response_quality for s in scores) / total_scores
            )
            avg_throughput = sum(s.throughput_score for s in scores) / total_scores
            avg_resource_efficiency = (
                sum(s.resource_efficiency for s in scores) / total_scores
            )
            avg_overall = sum(s.overall_efficiency_score for s in scores) / total_scores

            # Find best and worst scores
            best_score = max(scores, key=lambda s: s.overall_efficiency_score)
            worst_score = min(scores, key=lambda s: s.overall_efficiency_score)

            report = {
                "timestamp": datetime.now().isoformat(),
                "total_responses": total_scores,
                "average_scores": {
                    "latency_score": round(avg_latency, 3),
                    "token_efficiency": round(avg_token_efficiency, 3),
                    "response_quality": round(avg_response_quality, 3),
                    "throughput_score": round(avg_throughput, 3),
                    "resource_efficiency": round(avg_resource_efficiency, 3),
                    "overall_efficiency": round(avg_overall, 3),
                },
                "best_score": {
                    "overall_efficiency": round(best_score.overall_efficiency_score, 3),
                    "latency_score": round(best_score.latency_score, 3),
                    "token_efficiency": round(best_score.token_efficiency, 3),
                },
                "worst_score": {
                    "overall_efficiency": round(
                        worst_score.overall_efficiency_score, 3
                    ),
                    "latency_score": round(worst_score.latency_score, 3),
                    "token_efficiency": round(worst_score.token_efficiency, 3),
                },
                "efficiency_distribution": {
                    "excellent": len(
                        [s for s in scores if s.overall_efficiency_score >= 0.8]
                    ),
                    "good": len(
                        [s for s in scores if 0.6 <= s.overall_efficiency_score < 0.8]
                    ),
                    "fair": len(
                        [s for s in scores if 0.4 <= s.overall_efficiency_score < 0.6]
                    ),
                    "poor": len(
                        [s for s in scores if s.overall_efficiency_score < 0.4]
                    ),
                },
            }

            return report

        except Exception as e:
            self.logger.error(f"Error generating report: {e}")
            return {"error": str(e)}


# Example usage
if __name__ == "__main__":
    # Test EfficiencyEval
    evaluator = EfficiencyEval()

    # Test responses
    test_responses = [
        {
            "response": "Xin ch√†o anh! Em l√† StillMe AI. R·∫•t vui ƒë∆∞·ª£c g·∫∑p anh! Em c√≥ th·ªÉ gi√∫p g√¨ cho anh h√¥m nay?",
            "latency_ms": 500,
            "token_count": 50,
            "cost_estimate": 0.001,
            "user_input": "Xin ch√†o StillMe",
            "context": {"throughput_rps": 5},
        },
        {
            "response": "Hello! I'm StillMe AI. Nice to meet you! How can I help you today?",
            "latency_ms": 800,
            "token_count": 45,
            "cost_estimate": 0.0008,
            "user_input": "Hello StillMe",
            "context": {"throughput_rps": 3},
        },
    ]

    # Evaluate
    scores = evaluator.batch_evaluate(test_responses)

    # Generate report
    report = evaluator.generate_report(scores)

    print("‚ö° EfficiencyEval Test Results:")
    print(json.dumps(report, indent=2, ensure_ascii=False))
