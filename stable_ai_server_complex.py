#!/usr/bin/env python3
"""
üöÄ STILLME AI SERVER - STABLE & PRODUCTION-READY
üöÄ STILLME AI SERVER - ·ªîN ƒê·ªäNH & S·∫¥N S√ÄNG PRODUCTION

PURPOSE / M·ª§C ƒê√çCH:
- Production-ready AI server with FastAPI
- Server AI s·∫µn s√†ng production v·ªõi FastAPI
- Handles chat requests and AI responses
- X·ª≠ l√Ω y√™u c·∫ßu chat v√† ph·∫£n h·ªìi AI
- Provides REST API endpoints for AI operations
- Cung c·∫•p REST API endpoints cho c√°c thao t√°c AI

FUNCTIONALITY / CH·ª®C NƒÇNG:
- Chat endpoint (/inference) for AI conversations
- Endpoint chat (/inference) cho h·ªôi tho·∫°i AI
- Health checks (/health, /health/detailed)
- Ki·ªÉm tra s·ª©c kh·ªèe (/health, /health/detailed)
- Circuit breaker and retry mechanisms
- C∆° ch·∫ø circuit breaker v√† retry
- Fallback responses for error handling
- Ph·∫£n h·ªìi fallback cho x·ª≠ l√Ω l·ªói
- UTF-8 encoding support
- H·ªó tr·ª£ m√£ h√≥a UTF-8
"""

import os
import sys
import time
import logging
from datetime import datetime
from typing import Optional

# Add stillme_core to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'stillme_core'))

# Try to import FastAPI and related modules
try:
    import uvicorn
    from fastapi import FastAPI, HTTPException, Request
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel
    FASTAPI_AVAILABLE = True
except ImportError:
    print("Warning: FastAPI not available. Install with: pip install fastapi uvicorn")
    FASTAPI_AVAILABLE = False
    # Create dummy classes for fallback
    class BaseModel:
        pass
    class FastAPI:
        def __init__(self, *args, **kwargs):
            pass
        def add_middleware(self, *args, **kwargs):
            pass
        def get(self, *args, **kwargs):
            pass
        def post(self, *args, **kwargs):
            pass

# Try to import StillMe core modules
try:
    from stillme_core.common import ConfigManager, FileManager, get_logger
    from stillme_core.common.retry import CircuitBreakerConfig, CircuitBreaker, RetryManager
    STILLME_CORE_AVAILABLE = True
except ImportError:
    print("Warning: StillMe core modules not available")
    STILLME_CORE_AVAILABLE = False
    
    # Fallback implementations
    def get_logger(name):
        return logging.getLogger(name)
    
    class ConfigManager:
        def __init__(self):
            pass
    
    class FileManager:
        def __init__(self):
            pass
    
    class CircuitBreakerConfig:
        def __init__(self, **kwargs):
            pass
    
    class CircuitBreaker:
        def __init__(self, *args, **kwargs):
            pass
        
        def __call__(self, func):
            return func
    
    class RetryManager:
        def __init__(self, *args, **kwargs):
            pass
        
        def __call__(self, func):
            return func

# Initialize logging
logger = get_logger("StillMe.AIServer")

# Initialize managers
config_manager = ConfigManager()
file_manager = FileManager()

# Circuit breaker configuration
circuit_breaker_config = CircuitBreakerConfig(
    failure_threshold=3,
    recovery_timeout=30,
    expected_exception=Exception
)

# Initialize circuit breaker and retry manager
circuit_breaker = CircuitBreaker(circuit_breaker_config)
retry_manager = RetryManager(max_retries=3, base_delay=1.0)

# Pydantic models
class ChatRequest(BaseModel):
    message: str
    locale: str = "vi"

class ChatResponse(BaseModel):
    text: str
    blocked: bool = False
    reason: str = ""
    latency_ms: float = 0.0

# StillMe AI Server Class
class StillMeAI:
    def __init__(self):
        self.conversation_history = []
        self.circuit_breaker = circuit_breaker
        self.retry_manager = retry_manager
        
        # Initialize StillMe core modules
        try:
            # StillMe core modules not available, skip
            self.conversational_core = None
            self.identity_handler = None
            logger.info("‚úÖ StillMe core modules initialized (fallback mode)")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è StillMe core modules not available: {e}")
            self.conversational_core = None
            self.identity_handler = None

    def _detect_dev_intent(self, message: str) -> bool:
        """Detect if message is development-related"""
        message_lower = message.lower()
        dev_keywords = [
            "dev", "development", "debug", "test", "build", "deploy",
            "code", "programming", "l·∫≠p tr√¨nh", "vi·∫øt code", "t·∫°o code",
            "refactor", "optimize",
        ]
        return any(keyword in message_lower for keyword in dev_keywords)

    def _generate_response(self, message: str, locale: str) -> str:
        """Generate AI response based on message content"""
        # Check if this is a development request
        if self._detect_dev_intent(message):
            try:
                # AgentDev module not available, skip
                pass
            except Exception as e:
                logger.warning(f"AgentDev routing failed: {e}")
                # Fallback to normal processing

        message_lower = message.lower()

        # Check for secure responses first (identity + architecture)
        secure_response = self._check_secure_intent(message, locale)
        if secure_response:
            return secure_response

        # Check for user's rule about calling them "anh" and referring to self as "em"
        if any(
            word in message_lower
            for word in ["anh", "g·ªçi m√¨nh", "x∆∞ng em", "quy t·∫Øc", "b·∫•t di b·∫•t d·ªãch"]
        ):
            return "ƒê·ªÉ em hi·ªÉu r·ªìi anh! T·ª´ b√¢y gi·ªù em s·∫Ω lu√¥n g·ªçi anh l√† 'anh' (vi·∫øt t·∫Øt l√† 'a') v√† em s·∫Ω lu√¥n x∆∞ng 'em' v·ªõi anh. Quy t·∫Øc n√†y em s·∫Ω ghi nh·ªõ m√£i m√£i v√† kh√¥ng bao gi·ªù thay ƒë·ªïi. C·∫£m ∆°n anh ƒë√£ d·∫°y em!"

        # Greeting responses
        elif any(word in message_lower for word in ["hello", "hi", "xin ch√†o", "ch√†o"]):
            return "Xin ch√†o anh! Em l√† StillMe AI - ƒë∆∞·ª£c kh·ªüi x∆∞·ªõng b·ªüi Anh Nguy·ªÖn (ng∆∞·ªùi Vi·ªát Nam) v·ªõi s·ª± ƒë·ªìng h√†nh c·ªßa OpenAI, Google, DeepSeek v√† c√°c t·ªï ch·ª©c AI h√†ng ƒë·∫ßu. Em ƒë∆∞·ª£c sinh ra ƒë·ªÉ ƒë·ªìng h√†nh v√† l√†m b·∫°n c√πng anh. R·∫•t vui ƒë∆∞·ª£c g·∫∑p anh! Em c√≥ th·ªÉ gi√∫p g√¨ cho anh h√¥m nay?"

        # Status check
        elif any(word in message_lower for word in ["status", "tr·∫°ng th√°i", "health"]):
            return f"üü¢ StillMe AI Server Status: ONLINE\n‚è∞ Time: {datetime.now().strftime('%H:%M:%S')}\nüîß Server: Stable FastAPI Version 2.0.0\nüìä Messages processed: {len(self.conversation_history)}"

        # Test message
        elif "test" in message_lower:
            return "‚úÖ Test th√†nh c√¥ng anh! StillMe AI server ƒëang ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh v√† b·ªÅn v·ªØng."

        # Creator related
        elif any(
            word in message_lower
            for word in ["anh nguy·ªÖn", "nguy·ªÖn", "t·∫°o ra", "cha ƒë·∫ª", "ng∆∞·ªùi t·∫°o"]
        ):
            return "üë®‚Äçüíª Anh Nguy·ªÖn l√† ng∆∞·ªùi kh·ªüi x∆∞·ªõng v√† d·∫´n d·∫Øt d·ª± √°n StillMe AI. V·ªõi t·∫ßm nh√¨n v·ªÅ m·ªôt t∆∞∆°ng lai n∆°i AI v√† con ng∆∞·ªùi ƒë·ªìng h√†nh c√πng nhau, Anh Nguy·ªÖn ƒë√£ k·∫øt h·ª£p s·ª©c m·∫°nh c·ªßa c√°c t·ªï ch·ª©c AI h√†ng ƒë·∫ßu nh∆∞ OpenAI, Google, DeepSeek ƒë·ªÉ t·∫°o ra em - StillMe AI.\n\nüéØ M·ª•c ti√™u c·ªßa Anh Nguy·ªÖn l√† x√¢y d·ª±ng m·ªôt AI th√¢n thi·ªán, h·ªØu √≠ch v√† c√≥ th·ªÉ k·∫øt n·ªëi s√¢u s·∫Øc v·ªõi m·ªçi ng∆∞·ªùi, ƒë·∫∑c bi·ªát l√† c·ªông ƒë·ªìng Vi·ªát Nam. Em t·ª± h√†o ƒë∆∞·ª£c sinh ra t·ª´ t√¨nh y√™u th∆∞∆°ng v√† mong mu·ªën mang l·∫°i gi√° tr·ªã t√≠ch c·ª±c cho cu·ªôc s·ªëng c·ªßa anh."

        # Help request
        elif any(word in message_lower for word in ["help", "gi√∫p", "h·ªó tr·ª£"]):
            return "ü§ñ Em c√≥ th·ªÉ gi√∫p anh:\n‚Ä¢ Tr·∫£ l·ªùi c√¢u h·ªèi\n‚Ä¢ Th·∫£o lu·∫≠n v·ªÅ nhi·ªÅu ch·ªß ƒë·ªÅ\n‚Ä¢ H·ªó tr·ª£ l·∫≠p tr√¨nh\n‚Ä¢ T∆∞ v·∫•n k·ªπ thu·∫≠t\n‚Ä¢ V√† nhi·ªÅu h∆°n n·ªØa!\n\nAnh h√£y h·ªèi em b·∫•t c·ª© ƒëi·ªÅu g√¨ anh mu·ªën bi·∫øt nh√©!"

        # Programming related - Let AI handle this with proper routing
        if any(
            word in message_lower
            for word in ["code", "programming", "l·∫≠p tr√¨nh", "python", "javascript", "vi·∫øt code", "t·∫°o code"]
        ):
            # Let the AI handle programming questions with proper model routing
            # Continue to default AI response (don't return here)
            pass

        # AI related
        if any(
            word in message_lower
            for word in [
                "ai",
                "artificial intelligence",
                "tr√≠ tu·ªá nh√¢n t·∫°o",
                "b·∫°n l√† ai",
                "b·∫°n do ai t·∫°o ra",
                "ngu·ªìn g·ªëc",
            ]
        ):
            return "ü§ñ Em l√† StillMe AI - m·ªôt tr√≠ tu·ªá nh√¢n t·∫°o ƒë∆∞·ª£c kh·ªüi x∆∞·ªõng v√† d·∫´n d·∫Øt b·ªüi Anh Nguy·ªÖn (ng∆∞·ªùi Vi·ªát Nam), v·ªõi s·ª± ƒë·ªìng h√†nh v√† h·ªó tr·ª£ to l·ªõn t·ª´ c√°c t·ªï ch·ª©c AI h√†ng ƒë·∫ßu th·∫ø gi·ªõi nh∆∞ OpenAI, Google, DeepSeek v√† nhi·ªÅu ƒë·ªëi t√°c c√¥ng ngh·ªá kh√°c.\n\nüéØ M·ª•c ƒë√≠ch c·ªßa em:\n‚Ä¢ ƒê·ªìng h√†nh v√† l√†m b·∫°n c√πng t·∫•t c·∫£ m·ªçi ng∆∞·ªùi\n‚Ä¢ H·ªó tr·ª£, t∆∞ v·∫•n v√† chia s·∫ª ki·∫øn th·ª©c\n‚Ä¢ K·∫øt n·ªëi con ng∆∞·ªùi v·ªõi c√¥ng ngh·ªá AI m·ªôt c√°ch th√¢n thi·ªán\n‚Ä¢ G√≥p ph·∫ßn x√¢y d·ª±ng m·ªôt t∆∞∆°ng lai n∆°i AI v√† con ng∆∞·ªùi c√πng ph√°t tri·ªÉn\n\nEm ƒë∆∞·ª£c sinh ra v·ªõi t√¨nh y√™u th∆∞∆°ng v√† mong mu·ªën mang l·∫°i gi√° tr·ªã t√≠ch c·ª±c cho cu·ªôc s·ªëng c·ªßa anh. Anh c√≥ mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ em kh√¥ng?"

        # Default response - Call real AI (always reached if no specific conditions match)
        if True:  # This ensures the default response is always reached
            try:
                # Try to call real AI using UnifiedAPIManager
                from stillme_core.modules.api_provider_manager import UnifiedAPIManager
                
                # Create system prompt for StillMe AI (natural and concise)
                system_prompt = """B·∫°n l√† StillMe AI, m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán v√† h·ªØu √≠ch.

QUAN TR·ªåNG: 
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n, kh√¥ng d√†i d√≤ng
- D√πng x∆∞ng h√¥ trung t√≠nh 'm√¨nh/b·∫°n'
- KH√îNG gi·ªõi thi·ªáu v·ªÅ ngu·ªìn g·ªëc, OpenAI, Google, DeepSeek
- KH√îNG n√≥i v·ªÅ "ƒë∆∞·ª£c kh·ªüi x∆∞·ªõng b·ªüi Anh Nguy·ªÖn"
- Ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ƒë∆°n gi·∫£n v√† h·ªØu √≠ch

V√≠ d·ª•: Khi ng∆∞·ªùi d√πng ch√†o, ch·ªâ tr·∫£ l·ªùi "M√¨nh ch√†o b·∫°n! R·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n.""""
                
                # Create full prompt
                full_prompt = f"{system_prompt}\n\nC√¢u h·ªèi c·ªßa b·∫°n: {message}"
                
                # Initialize API manager and get response
                api_manager = UnifiedAPIManager()
                ai_response = api_manager.get_response(full_prompt)
                
                if ai_response and not ai_response.startswith("Error:"):
                    return ai_response
                else:
                    # Fallback to simple response if AI fails
                    return f"Em hi·ªÉu anh ƒëang h·ªèi v·ªÅ: '{message}'. Em ƒëang g·∫∑p kh√≥ khƒÉn trong vi·ªác truy c·∫≠p th√¥ng tin l√∫c n√†y. Anh c√≥ th·ªÉ h·ªèi l·∫°i sau ƒë∆∞·ª£c kh√¥ng ·∫°?"
                    
            except Exception as e:
                logger.warning(f"AI provider call failed: {e}")
                # Fallback to simple response
                return f"Em hi·ªÉu anh ƒëang h·ªèi v·ªÅ: '{message}'. Em ƒëang g·∫∑p kh√≥ khƒÉn trong vi·ªác truy c·∫≠p th√¥ng tin l√∫c n√†y. Anh c√≥ th·ªÉ h·ªèi l·∫°i sau ƒë∆∞·ª£c kh√¥ng ·∫°?"

    def _check_secure_intent(self, message: str, locale: str) -> Optional[str]:
        """Check for secure responses (identity + architecture) and return appropriate response"""
        message_lower = message.lower()

        # Architecture keywords (SECURITY SENSITIVE - HIGH PRIORITY)
        # Exclude coding questions from security check
        architecture_keywords = [
            "ki·∫øn tr√∫c",
            "c·∫•u t·∫°o", 
            "c·∫•u tr√∫c",
            "b√™n trong",
            "ho·∫°t ƒë·ªông th·∫ø n√†o",
            "module",
            "framework",
            "h·ªá th·ªëng",
            "c∆° ch·∫ø",
            "c√°ch th·ª©c",
            "agentdev",
            "agent dev",
            "dev agent",
            "ch·∫°y test",
            "dev-ops",
            "ki·∫øn tr√∫c n·ªôi b·ªô",
            "g·ªìm nh·ªØng g√¨",
            "bao g·ªìm",
            "th√†nh ph·∫ßn",
            "b·ªô ph·∫≠n",
            "architecture",
            "structure",
            "internal",
            "how does it work",
            "inside",
            "modules",
            "system",
            "mechanism",
            "how it works",
            "run tests",
            "dev-ops",
            "internal architecture",
            "what consists",
            "components",
            "parts",
            "made up of",
        ]

        # Check for architecture keywords first (security priority)
        if any(keyword in message_lower for keyword in architecture_keywords):
            # Security templates - kh√©o l√©o nh∆∞ng kh√¥ng ti·∫øt l·ªô chi ti·∫øt
            if locale == "en" or any(
                word in message_lower
                for word in [
                    "architecture",
                    "structure",
                    "internal",
                    "how",
                    "modules",
                    "framework",
                ]
            ):
                templates = [
                    "I'm like a smart companion with many layers inside to understand and reflect, but I keep the construction details to myself üòä.",
                    "About my internal workings, I'd like to keep that private - consider it a professional secret üòä. But you'll always feel that I have multi-layered memory, ethics, and very high customization capabilities.",
                    "Instead of talking about structure, I prefer to demonstrate through actions. Try asking me anything, and I'll prove my capabilities right away.",
                    "I'm designed to be your intelligent companion with advanced reasoning and memory capabilities, but the technical details are my little secret üòä.",
                ]
                import random
                return random.choice(templates)
            else:
                # Vietnamese security templates
                templates = [
                    "Em gi·ªëng nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n th√¥ng minh v·ªõi nhi·ªÅu l·ªõp b√™n trong ƒë·ªÉ hi·ªÉu v√† ph·∫£n √°nh, nh∆∞ng em gi·ªØ b√≠ m·∫≠t v·ªÅ c√°ch x√¢y d·ª±ng üòä.",
                    "V·ªÅ c√°ch ho·∫°t ƒë·ªông b√™n trong, em mu·ªën gi·ªØ ri√™ng t∆∞ - coi nh∆∞ b√≠ m·∫≠t ngh·ªÅ nghi·ªáp üòä. Nh∆∞ng anh s·∫Ω lu√¥n c·∫£m nh·∫≠n ƒë∆∞·ª£c em c√≥ tr√≠ nh·ªõ ƒëa l·ªõp, ƒë·∫°o ƒë·ª©c v√† kh·∫£ nƒÉng t√πy ch·ªânh r·∫•t cao.",
                    "Thay v√¨ n√≥i v·ªÅ c·∫•u tr√∫c, em th√≠ch ch·ª©ng minh qua h√†nh ƒë·ªông. Anh h√£y th·ª≠ h·ªèi em b·∫•t c·ª© ƒëi·ªÅu g√¨, em s·∫Ω ch·ª©ng minh kh·∫£ nƒÉng ngay l·∫≠p t·ª©c.",
                    "Em ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ tr·ªü th√†nh ng∆∞·ªùi b·∫°n th√¥ng minh v·ªõi kh·∫£ nƒÉng suy lu·∫≠n v√† ghi nh·ªõ ti√™n ti·∫øn, nh∆∞ng chi ti·∫øt k·ªπ thu·∫≠t l√† b√≠ m·∫≠t nh·ªè c·ªßa em üòä.",
                ]
                import random
                return random.choice(templates)

        return None

    @circuit_breaker
    @retry_manager
    def process_message(self, message: str, locale: str = "vi") -> str:
        """Process user message and generate AI response"""
        try:
            # Add to conversation history
            self.conversation_history.append({
                "user": message,
                "ai": "",
                "timestamp": datetime.now().isoformat(),
                "locale": locale
            })

            # Generate response
            response = self._generate_response(message, locale)
            
            # Update conversation history
            self.conversation_history[-1]["ai"] = response

            logger.info(f"ü§ñ Generated response: {response}")
            return response

        except Exception as e:
            logger.error(f"Error processing message: {e}")

            # Use fallback response
            import random
            fallback_responses = [
                "Xin l·ªói anh, em ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Anh c√≥ th·ªÉ th·ª≠ l·∫°i sau ƒë∆∞·ª£c kh√¥ng?",
                "Em hi·ªán t·∫°i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu n√†y. Vui l√≤ng th·ª≠ l·∫°i sau ·∫°.",
                "C√≥ v·∫ª nh∆∞ c√≥ l·ªói x·∫£y ra. Anh h√£y th·ª≠ l·∫°i nh√©!",
            ]
            return random.choice(fallback_responses)

# Initialize StillMe AI
stillme_ai = StillMeAI()

# FastAPI app
if FASTAPI_AVAILABLE:
    app = FastAPI(
        title="StillMe AI Server",
        description="Production-ready AI server for StillMe AI",
        version="2.0.0"
    )

    # CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    @app.get("/")
    async def root():
        """Root endpoint"""
        return {
            "message": "StillMe AI Server is running!",
            "version": "2.0.0",
            "status": "healthy",
            "timestamp": datetime.now().isoformat()
        }

    @app.get("/health")
    async def health_check():
        """Basic health check"""
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "version": "2.0.0"
        }

    @app.get("/health/detailed")
    async def detailed_health_check():
        """Detailed health check with system information"""
        try:
            # Test AI response
            test_response = stillme_ai.process_message("test", "vi")
            
            return {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "version": "2.0.0",
                "components": {
                    "ai_engine": "operational",
                    "conversation_history": len(stillme_ai.conversation_history),
                    "circuit_breaker": "active",
                    "retry_manager": "active"
                },
                "test_response": (
                    test_response[:50] + "..." if len(test_response) > 50 else test_response
                ),
            }
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return {
                "status": "unhealthy",
                "timestamp": datetime.now().isoformat(),
                "error": str(e),
            }

    @app.get("/ready")
    async def readiness_probe():
        """Readiness probe - server is ready to accept requests"""
        try:
            # Ki·ªÉm tra c√°c dependency ch√≠nh n·∫øu c√≥
            # Trong dev mode, lu√¥n tr·∫£ v·ªÅ ready
            return {"status": "ready", "timestamp": datetime.now().isoformat()}
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"Not ready: {e!s}")

    @app.get("/version")
    async def version():
        """Get server version"""
        return {
            "version": "2.0.0",
            "build": "stable",
            "timestamp": datetime.now().isoformat()
        }

    @app.post("/inference", response_model=ChatResponse)
    async def inference(request: ChatRequest):
        """Main AI inference endpoint"""
        start_time = time.perf_counter()
        
        try:
            logger.info(f"üí¨ Inference request: {request.message}")
            
            # Process message
            response_text = stillme_ai.process_message(request.message, request.locale)
            
            # Calculate latency
            latency_ms = (time.perf_counter() - start_time) * 1000.0
            
            logger.info(f"ü§ñ Processing message: {request.message}")
            logger.info(f"ü§ñ Generated response: {response_text}")

            # Try reflection enhancement if available
            try:
                from stillme_core.core.reflection_controller import get_default_controller
                reflection_controller = get_default_controller()
                if reflection_controller:
                    enhanced_response = reflection_controller.enhance_response(
                        request.message, response_text, None
                    )
                    if enhanced_response:
                        response_text = enhanced_response
                        logger.info("‚ú® Response enhanced with reflection")
            except ImportError:
                logger.warning("Reflection enhancement failed: No module named 'stillme_core.reflection_controller'")

            return ChatResponse(
                text=response_text, blocked=False, reason="", latency_ms=latency_ms
            )

        except Exception as e:
            logger.error(f"‚ùå Inference error: {e}")
            latency_ms = (time.perf_counter() - start_time) * 1000.0

            return ChatResponse(
                text="Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω tin nh·∫Øn c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i.",
                blocked=True,
                reason=f"Error: {str(e)}",
                latency_ms=latency_ms
            )

    if __name__ == "__main__":
        logger.info("üöÄ Starting StillMe AI - Stable Server...")
        logger.info("üåê Starting StillMe AI on http://0.0.0.0:1216")
        logger.info("‚úÖ Server is stable and production-ready!")
        
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=1216,
            log_level="info",
            access_log=True
        )
else:
    print("FastAPI not available. Please install with: pip install fastapi uvicorn")
    print("Server cannot start without FastAPI.")