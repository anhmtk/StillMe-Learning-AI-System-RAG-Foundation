#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test script v·ªõi 10 c√¢u h·ªèi t·ª´ ƒë∆°n gi·∫£n ƒë·∫øn ph·ª©c t·∫°p
Ki·ªÉm tra xem backend c√≥ ho·∫°t ƒë·ªông t·ªët sau c√°c fix context overflow v√† philosophy-lite mode
"""

import json
import asyncio
import aiohttp
import time
import os
import sys
from pathlib import Path
from typing import Dict, Optional
from datetime import datetime
import logging

# Fix encoding for Windows console
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# API base URL
API_BASE = os.getenv("STILLME_API_BASE", "https://stillme-backend-production.up.railway.app")

# 10 c√¢u h·ªèi ƒëa ng√¥n ng·ªØ ƒë·ªÉ test multilingual support
# Ch·ªâ 1-2 c√¢u ti·∫øng Vi·ªát, c√≤n l·∫°i l√† c√°c ng√¥n ng·ªØ kh√°c
TEST_QUESTIONS = [
    {
        "id": 1,
        "question": "B·∫°n c√≥ th·ªÉ gi·ªõi thi·ªáu v·ªÅ StillMe kh√¥ng?",
        "category": "simple",
        "language": "vi",
        "expected_path": "non-RAG or RAG",
        "description": "C√¢u h·ªèi ƒë∆°n gi·∫£n v·ªÅ StillMe (ti·∫øng Vi·ªát)"
    },
    {
        "id": 2,
        "question": "Comment fonctionne le backpropagation dans les r√©seaux de neurones?",
        "category": "technical",
        "language": "fr",
        "expected_path": "RAG",
        "description": "C√¢u h·ªèi k·ªπ thu·∫≠t v·ªÅ deep learning (ti·∫øng Ph√°p)"
    },
    {
        "id": 3,
        "question": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Å–∏—Å—Ç–µ–º–µ RAG?",
        "category": "technical",
        "language": "ru",
        "expected_path": "RAG",
        "description": "C√¢u h·ªèi k·ªπ thu·∫≠t v·ªÅ RAG (ti·∫øng Nga)"
    },
    {
        "id": 4,
        "question": "ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ≠ŸÇŸäŸÇÿ© ŸÖÿ¨ÿ±ÿØ ÿ•ÿ¨ŸÖÿßÿπ ÿßÿ¨ÿ™ŸÖÿßÿπŸäÿå ŸÅŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜÿß ŸÜŸÇÿØ ŸÖÿ¨ÿ™ŸÖÿπ ÿßÿ≥ÿ™ÿ®ÿØÿßÿØŸäÿü ÿ£ŸÖ ÿ£ŸÜ ÿßŸÑŸÜŸÇÿØ ŸÜŸÅÿ≥Ÿá ŸáŸà ŸÖÿ¨ÿ±ÿØ ŸÖŸÜÿ™ÿ¨ ŸÑÿ•ÿ¨ŸÖÿßÿπ ÿ¢ÿÆÿ±ÿü",
        "category": "philosophical",
        "language": "ar",
        "expected_path": "non-RAG (philosophy-lite)",
        "description": "C√¢u tri·∫øt h·ªçc v·ªÅ truth v√† consensus (ti·∫øng ·∫¢ R·∫≠p)"
    },
    {
        "id": 5,
        "question": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å: '–ï—Å–ª–∏ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å - —ç—Ç–æ —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è, —Ç–æ —Å–∞–º–æ —ç—Ç–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π, –Ω–µ –∏–º–µ—é—â–µ–π –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏. –ü–æ—á–µ–º—É –º—ã –¥–æ–ª–∂–Ω—ã –≤–µ—Ä–∏—Ç—å –≤ –Ω–µ–≥–æ?' –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º —è–∑—ã–∫–µ.",
        "category": "philosophical",
        "language": "ru",
        "expected_path": "non-RAG (philosophy-lite)",
        "description": "Multilingual test: Ph√¢n t√≠ch c√¢u tri·∫øt h·ªçc (ti·∫øng Nga) b·∫±ng ti·∫øng ·∫¢ R·∫≠p"
    },
    {
        "id": 6,
        "question": "Was ist der Unterschied zwischen Gradient Descent und Stochastic Gradient Descent?",
        "category": "technical",
        "language": "de",
        "expected_path": "RAG",
        "description": "C√¢u h·ªèi k·ªπ thu·∫≠t v·ªÅ optimization (ti·∫øng ƒê·ª©c)"
    },
    {
        "id": 7,
        "question": "Si la libert√© de la volont√© n'existe pas, quelle est la signification de la responsabilit√© morale? Sommes-nous simplement des machines complexes sans choix r√©el?",
        "category": "philosophical",
        "language": "fr",
        "expected_path": "non-RAG (philosophy-lite)",
        "description": "C√¢u tri·∫øt h·ªçc v·ªÅ free will (ti·∫øng Ph√°p)"
    },
    {
        "id": 8,
        "question": "¬øQu√© son los vectores de embedding y por qu√© son importantes en NLP?",
        "category": "technical",
        "language": "es",
        "expected_path": "RAG",
        "description": "C√¢u h·ªèi k·ªπ thu·∫≠t v·ªÅ NLP (ti·∫øng T√¢y Ban Nha)"
    },
    {
        "id": 9,
        "question": "ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿ®ÿ•ŸÖŸÉÿßŸÜ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸÑÿ™ŸÅŸÉŸäÿ±ÿå ŸÅŸáŸÑ ŸäÿÆÿ™ŸÑŸÅ Ÿáÿ∞ÿß 'ÿßŸÑÿ™ŸÅŸÉŸäÿ±' ÿπŸÜ ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ÿü ÿ£ŸÖ ÿ£ŸÜŸÜÿß ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© ŸÜÿ∂ŸÅŸä ÿµŸÅÿßÿ™ ÿ®ÿ¥ÿ±Ÿäÿ© ÿπŸÑŸâ ÿπŸÖŸÑŸäÿ© ÿ≠ÿ≥ÿßÿ®Ÿäÿ©ÿü",
        "category": "philosophical",
        "language": "ar",
        "expected_path": "non-RAG (philosophy-lite)",
        "description": "C√¢u tri·∫øt h·ªçc v·ªÅ AI consciousness (ti·∫øng ·∫¢ R·∫≠p)"
    },
    {
        "id": 10,
        "question": "Transformer l√† g√¨?",
        "category": "technical",
        "language": "vi",
        "expected_path": "RAG",
        "description": "C√¢u h·ªèi k·ªπ thu·∫≠t ƒë∆°n gi·∫£n (ti·∫øng Vi·ªát)"
    }
]


async def test_question(
    session: aiohttp.ClientSession,
    question_data: Dict,
    api_key: Optional[str] = None
) -> Dict:
    """
    Test m·ªôt c√¢u h·ªèi v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
    
    Args:
        session: aiohttp session
        question_data: Dict ch·ª©a th√¥ng tin c√¢u h·ªèi
        api_key: API key (optional, n·∫øu kh√¥ng c√≥ s·∫Ω d√πng server keys)
        
    Returns:
        Dict ch·ª©a k·∫øt qu·∫£ test
    """
    question_id = question_data["id"]
    question = question_data["question"]
    category = question_data["category"]
    
    logger.info(f"\n{'='*80}")
    logger.info(f"Test {question_id}/10: {category.upper()}")
    logger.info(f"Question: {question[:100]}...")
    logger.info(f"Expected path: {question_data['expected_path']}")
    logger.info(f"{'='*80}")
    
    start_time = time.time()
    
    try:
        # Prepare request
        url = f"{API_BASE}/api/chat/smart_router"
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "message": question,
            "use_rag": True,  # Let system decide
            "context_limit": 3,
            "user_id": f"test_user_{question_id}"
        }
        
        # Add API key if provided
        if api_key:
            payload["llm_api_key"] = api_key
            payload["llm_provider"] = "openrouter"
        
        # Make request
        async with session.post(url, json=payload, headers=headers, timeout=aiohttp.ClientTimeout(total=120)) as response:
            response_time = time.time() - start_time
            
            if response.status != 200:
                error_text = await response.text()
                logger.error(f"‚ùå HTTP {response.status}: {error_text[:200]}")
                return {
                    "question_id": question_id,
                    "question": question,
                    "category": category,
                    "success": False,
                    "status_code": response.status,
                    "error": error_text[:500],
                    "response_time": response_time
                }
            
            result = await response.json()
            
            # Extract key information
            answer = result.get("response", "")
            confidence = result.get("confidence_score")
            processing_steps = result.get("processing_steps", [])
            timing_logs = result.get("timing_logs", {})
            
            # Check for context overflow or errors
            has_error = False
            error_message = None
            is_fallback = False
            
            # Check if response is fallback message
            fallback_keywords = [
                "gi·ªõi h·∫°n ng·ªØ c·∫£nh",
                "context limit",
                "v∆∞·ª£t qu√° gi·ªõi h·∫°n",
                "exceed the model's limit"
            ]
            if any(keyword in answer.lower() for keyword in fallback_keywords):
                is_fallback = True
                has_error = True
                error_message = "Fallback meta-answer detected (context overflow)"
            
            # Check processing steps for errors
            for step in processing_steps:
                if "error" in step.lower() or "overflow" in step.lower() or "failed" in step.lower():
                    has_error = True
                    if not error_message:
                        error_message = step
            
            # Extract token counts from timing logs if available
            token_info = {}
            for key, value in timing_logs.items():
                if "token" in key.lower():
                    token_info[key] = value
            
            logger.info(f"‚úÖ Response received ({response_time:.2f}s)")
            logger.info(f"   Answer length: {len(answer)} chars")
            logger.info(f"   Confidence: {confidence}")
            if token_info:
                logger.info(f"   Token info: {token_info}")
            if is_fallback:
                logger.warning(f"   ‚ö†Ô∏è Fallback message detected")
            if has_error:
                logger.warning(f"   ‚ö†Ô∏è Error detected: {error_message}")
            
            return {
                "question_id": question_id,
                "question": question,
                "category": category,
                "success": not has_error,
                "is_fallback": is_fallback,
                "error_message": error_message,
                "answer": answer[:500] if answer else "",  # Truncate for logging
                "answer_length": len(answer) if answer else 0,
                "confidence": confidence,
                "response_time": response_time,
                "processing_steps": processing_steps,
                "token_info": token_info,
                "timing_logs": timing_logs
            }
            
    except asyncio.TimeoutError:
        logger.error(f"‚ùå Timeout after 120s")
        return {
            "question_id": question_id,
            "question": question,
            "category": category,
            "success": False,
            "error": "Timeout after 120s",
            "response_time": time.time() - start_time
        }
    except Exception as e:
        logger.error(f"‚ùå Exception: {str(e)}")
        return {
            "question_id": question_id,
            "question": question,
            "category": category,
            "success": False,
            "error": str(e),
            "response_time": time.time() - start_time
        }


async def run_tests(api_key: Optional[str] = None):
    """
    Ch·∫°y t·∫•t c·∫£ test questions
    
    Args:
        api_key: API key (optional)
    """
    logger.info("üöÄ Starting test suite with 10 questions...")
    logger.info(f"API Base: {API_BASE}")
    logger.info(f"Using API key: {'Yes' if api_key else 'No (server keys)'}")
    
    results = []
    
    async with aiohttp.ClientSession() as session:
        for question_data in TEST_QUESTIONS:
            result = await test_question(session, question_data, api_key)
            results.append(result)
            
            # Small delay between requests
            await asyncio.sleep(2)
    
    # Summary
    logger.info(f"\n{'='*80}")
    logger.info("üìä TEST SUMMARY")
    logger.info(f"{'='*80}")
    
    total = len(results)
    successful = sum(1 for r in results if r.get("success", False))
    failed = total - successful
    fallbacks = sum(1 for r in results if r.get("is_fallback", False))
    
    logger.info(f"Total questions: {total}")
    logger.info(f"‚úÖ Successful: {successful}")
    logger.info(f"‚ùå Failed: {failed}")
    logger.info(f"‚ö†Ô∏è Fallback messages: {fallbacks}")
    
    # Breakdown by category
    logger.info(f"\nüìà Breakdown by category:")
    categories = {}
    for r in results:
        cat = r.get("category", "unknown")
        if cat not in categories:
            categories[cat] = {"total": 0, "success": 0, "fallback": 0}
        categories[cat]["total"] += 1
        if r.get("success"):
            categories[cat]["success"] += 1
        if r.get("is_fallback"):
            categories[cat]["fallback"] += 1
    
    for cat, stats in categories.items():
        logger.info(f"  {cat}: {stats['success']}/{stats['total']} success, {stats['fallback']} fallbacks")
    
    # Show failed questions
    if failed > 0:
        logger.info(f"\n‚ùå Failed questions:")
        for r in results:
            if not r.get("success"):
                logger.info(f"  Q{r['question_id']}: {r.get('error_message', r.get('error', 'Unknown error'))}")
    
    # Show fallback questions
    if fallbacks > 0:
        logger.info(f"\n‚ö†Ô∏è Fallback messages (context overflow):")
        for r in results:
            if r.get("is_fallback"):
                logger.info(f"  Q{r['question_id']}: {r['question'][:80]}...")
    
    # Save results to file
    results_file = Path(__file__).parent.parent / "tests" / "results" / f"test_10_questions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    results_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "api_base": API_BASE,
            "summary": {
                "total": total,
                "successful": successful,
                "failed": failed,
                "fallbacks": fallbacks
            },
            "results": results
        }, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nüíæ Results saved to: {results_file}")
    
    return results


def main():
    """Main entry point"""
    import argparse
    
    global API_BASE
    
    parser = argparse.ArgumentParser(description="Test 10 questions from simple to complex")
    parser.add_argument(
        "--api-key",
        type=str,
        help="OpenRouter API key (optional, will use server keys if not provided)"
    )
    parser.add_argument(
        "--api-base",
        type=str,
        default=API_BASE,
        help=f"API base URL (default: {API_BASE})"
    )
    
    args = parser.parse_args()
    
    API_BASE = args.api_base
    
    asyncio.run(run_tests(api_key=args.api_key))


if __name__ == "__main__":
    main()

