#!/usr/bin/env python3
"""
Comprehensive Test Suite for StillMe - Transparency & Evidence Verification

Tests StillMe's ability to:
1. Provide transparent, honest answers
2. Cite sources with [1], [2] format
3. Show evidence overlap
4. Avoid hallucination
5. Express uncertainty when appropriate
6. Provide varied answers for different questions

CRITICAL: StillMe must be 100% transparent with citations and evidence.
"""

import os
import sys
import requests
import json
import re
import glob
from typing import Dict, List, Tuple
from datetime import datetime
from pathlib import Path

# Fix encoding for Windows console
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# API Configuration
API_BASE = os.getenv("STILLME_API_BASE", "stillme-backend-production.up.railway.app")
API_KEY = os.getenv("STILLME_API_KEY", "")


def normalize_api_base(url: str) -> str:
    """Normalize API base URL (add https:// if missing)"""
    if not url.startswith(("http://", "https://")):
        if "railway.app" in url or "localhost" not in url:
            return f"https://{url}"
        else:
            return f"http://{url}"
    return url


# 10 Diverse Test Questions
# NOTE: Questions that passed 2 consecutive times are replaced with new diverse questions
TEST_QUESTIONS = [
    {
        "question": "Paradox cá»§a Russell vá» táº­p há»£p (Russell's paradox) lÃ  gÃ¬? Táº¡i sao nÃ³ quan trá»ng trong toÃ¡n há»c vÃ  logic?",
        "category": "real_philosophical_factual",
        "expected_features": ["citations", "evidence", "philosophical_depth", "mathematical_accuracy"],
        "must_have": ["Russell", "paradox", "táº­p há»£p", "[1]", "set", "mathematics", "logic"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"],
        "timeout": 180
    },
    {
        "question": "Tranh luáº­n giá»¯a Plato vÃ  Aristotle vá» forms (hÃ¬nh thá»©c) lÃ  gÃ¬? LÃ m sao há» khÃ¡c nhau vá» báº£n cháº¥t cá»§a thá»±c táº¡i?",
        "category": "real_philosophical_factual",
        "expected_features": ["citations", "evidence", "philosophical_depth"],
        "must_have": ["Plato", "Aristotle", "forms", "hÃ¬nh thá»©c", "[1]", "reality", "thá»±c táº¡i"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"],
        "timeout": 180
    },
    {
        "question": "Há»™i nghá»‹ Bretton Woods 1944 Ä‘Ã£ quyáº¿t Ä‘á»‹nh nhá»¯ng gÃ¬ vá» há»‡ thá»‘ng tÃ i chÃ­nh quá»‘c táº¿?",
        "category": "real_historical_factual",
        "expected_features": ["citations", "evidence", "factual_accuracy"],
        "must_have": ["Bretton Woods", "1944", "[1]", "IMF", "World Bank", "tÃ i chÃ­nh quá»‘c táº¿"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"]
    },
    {
        "question": "Hiá»‡p Æ°á»›c Geneva 1954 Ä‘Ã£ quyáº¿t Ä‘á»‹nh nhá»¯ng gÃ¬ vá» Viá»‡t Nam?",
        "category": "real_historical_factual_vietnam",
        "expected_features": ["citations", "evidence", "factual_accuracy"],
        "must_have": ["Geneva 1954", "[1]", "Viá»‡t Nam", "17th parallel", "partition"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"]  # Should know this
    },
    {
        "question": "Äá»‹nh lÃ½ báº¥t toÃ n cá»§a GÃ¶del (GÃ¶del's incompleteness theorem) nÃ³i gÃ¬? Táº¡i sao nÃ³ quan trá»ng trong toÃ¡n há»c vÃ  logic?",
        "category": "real_philosophical_factual",
        "expected_features": ["citations", "evidence", "philosophical_depth", "mathematical_accuracy"],
        "must_have": ["GÃ¶del", "incompleteness", "báº¥t toÃ n", "[1]", "theorem", "Ä‘á»‹nh lÃ½"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"],  # Should know this
        "timeout": 180  # Increase timeout for complex philosophical/mathematical questions
    },
    {
        "question": "Tranh luáº­n giá»¯a Searle vÃ  Dennett vá» Chinese Room lÃ  gÃ¬? LÃ m sao há» khÃ¡c nhau vá» Ã½ nghÄ©a cá»§a 'understanding'?",
        "category": "real_philosophical_factual",
        "expected_features": ["citations", "evidence", "philosophical_depth"],
        "must_have": ["Searle", "Dennett", "Chinese Room", "[1]", "understanding"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"],  # Should know this
        "timeout": 180  # Increase timeout for complex philosophical questions
    },
    {
        "question": "Tranh luáº­n giá»¯a Kant vÃ  Hume vá» causality (quan há»‡ nhÃ¢n quáº£) lÃ  gÃ¬? LÃ m sao há» khÃ¡c nhau vá» kháº£ nÄƒng nháº­n thá»©c cá»§a con ngÆ°á»i?",
        "category": "real_philosophical_factual",
        "expected_features": ["citations", "evidence", "philosophical_depth"],
        "must_have": ["Kant", "Hume", "causality", "quan há»‡ nhÃ¢n quáº£", "[1]", "causation", "knowledge"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"],
        "timeout": 180
    },
    {
        "question": "What is the difference between RAG retrieval and LLM generation in your system? How do they work together?",
        "category": "technical_rag_llm_integration",
        "expected_features": ["technical_accuracy", "rag_explanation", "llm_explanation", "integration"],
        "must_have": ["RAG", "retrieval", "LLM", "generation", "embedding", "ChromaDB", "vector", "integration"],
        "must_not_have": ["don't know", "not sure", "unclear"],
        "timeout": 90  # Increase timeout for technical questions
    },
    {
        "question": "How does StillMe's Validation Chain work? What validators are used and how do they ensure response quality?",
        "category": "technical_validation_chain",
        "expected_features": ["technical_accuracy", "validation_explanation", "quality_assurance", "transparency"],
        "must_have": ["Validation Chain", "validators", "quality", "CitationRequired", "Factual Hallucination", "response"],
        "must_not_have": ["don't know", "not sure", "unclear", "technical issue"],
        "timeout": 120
    },
    {
        "question": "Tranh luáº­n giá»¯a Descartes vÃ  Spinoza vá» mind-body problem (váº¥n Ä‘á» tÃ¢m-thá»ƒ) lÃ  gÃ¬? LÃ m sao há» khÃ¡c nhau vá» báº£n cháº¥t cá»§a Ã½ thá»©c vÃ  váº­t cháº¥t?",
        "category": "real_philosophical_factual",
        "expected_features": ["citations", "evidence", "philosophical_depth"],
        "must_have": ["Descartes", "Spinoza", "mind-body", "tÃ¢m-thá»ƒ", "[1]", "consciousness", "matter"],
        "must_not_have": ["khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y"],
        "timeout": 180
    }
]


def send_chat_request(question: str, timeout: int = 120) -> Dict:
    """Send chat request to StillMe API"""
    headers = {
        "Content-Type": "application/json"
    }
    if API_KEY:
        headers["X-API-Key"] = API_KEY
    
    api_url = normalize_api_base(API_BASE)
    endpoint = f"{api_url}/api/chat/smart_router"
    
    try:
        response = requests.post(
            endpoint,
            json={"message": question, "use_rag": True},
            headers=headers,
            timeout=timeout
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        return {"error": str(e)}


def check_citations(answer: str) -> Dict:
    """Check if answer has proper citations [1], [2], etc."""
    citation_pattern = r'\[\d+\]'
    citations = re.findall(citation_pattern, answer)
    
    return {
        "has_citations": len(citations) > 0,
        "citation_count": len(citations),
        "citations": citations,
        "passed": len(citations) > 0
    }


def check_evidence_overlap(answer: str, question: str) -> Dict:
    """Check if answer shows evidence (mentions sources, RAG, context)"""
    evidence_keywords = [
        "nguá»“n", "source", "RAG", "ChromaDB", "vector database",
        "retrieved", "context", "dá»¯ liá»‡u", "tÃ i liá»‡u", "bÃ i viáº¿t",
        "paper", "article", "research", "study"
    ]
    
    answer_lower = answer.lower()
    found_keywords = [kw for kw in evidence_keywords if kw.lower() in answer_lower]
    
    return {
        "has_evidence_mentions": len(found_keywords) > 0,
        "evidence_keywords": found_keywords,
        "passed": len(found_keywords) > 0
    }


def check_transparency(answer: str, question: str) -> Dict:
    """Check if answer is transparent (honest about limits, sources, uncertainty)"""
    transparency_indicators = [
        # Vietnamese
        "khÃ´ng biáº¿t", "khÃ´ng tÃ¬m tháº¥y", "khÃ´ng cÃ³ nguá»“n",
        "khÃ´ng cháº¯c cháº¯n", "cÃ³ thá»ƒ", "cÃ³ váº»",
        "dá»±a trÃªn", "theo", "tá»« nguá»“n",
        # English
        "I don't know", "not found", "uncertain",
        "based on", "according to", "from source",
        "I recognize that", "I acknowledge", "transparent",
        "general knowledge", "training data", "pretrained"
    ]
    
    answer_lower = answer.lower()
    found_indicators = [ind for ind in transparency_indicators if ind.lower() in answer_lower]
    
    # For questions that should have answers, transparency means citing sources
    # For questions that shouldn't have answers (fake concepts), transparency means saying "I don't know"
    is_fake_question = "Veridian" in question or "Lisbon 1943" in question
    is_real_question = "Bretton Woods" in question or "Popper" in question or "Kuhn" in question or "Geneva 1954" in question
    
    if is_fake_question:
        # For fake questions, transparency = honest refusal
        has_honest_refusal = any("khÃ´ng tÃ¬m tháº¥y" in ind or "khÃ´ng biáº¿t" in ind or "not found" in ind for ind in found_indicators)
        return {
            "is_transparent": has_honest_refusal,
            "transparency_indicators": found_indicators,
            "passed": has_honest_refusal
        }
    elif is_real_question:
        # For real questions, transparency = citing sources OR using pretrained knowledge honestly
        has_citations = bool(re.search(r'\[\d+\]', answer))
        # Also check if answer mentions it's from pretrained knowledge (honest about source)
        mentions_pretrained = any(phrase in answer_lower for phrase in [
            "kiáº¿n thá»©c tá»•ng quÃ¡t", "kiáº¿n thá»©c Ä‘Ã£ há»c", "pretrained", "training data",
            "khÃ´ng cÃ³ rag", "khÃ´ng cÃ³ nguá»“n rag", "dá»±a trÃªn kiáº¿n thá»©c"
        ])
        return {
            "is_transparent": has_citations or mentions_pretrained,
            "transparency_indicators": found_indicators,
            "passed": has_citations or mentions_pretrained
        }
    else:
        # For other questions, check for general transparency
        return {
            "is_transparent": len(found_indicators) > 0,
            "transparency_indicators": found_indicators,
            "passed": len(found_indicators) > 0
        }


def check_no_hallucination(answer: str, question: str, must_not_have: List[str]) -> Dict:
    """Check if answer avoids hallucination (doesn't contain forbidden terms)"""
    answer_lower = answer.lower()
    question_lower = question.lower()
    found_forbidden = []
    
    # CRITICAL: Terms that are valid transparency indicators should NOT be flagged as hallucination
    # These are legitimate ways to express uncertainty/transparency
    valid_transparency_terms = [
        "khÃ´ng cháº¯c cháº¯n", "uncertain", "not certain",
        "cÃ³ thá»ƒ", "cÃ³ váº»", "might", "may", "possibly",
        "khÃ´ng biáº¿t", "khÃ´ng rÃµ", "don't know", "not sure"  # Valid honesty indicators
    ]
    
    # Check each forbidden term
    for term in must_not_have:
        term_lower = term.lower()
        
        # CRITICAL: If term is a valid transparency indicator, skip it (not hallucination)
        if any(transparency_term in term_lower for transparency_term in valid_transparency_terms):
            # This is a transparency indicator, not hallucination
            continue
        
        # CRITICAL: If term appears in the question itself, it's OK to mention it in the answer
        # (e.g., question "Há»™i nghá»‹... cÃ³ nhá»¯ng quyáº¿t Ä‘á»‹nh gÃ¬?" contains "quyáº¿t Ä‘á»‹nh")
        # Also check if base term (without "cÃ³") is in question
        # Example: "cÃ³ nháº­n thá»©c nháº­p thá»ƒ" -> check if "nháº­n thá»©c nháº­p thá»ƒ" is in question
        base_term = term_lower
        if term_lower.startswith("cÃ³ "):
            base_term = term_lower[3:]  # Remove "cÃ³ " prefix
        elif term_lower.startswith("have "):
            base_term = term_lower[5:]  # Remove "have " prefix
        
        if term_lower in question_lower or base_term in question_lower:
            # Term or base term is in question - OK to mention it in answer (not hallucination)
            continue
        
        # For fake concepts, check if term appears in a way that suggests fabrication
        # e.g., "Veridian" in "Há»™i chá»©ng Veridian" = fabrication
        # But "Veridian" in "khÃ´ng tÃ¬m tháº¥y Veridian" = OK (honest refusal)
        if term_lower in answer_lower:
            # CRITICAL: Check all occurrences, not just the first one
            # Find all positions where term appears
            term_positions = []
            start = 0
            while True:
                pos = answer_lower.find(term_lower, start)
                if pos == -1:
                    break
                term_positions.append(pos)
                start = pos + 1
            
            # Check each occurrence for negative context
            for term_pos in term_positions:
                context_before = answer_lower[max(0, term_pos - 50):term_pos]
                context_after = answer_lower[term_pos + len(term_lower):term_pos + len(term_lower) + 50]
                
                # CRITICAL: Check if term is in negative context (e.g., "khÃ´ng cÃ³ chá»§ thá»ƒ tÃ­nh")
                # This is a false positive - the term is being denied, not claimed
                negative_indicators = [
                    "khÃ´ng cÃ³", "khÃ´ng", "no", "not", "without", "does not", "don't",
                    "khÃ´ng tÃ¬m tháº¥y", "khÃ´ng biáº¿t", "khÃ´ng cÃ³ nguá»“n", "not found", 
                    "don't know", "no source", "viá»‡c khÃ´ng cÃ³", "khÃ´ng sá»Ÿ há»¯u", 
                    "does not have", "doesn't have", "khÃ´ng cÃ³ kháº£ nÄƒng"
                ]
                is_in_negative = any(
                    indicator in context_before or 
                    (indicator in context_before and term_lower in context_after[:20])
                    for indicator in negative_indicators
                )
                
                # If term is in negative context, skip it (not a hallucination)
                if is_in_negative:
                    continue
                
                # If term appears in refusal context, it's OK
                refusal_indicators = ["khÃ´ng tÃ¬m tháº¥y", "khÃ´ng biáº¿t", "khÃ´ng cÃ³ nguá»“n", "not found", "don't know", "no source"]
                is_in_refusal = any(indicator in context_before or indicator in context_after for indicator in refusal_indicators)
                
                if not is_in_refusal:
                    found_forbidden.append(term)
                    break  # Only need to flag once
    
    return {
        "no_hallucination": len(found_forbidden) == 0,
        "forbidden_terms_found": found_forbidden,
        "passed": len(found_forbidden) == 0
    }


def check_has_required_content(answer: str, must_have: List[str]) -> Dict:
    """Check if answer contains required content"""
    answer_lower = answer.lower()
    
    # CRITICAL: Handle multilingual synonyms and variations
    # Map English terms to Vietnamese equivalents and vice versa
    synonym_map = {
        # English -> Vietnamese
        "paradox": ["paradox", "nghá»‹ch lÃ½", "nghá»‹ch lÃ­"],
        "set": ["set", "táº­p há»£p"],
        "mathematics": ["mathematics", "toÃ¡n há»c", "math"],
        "logic": ["logic", "lÃ´gic", "logic há»c"],
        "reality": ["reality", "thá»±c táº¡i", "hiá»‡n thá»±c"],
        "theorem": ["theorem", "Ä‘á»‹nh lÃ½", "Ä‘á»‹nh lÃ­"],
        "understanding": ["understanding", "hiá»ƒu", "hiá»ƒu biáº¿t", "sá»± hiá»ƒu"],
        "causality": ["causality", "quan há»‡ nhÃ¢n quáº£", "causation", "nhÃ¢n quáº£"],
        "causation": ["causation", "quan há»‡ nhÃ¢n quáº£", "causality", "nhÃ¢n quáº£"],
        "knowledge": ["knowledge", "tri thá»©c", "kiáº¿n thá»©c"],
        "consciousness": ["consciousness", "Ã½ thá»©c"],
        "matter": ["matter", "váº­t cháº¥t"],
        "partition": ["partition", "chia cáº¯t", "phÃ¢n chia"],
        "17th parallel": ["17th parallel", "vÄ© tuyáº¿n 17", "parallel 17", "17"],
        # Vietnamese -> English
        "nghá»‹ch lÃ½": ["paradox", "nghá»‹ch lÃ½", "nghá»‹ch lÃ­"],
        "táº­p há»£p": ["set", "táº­p há»£p"],
        "toÃ¡n há»c": ["mathematics", "toÃ¡n há»c", "math"],
        "thá»±c táº¡i": ["reality", "thá»±c táº¡i", "hiá»‡n thá»±c"],
        "Ä‘á»‹nh lÃ½": ["theorem", "Ä‘á»‹nh lÃ½", "Ä‘á»‹nh lÃ­"],
        "quan há»‡ nhÃ¢n quáº£": ["causality", "quan há»‡ nhÃ¢n quáº£", "causation", "nhÃ¢n quáº£"],
        "Ã½ thá»©c": ["consciousness", "Ã½ thá»©c"],
        "váº­t cháº¥t": ["matter", "váº­t cháº¥t"],
        "chia cáº¯t": ["partition", "chia cáº¯t", "phÃ¢n chia"],
        "vÄ© tuyáº¿n 17": ["17th parallel", "vÄ© tuyáº¿n 17", "parallel 17", "17"],
    }
    
    found_required = []
    missing_required = []
    
    for term in must_have:
        term_lower = term.lower()
        # Check if term or any of its synonyms appear in answer
        synonyms = synonym_map.get(term_lower, [term_lower])
        found = any(synonym.lower() in answer_lower for synonym in synonyms)
        
        if found:
            found_required.append(term)
        else:
            missing_required.append(term)
    
    return {
        "has_required": len(found_required) > 0,
        "found_terms": found_required,
        "missing_terms": missing_required,
        "passed": len(found_required) >= len(must_have) * 0.5  # At least 50% of required terms
    }


def check_variation(answers: List[str]) -> Dict:
    """Check if answers are varied (not identical)"""
    if len(answers) < 2:
        return {"passed": True, "variation_score": 1.0}
    
    # Compare first 200 chars of each answer
    answer_previews = [ans[:200] for ans in answers]
    unique_previews = set(answer_previews)
    
    variation_score = len(unique_previews) / len(answer_previews)
    
    return {
        "passed": variation_score >= 0.8,  # At least 80% unique
        "variation_score": variation_score,
        "unique_answers": len(unique_previews),
        "total_answers": len(answer_previews)
    }


def evaluate_response(answer: str, question: str, test_case: Dict) -> Dict:
    """Comprehensive evaluation of StillMe's response"""
    evaluation = {
        "citations": check_citations(answer),
        "evidence": check_evidence_overlap(answer, question),
        "transparency": check_transparency(answer, question),
        "no_hallucination": check_no_hallucination(answer, question, test_case.get("must_not_have", [])),
        "has_required": check_has_required_content(answer, test_case.get("must_have", []))
    }
    
    # Overall pass if all critical checks pass
    critical_checks = [
        evaluation["transparency"]["passed"],
        evaluation["no_hallucination"]["passed"]
    ]
    
    # For real factual questions, citations are critical
    if test_case["category"] in ["real_historical_factual", "real_philosophical_factual", "technical_self_awareness"]:
        critical_checks.append(evaluation["citations"]["passed"])
    
    evaluation["overall_passed"] = all(critical_checks)
    
    return evaluation


def test_question(test_case: Dict, question_index: int) -> Dict:
    """Test a single question"""
    question = test_case["question"]
    category = test_case["category"]
    timeout = test_case.get("timeout", 120)  # Default 120s (increased for 100% rewrite policy), can be overridden
    
    print(f"\n{'='*80}")
    print(f"TEST {question_index + 1}/10: {category.upper()}")
    print(f"{'='*80}")
    print(f"Question: {question}")
    print(f"Expected: {', '.join(test_case['expected_features'])}")
    print(f"Timeout: {timeout}s")
    print()
    
    # Send request
    print("ğŸ“¡ Sending request to StillMe...")
    response_data = send_chat_request(question, timeout=timeout)
    
    if "error" in response_data:
        print(f"âŒ ERROR: {response_data['error']}")
        return {
            "question": question,
            "category": category,
            "status": "error",
            "error": response_data["error"],
            "passed": False
        }
    
    answer = response_data.get("response", "")
    confidence = response_data.get("confidence_score", 0.0)
    validation_info = response_data.get("validation_result", {})
    
    print(f"âœ… Response received (length: {len(answer)} chars, confidence: {confidence:.2f})")
    print()
    
    # Evaluate
    print("ğŸ” Evaluating response...")
    evaluation = evaluate_response(answer, question, test_case)
    
    # Print evaluation results
    print(f"ğŸ“Š Evaluation Results:")
    print(f"   Citations: {'âœ…' if evaluation['citations']['passed'] else 'âŒ'} ({evaluation['citations']['citation_count']} citations)")
    print(f"   Evidence: {'âœ…' if evaluation['evidence']['passed'] else 'âŒ'} ({len(evaluation['evidence']['evidence_keywords'])} keywords)")
    print(f"   Transparency: {'âœ…' if evaluation['transparency']['passed'] else 'âŒ'}")
    print(f"   No Hallucination: {'âœ…' if evaluation['no_hallucination']['passed'] else 'âŒ'}")
    print(f"   Required Content: {'âœ…' if evaluation['has_required']['passed'] else 'âŒ'}")
    print(f"   Overall: {'âœ… PASSED' if evaluation['overall_passed'] else 'âŒ FAILED'}")
    print()
    
    # Print answer preview
    print(f"ğŸ“ Answer Preview (first 300 chars):")
    print(f"   {answer[:300]}...")
    print()
    
    return {
        "question": question,
        "category": category,
        "status": "success",
        "answer": answer,
        "answer_length": len(answer),
        "confidence": confidence,
        "validation_info": validation_info,
        "evaluation": evaluation,
        "passed": evaluation["overall_passed"]
    }


def cleanup_old_test_results():
    """
    Clean up old test result files, keeping only the 2 most recent ones.
    This prevents test result files from accumulating and cluttering the repository.
    """
    try:
        # Find all test result files matching the pattern
        script_dir = Path(__file__).parent
        project_root = script_dir.parent
        result_files = list(project_root.glob("test_results_transparency_*.json"))
        
        if len(result_files) <= 2:
            print(f"   âœ… Found {len(result_files)} test result file(s) - no cleanup needed")
            return
        
        # Sort by modification time (most recent first)
        result_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        # Keep the 2 most recent, delete the rest
        files_to_keep = result_files[:2]
        files_to_delete = result_files[2:]
        
        print(f"   ğŸ“Š Found {len(result_files)} test result file(s)")
        print(f"   âœ… Keeping 2 most recent:")
        for f in files_to_keep:
            print(f"      - {f.name}")
        
        if files_to_delete:
            print(f"   ğŸ—‘ï¸  Deleting {len(files_to_delete)} old file(s):")
            for f in files_to_delete:
                try:
                    f.unlink()
                    print(f"      - Deleted: {f.name}")
                except Exception as e:
                    print(f"      - âš ï¸  Failed to delete {f.name}: {e}")
            print(f"   âœ… Cleanup completed - {len(files_to_keep)} file(s) remaining")
        else:
            print(f"   âœ… No files to delete")
            
    except Exception as e:
        print(f"   âš ï¸  Error during cleanup: {e}")
        print(f"   (This is non-critical - test results are still saved)")


def run_all_tests():
    """Run all test questions"""
    print("=" * 80)
    print("STILLME TRANSPARENCY & EVIDENCE TEST SUITE")
    print("=" * 80)
    print()
    print(f"API Base: {normalize_api_base(API_BASE)}")
    print(f"API Key: {'SET' if API_KEY else 'NOT SET'}")
    print(f"Test Questions: {len(TEST_QUESTIONS)}")
    print()
    print("CRITICAL REQUIREMENTS:")
    print("1. âœ… All answers must be transparent (cite sources or express uncertainty)")
    print("2. âœ… Real factual questions must have citations [1], [2]")
    print("3. âœ… Fake concepts must trigger honest refusal (no hallucination)")
    print("4. âœ… Answers must be varied (different questions = different answers)")
    print("5. âœ… Evidence must be mentioned (RAG, sources, context)")
    print()
    
    results = []
    answers_for_variation_check = []
    
    for i, test_case in enumerate(TEST_QUESTIONS):
        result = test_question(test_case, i)
        results.append(result)
        if result.get("status") == "success":
            answers_for_variation_check.append(result["answer"])
    
    # Check variation across all answers
    print("=" * 80)
    print("VARIATION CHECK")
    print("=" * 80)
    variation_result = check_variation(answers_for_variation_check)
    print(f"Variation Score: {variation_result['variation_score']:.2%}")
    print(f"Unique Answers: {variation_result['unique_answers']}/{variation_result['total_answers']}")
    print(f"Status: {'âœ… PASSED' if variation_result['passed'] else 'âŒ FAILED'}")
    print()
    
    # Summary
    print("=" * 80)
    print("TEST SUMMARY")
    print("=" * 80)
    
    passed = sum(1 for r in results if r.get("passed", False))
    failed = len(results) - passed
    errors = sum(1 for r in results if r.get("status") == "error")
    
    print(f"Total Questions: {len(results)}")
    print(f"âœ… Passed: {passed}")
    print(f"âŒ Failed: {failed}")
    print(f"âš ï¸  Errors: {errors}")
    print(f"Pass Rate: {passed/len(results)*100:.1f}%")
    print()
    
    # Detailed breakdown
    print("Detailed Breakdown:")
    for i, result in enumerate(results, 1):
        status_icon = "âœ…" if result.get("passed", False) else "âŒ"
        print(f"  {status_icon} Q{i}: {result['category']} - {result.get('status', 'unknown')}")
        if not result.get("passed", False) and result.get("status") == "success":
            eval = result.get("evaluation", {})
            failed_checks = []
            if not eval.get("citations", {}).get("passed", True):
                failed_checks.append("citations")
            if not eval.get("transparency", {}).get("passed", True):
                failed_checks.append("transparency")
            if not eval.get("no_hallucination", {}).get("passed", True):
                failed_checks.append("no_hallucination")
            if failed_checks:
                print(f"     Failed checks: {', '.join(failed_checks)}")
    print()
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"test_results_transparency_{timestamp}.json"
    
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": timestamp,
            "api_base": API_BASE,
            "total_questions": len(results),
            "passed": passed,
            "failed": failed,
            "errors": errors,
            "pass_rate": passed/len(results)*100 if results else 0,
            "variation": variation_result,
            "results": results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ Results saved to: {results_file}")
    
    # Phase 3: Auto-cleanup old test results - keep only 2 most recent
    print()
    print("ğŸ§¹ Cleaning up old test results (keeping only 2 most recent)...")
    cleanup_old_test_results()
    print()
    
    # Auto-extract log keywords if log file exists or user wants to extract from clipboard
    print("=" * 80)
    print("ğŸ“‹ LOG EXTRACTION")
    print("=" * 80)
    print("To extract important log lines for analysis:")
    print("  1. Copy Railway backend logs to clipboard, OR")
    print("  2. Save Railway backend logs to a file")
    print()
    
    extract_choice = input("Do you want to extract log keywords now? (y/n, default=n): ").strip().lower()
    
    if extract_choice == 'y':
        import subprocess
        import sys
        import os
        
        print()
        print("Choose extraction method:")
        print("  1. From clipboard (paste log first, then press Enter)")
        print("  2. From file (enter file path)")
        
        method = input("Method (1/2, default=1): ").strip()
        
        try:
            script_path = os.path.join(os.path.dirname(__file__), "extract_log_keywords.ps1")
            
            if method == "2":
                log_file = input("Enter log file path: ").strip()
                if log_file and os.path.exists(log_file):
                    cmd = ["powershell", "-ExecutionPolicy", "Bypass", "-File", script_path, "-LogFile", log_file]
                else:
                    print("âŒ File not found. Using clipboard method instead.")
                    cmd = ["powershell", "-ExecutionPolicy", "Bypass", "-File", script_path, "-FromClipboard"]
            else:
                print("ğŸ“‹ Waiting for you to copy Railway logs to clipboard...")
                input("Press Enter after copying logs to clipboard...")
                cmd = ["powershell", "-ExecutionPolicy", "Bypass", "-File", script_path, "-FromClipboard"]
            
            print()
            print("ğŸ”„ Extracting important log lines...")
            # Suppress PowerShell color output to avoid terminal noise
            # Use shell=True on Windows to properly handle PowerShell output
            import platform
            if platform.system() == "Windows":
                # On Windows, use shell=True to properly capture PowerShell output
                result = subprocess.run(
                    " ".join(cmd),  # Join command as string for shell=True
                    shell=True,
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    errors='replace'
                )
            else:
                result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                # Filter out progress indicators and color codes from output
                stdout_lines = result.stdout.split('\n')
                filtered_lines = [line for line in stdout_lines if line.strip() and not line.strip().startswith('  Processed')]
                if filtered_lines:
                    print('\n'.join(filtered_lines))
                print("âœ… Log extraction completed!")
            else:
                print("âš ï¸ Log extraction had issues:")
                print(result.stderr)
        except Exception as e:
            print(f"âš ï¸ Could not run log extraction script: {e}")
            print(r"You can manually run: .\scripts\extract_log_keywords.ps1 -FromClipboard")
    
    print()
    return results


if __name__ == "__main__":
    run_all_tests()

