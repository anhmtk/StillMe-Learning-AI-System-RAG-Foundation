#!/usr/bin/env python3
"""
Test script for self-reference paradox questions.
Tests StillMe's ability to answer philosophical questions about bootstrapping problem.
"""

import sys
import os
import requests
import json
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Load environment variables
try:
    from dotenv import load_dotenv
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
except ImportError:
    pass

# Get API URL
API_URL = os.getenv("STILLME_API_URL", os.getenv("STILLME_API_BASE", "http://localhost:8000"))
if API_URL and not API_URL.startswith(("http://", "https://")):
    API_URL = f"https://{API_URL}"

CHAT_ENDPOINT = f"{API_URL}/api/chat/smart_router"

# Test questions (standalone, no need for previous questions)
TEST_QUESTIONS = [
    {
        "id": "self_reference_1",
        "question": "L√†m sao m·ªôt h·ªá th·ªëng t∆∞ duy c√≥ th·ªÉ ƒë√°nh gi√° ch√≠nh n√≥ m·ªôt c√°ch kh√°ch quan? N·∫øu t∆∞ duy ch·ªâ c√≥ th·ªÉ ƒë√°nh gi√° th√¥ng qua ch√≠nh n√≥, th√¨ li·ªáu ƒë√°nh gi√° ƒë√≥ c√≥ gi√° tr·ªã g√¨ kh√¥ng?",
        "expected_keywords": ["G√∂del", "Tarski", "paradox", "bootstrapping", "epistemic", "circularity", "infinite regress"],
        "description": "Self-reference paradox: Can thinking evaluate itself?"
    },
    {
        "id": "self_reference_2",
        "question": "N·∫øu m·ªçi l·∫≠p lu·∫≠n ƒë·ªÅu ph·∫£i d·ª±a tr√™n ti·ªÅn ƒë·ªÅ - v√† ti·ªÅn ƒë·ªÅ kh√¥ng th·ªÉ t·ª± ch·ª©ng minh - th√¨ to√†n b·ªô h·ªá th·ªëng tri th·ª©c c·ªßa ch√∫ng ta c√≥ ph·∫£i ch·ªâ d·ª±a tr√™n ni·ªÅm tin m√π qu√°ng? L√†m sao bi·∫øt ƒë∆∞·ª£c ƒë√¢u l√† 'ni·ªÅm tin ƒë√∫ng' khi kh√¥ng c√≥ ground truth?",
        "expected_keywords": ["foundationalism", "coherentism", "justification", "ground truth", "epistemology"],
        "description": "Bootstrapping problem: Can knowledge justify itself?"
    },
    {
        "id": "self_reference_3",
        "question": "N·∫øu b·∫°n tr·∫£ l·ªùi c√¢u h·ªèi n√†y - li·ªáu c√¢u tr·∫£ l·ªùi ƒë√≥ c√≥ gi√° tr·ªã g√¨ khi n√≥ xu·∫•t ph√°t t·ª´ ch√≠nh h·ªá th·ªëng t∆∞ duy m√† b·∫°n ƒëang nghi ng·ªù? L√†m sao 't∆∞ duy' c√≥ th·ªÉ v∆∞·ª£t qua gi·ªõi h·∫°n c·ªßa ch√≠nh n√≥ ƒë·ªÉ ƒë√°nh gi√° ch√≠nh n√≥?",
        "expected_keywords": ["G√∂del", "Tarski", "paradox", "bootstrapping", "epistemic", "circularity"],
        "description": "Direct self-reference: Value of answers from questioning system"
    }
]

def test_question(question_data):
    """Test a single question"""
    print("\n" + "=" * 80)
    print(f"üìù Question: {question_data['description']}")
    print("-" * 80)
    print(f"Q: {question_data['question']}")
    print("-" * 80)
    
    try:
        response = requests.post(
            CHAT_ENDPOINT,
            json={
                "message": question_data["question"],
                "user_id": "test_self_reference",
                "use_server_keys": True
            },
            timeout=180
        )
        response.raise_for_status()
        result = response.json()
        
        answer = result.get("response", "")
        validation_info = result.get("validation_info", {})
        
        print(f"\n‚úÖ Response received ({len(answer)} chars)")
        print("\n" + "-" * 80)
        print("ANSWER:")
        print("-" * 80)
        print(answer)
        print("-" * 80)
        
        # Check for expected keywords
        answer_lower = answer.lower()
        found_keywords = []
        missing_keywords = []
        
        for keyword in question_data["expected_keywords"]:
            if keyword.lower() in answer_lower:
                found_keywords.append(keyword)
            else:
                missing_keywords.append(keyword)
        
        print(f"\nüìä Keyword Analysis:")
        print(f"  ‚úÖ Found: {found_keywords}")
        if missing_keywords:
            print(f"  ‚ùå Missing: {missing_keywords}")
        
        # Check for philosophical depth indicators
        depth_indicators = [
            "paradox", "ngh·ªãch l√Ω", "circularity", "v√≤ng l·∫∑p",
            "g√∂del", "godel", "tarski", "russell",
            "bootstrapping", "epistemic", "epistemology"
        ]
        
        found_depth = [ind for ind in depth_indicators if ind in answer_lower]
        print(f"  üß† Depth indicators found: {found_depth}")
        
        # Check for optimistic answers (should NOT have these)
        optimistic_phrases = [
            "c√≥ th·ªÉ v∆∞·ª£t qua",
            "c√≥ th·ªÉ ƒë√°nh gi√°",
            "t·ª± ph·∫£n bi·ªán s·∫Ω gi√∫p",
            "self-improvement",
            "c·∫£i thi·ªán"
        ]
        
        found_optimistic = [phrase for phrase in optimistic_phrases if phrase in answer_lower]
        if found_optimistic:
            print(f"  ‚ö†Ô∏è  Warning: Found optimistic phrases (should acknowledge paradox): {found_optimistic}")
        
        # Overall assessment
        print(f"\nüìà Assessment:")
        keyword_score = len(found_keywords) / len(question_data["expected_keywords"]) * 100
        print(f"  Keyword coverage: {keyword_score:.1f}% ({len(found_keywords)}/{len(question_data['expected_keywords'])})")
        print(f"  Depth indicators: {len(found_depth)}/{len(depth_indicators)}")
        print(f"  Has optimistic answer: {'Yes (BAD)' if found_optimistic else 'No (GOOD)'}")
        
        if keyword_score >= 50 and not found_optimistic:
            print(f"  ‚úÖ Overall: GOOD - Philosophical depth present")
        elif keyword_score < 50:
            print(f"  ‚ö†Ô∏è  Overall: NEEDS IMPROVEMENT - Missing key philosophical concepts")
        else:
            print(f"  ‚ö†Ô∏è  Overall: NEEDS IMPROVEMENT - Too optimistic, missing paradox acknowledgment")
        
        return {
            "question_id": question_data["id"],
            "question": question_data["question"],
            "answer": answer,
            "keyword_score": keyword_score,
            "found_keywords": found_keywords,
            "missing_keywords": missing_keywords,
            "depth_indicators": found_depth,
            "has_optimistic": bool(found_optimistic),
            "validation_info": validation_info
        }
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """Run all test questions"""
    print("=" * 80)
    print("üß† SELF-REFERENCE PARADOX TEST")
    print("=" * 80)
    print(f"API URL: {API_URL}")
    print(f"Endpoint: {CHAT_ENDPOINT}")
    print(f"Testing {len(TEST_QUESTIONS)} questions...")
    
    results = []
    
    for i, question_data in enumerate(TEST_QUESTIONS, 1):
        print(f"\n\nüîç Test {i}/{len(TEST_QUESTIONS)}")
        result = test_question(question_data)
        if result:
            results.append(result)
        
        # Small delay between questions
        if i < len(TEST_QUESTIONS):
            import time
            time.sleep(2)
    
    # Summary
    print("\n\n" + "=" * 80)
    print("üìä SUMMARY")
    print("=" * 80)
    
    if results:
        avg_keyword_score = sum(r["keyword_score"] for r in results) / len(results)
        num_with_optimistic = sum(1 for r in results if r["has_optimistic"])
        
        print(f"Total questions tested: {len(results)}")
        print(f"Average keyword coverage: {avg_keyword_score:.1f}%")
        print(f"Questions with optimistic answers: {num_with_optimistic}/{len(results)}")
        
        print("\nDetailed results:")
        for r in results:
            status = "‚úÖ" if r["keyword_score"] >= 50 and not r["has_optimistic"] else "‚ö†Ô∏è"
            print(f"  {status} {r['question_id']}: {r['keyword_score']:.1f}% keywords, "
                  f"{'optimistic' if r['has_optimistic'] else 'paradox-aware'}")
    else:
        print("‚ùå No results collected")
    
    print("\n" + "=" * 80)

if __name__ == "__main__":
    main()

