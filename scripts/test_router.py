#!/usr/bin/env python3
"""
AI Router Test Script

This script provides comprehensive testing for the AI router,
including unit tests, integration tests, and performance tests.

Usage:
    python scripts/test_router.py --unit
    python scripts/test_router.py --integration
    python scripts/test_router.py --performance
    python scripts/test_router.py --all
"""

import os
import sys
import argparse
import time
import unittest
from typing import Dict, List, Tuple, Optional

# Add stillme_core to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'stillme_core'))

from modules.api_provider_manager import UnifiedAPIManager, ComplexityAnalyzer

class TestComplexityAnalyzer(unittest.TestCase):
    def setUp(self):
        self.analyzer = ComplexityAnalyzer()
    
    def test_simple_prompts(self):
        """Test that simple prompts get low complexity scores"""
        simple_prompts = [
            "ch√†o b·∫°n",
            "b·∫°n kh·ªèe kh√¥ng?",
            "2+2 b·∫±ng m·∫•y?",
            "th·ªß ƒë√¥ Vi·ªát Nam l√† g√¨?",
            "hello",
            "how are you?",
            "t√™n b·∫°n l√† g√¨?",
            "b·∫°n c√≥ th·ªÉ gi√∫p g√¨?",
            "c·∫£m ∆°n",
            "t·∫°m bi·ªát"
        ]
        
        for prompt in simple_prompts:
            with self.subTest(prompt=prompt):
                score, _ = self.analyzer.analyze_complexity(prompt)
                self.assertLess(score, 0.4, f"Simple prompt '{prompt}' got high complexity: {score}")
    
    def test_coding_prompts(self):
        """Test that coding prompts get medium complexity scores"""
        coding_prompts = [
            "vi·∫øt code Python",
            "l·∫≠p tr√¨nh JavaScript",
            "debug l·ªói",
            "t·∫°o function",
            "vi·∫øt code",
            "t·ªëi ∆∞u thu·∫≠t to√°n",
            "s·ª≠a l·ªói code",
            "t·∫°o class Python",
            "vi·∫øt code Python t√≠nh giai th·ª´a",
            "n·∫øu t√¥i mu·ªën h·ªçc l·∫≠p tr√¨nh th√¨ n√™n b·∫Øt ƒë·∫ßu t·ª´ ƒë√¢u?",
            "t·ªëi ∆∞u h√≥a performance c·ªßa database query",
            "s·ª≠a l·ªói trong thu·∫≠t to√°n s·∫Øp x·∫øp",
            "t·∫°o function JavaScript ƒë·ªÉ validate email",
            "debug l·ªói trong code Python",
            "t·ªëi ∆∞u h√≥a memory usage"
        ]
        
        for prompt in coding_prompts:
            with self.subTest(prompt=prompt):
                score, _ = self.analyzer.analyze_complexity(prompt)
                self.assertGreaterEqual(score, 0.3, f"Coding prompt '{prompt}' got too low complexity: {score}")
                self.assertLess(score, 0.7, f"Coding prompt '{prompt}' got too high complexity: {score}")
    
    def test_complex_prompts(self):
        """Test that complex prompts get high complexity scores"""
        complex_prompts = [
            "Gi·∫£i th√≠ch ƒë·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del",
            "Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa tri·∫øt h·ªçc v√† khoa h·ªçc",
            "So s√°nh c√°c ph∆∞∆°ng ph√°p h·ªçc m√°y",
            "T·∫°i sao c√°c h·ªá th·ªëng ph·ª©c t·∫°p l·∫°i t·ª± t·ªï ch·ª©c?",
            "√ù nghƒ©a c·ªßa cu·ªôc s·ªëng l√† g√¨?",
            "B·∫£n ch·∫•t c·ªßa th·ª±c t·∫°i l√† g√¨?",
            "T√°c ƒë·ªông c·ªßa AI ƒë·∫øn x√£ h·ªôi",
            "Ph√¢n t√≠ch xu h∆∞·ªõng ph√°t tri·ªÉn c√¥ng ngh·ªá",
            "gi·∫£ s·ª≠ t√¥i c√≥ m·ªôt b√†i to√°n ph·ª©c t·∫°p, l√†m th·∫ø n√†o ƒë·ªÉ gi·∫£i quy·∫øt n√≥?",
            "trong tr∆∞·ªùng h·ª£p n√†o th√¨ n√™n s·ª≠ d·ª•ng AI?",
            "Gi·∫£i th√≠ch ƒë·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del v√† t√°c ƒë·ªông c·ªßa n√≥ ƒë·∫øn to√°n h·ªçc hi·ªán ƒë·∫°i",
            "Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa tri·∫øt h·ªçc v√† khoa h·ªçc trong vi·ªác hi·ªÉu b·∫£n ch·∫•t c·ªßa th·ª±c t·∫°i",
            "So s√°nh v√† ƒë√°nh gi√° c√°c ph∆∞∆°ng ph√°p h·ªçc m√°y kh√°c nhau trong vi·ªác x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n",
            "T·∫°i sao c√°c h·ªá th·ªëng ph·ª©c t·∫°p l·∫°i c√≥ xu h∆∞·ªõng t·ª± t·ªï ch·ª©c v√† ph√°t tri·ªÉn theo quy lu·∫≠t n√†o?",
            "Ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫øn x√£ h·ªôi v√† t∆∞∆°ng lai c·ªßa nh√¢n lo·∫°i"
        ]
        
        for prompt in complex_prompts:
            with self.subTest(prompt=prompt):
                score, _ = self.analyzer.analyze_complexity(prompt)
                self.assertGreater(score, 0.7, f"Complex prompt '{prompt}' got low complexity: {score}")
    
    def test_fallback_detection(self):
        """Test fallback mechanism detection"""
        # Test cases that should trigger fallback
        fallback_triggers = [
            "sai r·ªìi",
            "kh√¥ng ƒë√∫ng",
            "???",
            "kh√¥ng ƒë√∫ng √Ω m√¨nh",
            "ch∆∞a ch√≠nh x√°c",
            "thi·∫øu th√¥ng tin",
            "kh√¥ng hi·ªÉu",
            "??",
            "sai",
            "kh√¥ng ƒë√∫ng √Ω",
            "ch∆∞a ƒë√∫ng",
            "thi·∫øu",
            "kh√¥ng r√µ",
            "m∆° h·ªì",
            "kh√¥ng ch√≠nh x√°c"
        ]
        
        for feedback in fallback_triggers:
            with self.subTest(feedback=feedback):
                should_fallback = self.analyzer.should_trigger_fallback(feedback, "original prompt", "gemma2:2b")
                self.assertTrue(should_fallback, f"Negative feedback '{feedback}' should trigger fallback")
        
        # Test cases that should NOT trigger fallback
        no_fallback_triggers = [
            "ƒë√∫ng r·ªìi",
            "ok",
            "c·∫£m ∆°n",
            "t·ªët l·∫Øm",
            "tuy·ªát v·ªùi",
            "c·∫£m ∆°n b·∫°n",
            "r·∫•t hay",
            "ch√≠nh x√°c",
            "ƒë√∫ng √Ω",
            "t·ªët",
            "hay",
            "ƒë∆∞·ª£c",
            "·ªïn",
            "t·ªët r·ªìi",
            "c·∫£m ∆°n nhi·ªÅu"
        ]
        
        for feedback in no_fallback_triggers:
            with self.subTest(feedback=feedback):
                should_fallback = self.analyzer.should_trigger_fallback(feedback, "original prompt", "gemma2:2b")
                self.assertFalse(should_fallback, f"Positive feedback '{feedback}' should not trigger fallback")
    
    def test_performance(self):
        """Test that complexity analysis is fast"""
        start_time = time.time()
        for _ in range(100):
            self.analyzer.analyze_complexity("This is a simple test prompt to check performance.")
        end_time = time.time()
        
        elapsed_ms = (end_time - start_time) * 1000 / 100
        self.assertLess(elapsed_ms, 5, f"Average analysis time {elapsed_ms:.2f}ms, expected < 5ms")
    
    def test_edge_cases(self):
        """Test edge cases"""
        # Empty prompt
        score, _ = self.analyzer.analyze_complexity("")
        self.assertEqual(score, 0.0, "Empty prompt should have complexity 0")
        
        # Very long prompt
        long_prompt = "a " * 1000
        score, _ = self.analyzer.analyze_complexity(long_prompt)
        self.assertGreater(score, 0.0, "Very long prompt should have some complexity")
        
        # Single word
        score, _ = self.analyzer.analyze_complexity("hello")
        self.assertLess(score, 0.4, "Single word should be simple")
        
        # Special characters
        score, _ = self.analyzer.analyze_complexity("!@#$%^&*()")
        self.assertLess(score, 0.4, "Special characters should be simple")

class TestUnifiedAPIManager(unittest.TestCase):
    def setUp(self):
        # Mock the API calls to prevent actual network requests
        self.patcher_ollama = unittest.mock.patch.object(UnifiedAPIManager, 'call_ollama_api', return_value="Mock Ollama Response")
        self.patcher_deepseek = unittest.mock.patch.object(UnifiedAPIManager, 'call_deepseek_api', return_value="Mock DeepSeek Response")
        self.mock_ollama = self.patcher_ollama.start()
        self.mock_deepseek = self.patcher_deepseek.start()
        self.addCleanup(self.patcher_ollama.stop)
        self.addCleanup(self.patcher_deepseek.stop)
        
        self.manager = UnifiedAPIManager()
    
    def test_simple_routing(self):
        """Test that simple prompts route to gemma2:2b"""
        simple_prompts = [
            "ch√†o b·∫°n",
            "b·∫°n t√™n g√¨?",
            "2+2 b·∫±ng m·∫•y?",
            "th·ªß ƒë√¥ Vi·ªát Nam l√† g√¨?",
            "hello",
            "how are you?",
            "t√™n b·∫°n l√† g√¨?",
            "b·∫°n c√≥ th·ªÉ gi√∫p g√¨?",
            "c·∫£m ∆°n",
            "t·∫°m bi·ªát"
        ]
        
        for prompt in simple_prompts:
            with self.subTest(prompt=prompt):
                selected_model = self.manager.choose_model(prompt)
                self.assertEqual(selected_model, "gemma2:2b", f"Prompt '{prompt}' should route to gemma2:2b, got {selected_model}")
    
    def test_coding_routing(self):
        """Test that coding prompts route to deepseek-coder:6.7b"""
        coding_prompts = [
            "vi·∫øt code Python",
            "l·∫≠p tr√¨nh JavaScript",
            "debug l·ªói",
            "t·∫°o function",
            "vi·∫øt code",
            "t·ªëi ∆∞u thu·∫≠t to√°n",
            "s·ª≠a l·ªói code",
            "t·∫°o class Python",
            "vi·∫øt code Python t√≠nh giai th·ª´a",
            "n·∫øu t√¥i mu·ªën h·ªçc l·∫≠p tr√¨nh th√¨ n√™n b·∫Øt ƒë·∫ßu t·ª´ ƒë√¢u?",
            "t·ªëi ∆∞u h√≥a performance c·ªßa database query",
            "s·ª≠a l·ªói trong thu·∫≠t to√°n s·∫Øp x·∫øp",
            "t·∫°o function JavaScript ƒë·ªÉ validate email",
            "debug l·ªói trong code Python",
            "t·ªëi ∆∞u h√≥a memory usage"
        ]
        
        for prompt in coding_prompts:
            with self.subTest(prompt=prompt):
                selected_model = self.manager.choose_model(prompt)
                self.assertEqual(selected_model, "deepseek-coder:6.7b", f"Prompt '{prompt}' should route to deepseek-coder:6.7b, got {selected_model}")
    
    def test_complex_routing(self):
        """Test that complex prompts route to deepseek-chat"""
        complex_prompts = [
            "Gi·∫£i th√≠ch ƒë·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del",
            "Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa tri·∫øt h·ªçc v√† khoa h·ªçc",
            "So s√°nh c√°c ph∆∞∆°ng ph√°p h·ªçc m√°y",
            "T·∫°i sao c√°c h·ªá th·ªëng ph·ª©c t·∫°p l·∫°i t·ª± t·ªï ch·ª©c?",
            "√ù nghƒ©a c·ªßa cu·ªôc s·ªëng l√† g√¨?",
            "B·∫£n ch·∫•t c·ªßa th·ª±c t·∫°i l√† g√¨?",
            "T√°c ƒë·ªông c·ªßa AI ƒë·∫øn x√£ h·ªôi",
            "Ph√¢n t√≠ch xu h∆∞·ªõng ph√°t tri·ªÉn c√¥ng ngh·ªá",
            "gi·∫£ s·ª≠ t√¥i c√≥ m·ªôt b√†i to√°n ph·ª©c t·∫°p, l√†m th·∫ø n√†o ƒë·ªÉ gi·∫£i quy·∫øt n√≥?",
            "trong tr∆∞·ªùng h·ª£p n√†o th√¨ n√™n s·ª≠ d·ª•ng AI?",
            "Gi·∫£i th√≠ch ƒë·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del v√† t√°c ƒë·ªông c·ªßa n√≥ ƒë·∫øn to√°n h·ªçc hi·ªán ƒë·∫°i",
            "Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa tri·∫øt h·ªçc v√† khoa h·ªçc trong vi·ªác hi·ªÉu b·∫£n ch·∫•t c·ªßa th·ª±c t·∫°i",
            "So s√°nh v√† ƒë√°nh gi√° c√°c ph∆∞∆°ng ph√°p h·ªçc m√°y kh√°c nhau trong vi·ªác x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n",
            "T·∫°i sao c√°c h·ªá th·ªëng ph·ª©c t·∫°p l·∫°i c√≥ xu h∆∞·ªõng t·ª± t·ªï ch·ª©c v√† ph√°t tri·ªÉn theo quy lu·∫≠t n√†o?",
            "Ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫øn x√£ h·ªôi v√† t∆∞∆°ng lai c·ªßa nh√¢n lo·∫°i"
        ]
        
        for prompt in complex_prompts:
            with self.subTest(prompt=prompt):
                selected_model = self.manager.choose_model(prompt)
                self.assertEqual(selected_model, "deepseek-chat", f"Prompt '{prompt}' should route to deepseek-chat, got {selected_model}")
    
    def test_model_preferences(self):
        """Test that model preferences are respected"""
        # Test that local models are preferred when available
        self.assertIn("gemma2:2b", self.manager.model_preferences)
        self.assertIn("deepseek-coder:6.7b", self.manager.model_preferences)
        self.assertIn("deepseek-chat", self.manager.model_preferences)
    
    def test_fallback_handling(self):
        """Test fallback mechanism"""
        # Test that fallback is triggered for negative feedback
        fallback_triggered = self.manager.handle_fallback("sai r·ªìi", "original prompt", "gemma2:2b")
        self.assertTrue(fallback_triggered, "Fallback should be triggered for negative feedback")
        
        # Test that fallback is not triggered for positive feedback
        fallback_not_triggered = self.manager.handle_fallback("ƒë√∫ng r·ªìi", "original prompt", "gemma2:2b")
        self.assertFalse(fallback_not_triggered, "Fallback should not be triggered for positive feedback")

class TestIntegration(unittest.TestCase):
    def setUp(self):
        self.manager = UnifiedAPIManager()
        self.analyzer = ComplexityAnalyzer()
    
    def test_end_to_end_routing(self):
        """Test end-to-end routing flow"""
        test_cases = [
            ("ch√†o b·∫°n", "gemma2:2b"),
            ("vi·∫øt code Python", "deepseek-coder:6.7b"),
            ("Gi·∫£i th√≠ch ƒë·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del", "deepseek-chat")
        ]
        
        for prompt, expected_model in test_cases:
            with self.subTest(prompt=prompt):
                # Test complexity analysis
                score, _ = self.analyzer.analyze_complexity(prompt)
                
                # Test model selection
                selected_model = self.manager.choose_model(prompt)
                
                # Verify routing
                self.assertEqual(selected_model, expected_model, f"Prompt '{prompt}' should route to {expected_model}, got {selected_model}")
    
    def test_performance_under_load(self):
        """Test performance under load"""
        test_prompts = [
            "ch√†o b·∫°n",
            "vi·∫øt code Python",
            "Gi·∫£i th√≠ch ƒë·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del"
        ]
        
        start_time = time.time()
        for _ in range(100):
            for prompt in test_prompts:
                self.manager.choose_model(prompt)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / (100 * len(test_prompts))
        self.assertLess(avg_time, 0.01, f"Average routing time {avg_time:.4f}s, expected < 0.01s")
    
    def test_consistency(self):
        """Test that routing is consistent"""
        prompt = "vi·∫øt code Python t√≠nh giai th·ª´a"
        
        # Test multiple times
        results = []
        for _ in range(10):
            selected_model = self.manager.choose_model(prompt)
            results.append(selected_model)
        
        # All results should be the same
        self.assertTrue(all(r == results[0] for r in results), f"Routing inconsistent: {results}")

class RouterTestSuite:
    def __init__(self):
        self.test_results = {}
    
    def run_unit_tests(self) -> Dict:
        """Run unit tests"""
        print("üß™ Running Unit Tests")
        print("=" * 40)
        
        # Create test suite
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        
        # Add test cases
        suite.addTests(loader.loadTestsFromTestCase(TestComplexityAnalyzer))
        suite.addTests(loader.loadTestsFromTestCase(TestUnifiedAPIManager))
        
        # Run tests
        runner = unittest.TextTestRunner(verbosity=2)
        result = runner.run(suite)
        
        # Compile results
        test_results = {
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0,
            'details': {
                'failures': [str(f[1]) for f in result.failures],
                'errors': [str(e[1]) for e in result.errors]
            }
        }
        
        print(f"\nüìä Unit Test Results:")
        print(f"  Tests run: {test_results['tests_run']}")
        print(f"  Failures: {test_results['failures']}")
        print(f"  Errors: {test_results['errors']}")
        print(f"  Success rate: {test_results['success_rate']:.1%}")
        
        return test_results
    
    def run_integration_tests(self) -> Dict:
        """Run integration tests"""
        print("\nüîó Running Integration Tests")
        print("=" * 40)
        
        # Create test suite
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        
        # Add test cases
        suite.addTests(loader.loadTestsFromTestCase(TestIntegration))
        
        # Run tests
        runner = unittest.TextTestRunner(verbosity=2)
        result = runner.run(suite)
        
        # Compile results
        test_results = {
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0,
            'details': {
                'failures': [str(f[1]) for f in result.failures],
                'errors': [str(e[1]) for e in result.errors]
            }
        }
        
        print(f"\nüìä Integration Test Results:")
        print(f"  Tests run: {test_results['tests_run']}")
        print(f"  Failures: {test_results['failures']}")
        print(f"  Errors: {test_results['errors']}")
        print(f"  Success rate: {test_results['success_rate']:.1%}")
        
        return test_results
    
    def run_performance_tests(self) -> Dict:
        """Run performance tests"""
        print("\n‚ö° Running Performance Tests")
        print("=" * 40)
        
        analyzer = ComplexityAnalyzer()
        manager = UnifiedAPIManager()
        
        # Test prompts
        test_prompts = [
            "ch√†o b·∫°n",
            "vi·∫øt code Python t√≠nh giai th·ª´a",
            "Gi·∫£i th√≠ch ƒë·ªãnh l√Ω b·∫•t to√†n c·ªßa G√∂del"
        ]
        
        # Test complexity analysis performance
        print("Testing complexity analysis performance...")
        start_time = time.time()
        for _ in range(1000):
            for prompt in test_prompts:
                analyzer.analyze_complexity(prompt)
        end_time = time.time()
        
        analysis_time = (end_time - start_time) / (1000 * len(test_prompts))
        
        # Test model selection performance
        print("Testing model selection performance...")
        start_time = time.time()
        for _ in range(1000):
            for prompt in test_prompts:
                manager.choose_model(prompt)
        end_time = time.time()
        
        selection_time = (end_time - start_time) / (1000 * len(test_prompts))
        
        # Compile results
        test_results = {
            'complexity_analysis_time': analysis_time,
            'model_selection_time': selection_time,
            'total_time': analysis_time + selection_time,
            'performance_grade': self._get_performance_grade(analysis_time + selection_time)
        }
        
        print(f"\nüìä Performance Test Results:")
        print(f"  Complexity analysis time: {analysis_time*1000:.2f}ms")
        print(f"  Model selection time: {selection_time*1000:.2f}ms")
        print(f"  Total time: {test_results['total_time']*1000:.2f}ms")
        print(f"  Performance grade: {test_results['performance_grade']}")
        
        return test_results
    
    def _get_performance_grade(self, total_time: float) -> str:
        """Get performance grade based on total time"""
        if total_time < 0.001:  # < 1ms
            return "A+ (Excellent)"
        elif total_time < 0.005:  # < 5ms
            return "A (Very Good)"
        elif total_time < 0.01:  # < 10ms
            return "B (Good)"
        elif total_time < 0.05:  # < 50ms
            return "C (Acceptable)"
        else:
            return "D (Needs Improvement)"
    
    def run_all_tests(self) -> Dict:
        """Run all tests"""
        print("üîç AI Router Full Test Suite")
        print("=" * 60)
        
        start_time = time.time()
        
        # Run all test suites
        unit_results = self.run_unit_tests()
        integration_results = self.run_integration_tests()
        performance_results = self.run_performance_tests()
        
        end_time = time.time()
        
        # Compile overall results
        overall_results = {
            'unit_tests': unit_results,
            'integration_tests': integration_results,
            'performance_tests': performance_results,
            'total_time': end_time - start_time,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Print summary
        print(f"\nüìä Full Test Suite Summary")
        print("=" * 60)
        print(f"Unit tests: {unit_results['success_rate']:.1%} success rate")
        print(f"Integration tests: {integration_results['success_rate']:.1%} success rate")
        print(f"Performance: {performance_results['performance_grade']}")
        print(f"Total test time: {overall_results['total_time']:.2f}s")
        
        # Overall status
        if (unit_results['success_rate'] >= 0.9 and 
            integration_results['success_rate'] >= 0.9 and 
            performance_results['performance_grade'].startswith('A')):
            overall_status = "PASS"
        elif (unit_results['success_rate'] >= 0.8 and 
              integration_results['success_rate'] >= 0.8 and 
              performance_results['performance_grade'].startswith('B')):
            overall_status = "WARN"
        else:
            overall_status = "FAIL"
        
        print(f"\nüéØ Overall Status: {overall_status}")
        
        return overall_results

def main():
    parser = argparse.ArgumentParser(description='AI Router Test Script')
    parser.add_argument('--unit', action='store_true', help='Run unit tests')
    parser.add_argument('--integration', action='store_true', help='Run integration tests')
    parser.add_argument('--performance', action='store_true', help='Run performance tests')
    parser.add_argument('--all', action='store_true', help='Run all tests')
    
    args = parser.parse_args()
    
    test_suite = RouterTestSuite()
    
    if args.unit:
        results = test_suite.run_unit_tests()
    elif args.integration:
        results = test_suite.run_integration_tests()
    elif args.performance:
        results = test_suite.run_performance_tests()
    elif args.all:
        results = test_suite.run_all_tests()
    else:
        print("Please specify --unit, --integration, --performance, or --all")
        print("Use --help for more information")
        return
    
    # Exit with appropriate code
    if 'success_rate' in results:
        if results['success_rate'] >= 0.9:
            sys.exit(0)  # Success
        else:
            sys.exit(1)  # Failure
    elif 'performance_grade' in results:
        if results['performance_grade'].startswith('A'):
            sys.exit(0)  # Success
        else:
            sys.exit(1)  # Failure
    else:
        sys.exit(0)  # Success

if __name__ == "__main__":
    main()
