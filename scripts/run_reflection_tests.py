#!/usr/bin/env python3
"""
Reflection Controller Test Runner
TrÃ¬nh cháº¡y kiá»ƒm thá»­ Reflection Controller

PURPOSE / Má»¤C ÄÃCH:
- Run comprehensive test suite for Reflection Controller
- Cháº¡y bá»™ kiá»ƒm thá»­ toÃ n diá»‡n cho Reflection Controller
- Generate test reports
- Táº¡o bÃ¡o cÃ¡o kiá»ƒm thá»­
- Performance benchmarking
- Äiá»ƒm chuáº©n hiá»‡u suáº¥t
"""

import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path


def run_tests():
    """Run all reflection controller tests / Cháº¡y táº¥t cáº£ kiá»ƒm thá»­ reflection controller"""
    print("ğŸ§ª Running Reflection Controller Test Suite...")
    print("ğŸ§ª Cháº¡y bá»™ kiá»ƒm thá»­ Reflection Controller...")

    # Ensure we're in the project root
    project_root = Path(__file__).parent.parent
    os.chdir(project_root)

    # Create logs directory if it doesn't exist
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)

    # Test results
    test_results = {"timestamp": datetime.now().isoformat(), "tests": {}, "summary": {}}

    # Run unit tests
    print("\nğŸ“‹ Running unit tests...")
    print("ğŸ“‹ Cháº¡y kiá»ƒm thá»­ Ä‘Æ¡n vá»‹...")

    try:
        result = subprocess.run(
            [
                sys.executable,
                "-m",
                "pytest",
                "tests/test_reflection_controller.py",
                "--timeout=60",
                "-v",
                "--tb=short",
            ],
            capture_output=True,
            text=True,
            timeout=300,
        )

        test_results["tests"]["unit_tests"] = {
            "returncode": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "success": result.returncode == 0,
        }

        if result.returncode == 0:
            print("âœ… Unit tests passed")
            print("âœ… Kiá»ƒm thá»­ Ä‘Æ¡n vá»‹ Ä‘Ã£ qua")
        else:
            print("âŒ Unit tests failed")
            print("âŒ Kiá»ƒm thá»­ Ä‘Æ¡n vá»‹ tháº¥t báº¡i")
            print(f"Error: {result.stderr}")

    except subprocess.TimeoutExpired:
        print("â° Unit tests timed out")
        print("â° Kiá»ƒm thá»­ Ä‘Æ¡n vá»‹ háº¿t thá»i gian")
        test_results["tests"]["unit_tests"] = {
            "returncode": -1,
            "stdout": "",
            "stderr": "Timeout after 300 seconds",
            "success": False,
        }
    except Exception as e:
        print(f"âŒ Error running unit tests: {e}")
        test_results["tests"]["unit_tests"] = {
            "returncode": -1,
            "stdout": "",
            "stderr": str(e),
            "success": False,
        }

    # Run integration tests
    print("\nğŸ”— Running integration tests...")
    print("ğŸ”— Cháº¡y kiá»ƒm thá»­ tÃ­ch há»£p...")

    try:
        result = subprocess.run(
            [
                sys.executable,
                "-m",
                "pytest",
                "tests/test_reflection_integration.py",
                "--timeout=120",
                "-v",
                "--tb=short",
            ],
            capture_output=True,
            text=True,
            timeout=600,
        )

        test_results["tests"]["integration_tests"] = {
            "returncode": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "success": result.returncode == 0,
        }

        if result.returncode == 0:
            print("âœ… Integration tests passed")
            print("âœ… Kiá»ƒm thá»­ tÃ­ch há»£p Ä‘Ã£ qua")
        else:
            print("âŒ Integration tests failed")
            print("âŒ Kiá»ƒm thá»­ tÃ­ch há»£p tháº¥t báº¡i")
            print(f"Error: {result.stderr}")

    except subprocess.TimeoutExpired:
        print("â° Integration tests timed out")
        print("â° Kiá»ƒm thá»­ tÃ­ch há»£p háº¿t thá»i gian")
        test_results["tests"]["integration_tests"] = {
            "returncode": -1,
            "stdout": "",
            "stderr": "Timeout after 600 seconds",
            "success": False,
        }
    except Exception as e:
        print(f"âŒ Error running integration tests: {e}")
        test_results["tests"]["integration_tests"] = {
            "returncode": -1,
            "stdout": "",
            "stderr": str(e),
            "success": False,
        }

    # Run smoke tests
    print("\nğŸ’¨ Running smoke tests...")
    print("ğŸ’¨ Cháº¡y kiá»ƒm thá»­ smoke...")

    try:
        smoke_result = run_smoke_tests()
        test_results["tests"]["smoke_tests"] = smoke_result

        if smoke_result["success"]:
            print("âœ… Smoke tests passed")
            print("âœ… Kiá»ƒm thá»­ smoke Ä‘Ã£ qua")
        else:
            print("âŒ Smoke tests failed")
            print("âŒ Kiá»ƒm thá»­ smoke tháº¥t báº¡i")

    except Exception as e:
        print(f"âŒ Error running smoke tests: {e}")
        test_results["tests"]["smoke_tests"] = {"success": False, "error": str(e)}

    # Generate summary
    total_tests = len(test_results["tests"])
    passed_tests = sum(
        1 for test in test_results["tests"].values() if test.get("success", False)
    )

    test_results["summary"] = {
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "failed_tests": total_tests - passed_tests,
        "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
    }

    # Save test results
    results_file = logs_dir / "reflection_test_results.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)

    # Print summary
    print("\nğŸ“Š Test Summary / TÃ³m táº¯t kiá»ƒm thá»­:")
    print(f"ğŸ“Š Total tests: {total_tests}")
    print(f"ğŸ“Š Passed: {passed_tests}")
    print(f"ğŸ“Š Failed: {total_tests - passed_tests}")
    print(f"ğŸ“Š Success rate: {test_results['summary']['success_rate']:.1%}")

    print(f"\nğŸ“„ Detailed results saved to: {results_file}")
    print(f"ğŸ“„ Káº¿t quáº£ chi tiáº¿t Ä‘Ã£ lÆ°u vÃ o: {results_file}")

    return test_results["summary"]["success_rate"] > 0.8


def run_smoke_tests():
    """Run smoke tests / Cháº¡y kiá»ƒm thá»­ smoke"""
    try:
        # Import components
        from stillme_core.reflection_controller import get_default_controller
        from stillme_core.reflection_scorer import get_default_scorer
        from stillme_core.secrecy_filter import get_default_filter

        # Test basic functionality
        controller = get_default_controller()
        scorer = get_default_scorer()
        filter_instance = get_default_filter()

        # Test should_reflect
        assert controller.should_reflect("Hello") is False
        assert controller.should_reflect("How to install Python?") is True

        # Test scoring
        result = scorer.score_response("Test response", "Test query")
        assert result.total_score >= 0.0

        # Test filtering
        filter_result = filter_instance.filter_content("Safe content", "Normal query")
        assert filter_result.is_safe is True

        return {"success": True, "message": "All smoke tests passed"}

    except Exception as e:
        return {"success": False, "error": str(e)}


def run_performance_benchmarks():
    """Run performance benchmarks / Cháº¡y Ä‘iá»ƒm chuáº©n hiá»‡u suáº¥t"""
    print("\nâš¡ Running performance benchmarks...")
    print("âš¡ Cháº¡y Ä‘iá»ƒm chuáº©n hiá»‡u suáº¥t...")

    try:
        import asyncio

        from stillme_core.reflection_controller import get_default_controller

        controller = get_default_controller()

        # Benchmark queries
        benchmark_queries = [
            "How to install Python?",
            "What is machine learning?",
            "How to optimize code?",
            "Best practices for API design?",
            "How to debug Python?",
        ]

        # Run benchmarks
        start_time = time.time()

        async def run_benchmark():
            tasks = []
            for query in benchmark_queries:
                task = controller.enhance_response("Test response", query)
                tasks.append(task)

            results = await asyncio.gather(*tasks)
            return results

        results = asyncio.run(run_benchmark())
        total_time = time.time() - start_time

        # Calculate metrics
        avg_time = total_time / len(benchmark_queries)
        total_improvements = sum(result.improvement for result in results)
        avg_improvement = total_improvements / len(results)

        benchmark_results = {
            "total_queries": len(benchmark_queries),
            "total_time": total_time,
            "avg_time_per_query": avg_time,
            "avg_improvement": avg_improvement,
            "success": True,
        }

        print("âš¡ Benchmark Results / Káº¿t quáº£ Ä‘iá»ƒm chuáº©n:")
        print(f"âš¡ Total queries: {len(benchmark_queries)}")
        print(f"âš¡ Total time: {total_time:.2f}s")
        print(f"âš¡ Average time per query: {avg_time:.2f}s")
        print(f"âš¡ Average improvement: {avg_improvement:.3f}")

        return benchmark_results

    except Exception as e:
        print(f"âŒ Error running benchmarks: {e}")
        return {"success": False, "error": str(e)}


def generate_test_report():
    """Generate comprehensive test report / Táº¡o bÃ¡o cÃ¡o kiá»ƒm thá»­ toÃ n diá»‡n"""
    print("\nğŸ“‹ Generating test report...")
    print("ğŸ“‹ Táº¡o bÃ¡o cÃ¡o kiá»ƒm thá»­...")

    # Read test results
    results_file = Path("logs/reflection_test_results.json")
    if not results_file.exists():
        print("âŒ No test results found")
        return

    with open(results_file, encoding="utf-8") as f:
        test_results = json.load(f)

    # Generate report
    report = f"""
# Reflection Controller Test Report
# BÃ¡o cÃ¡o kiá»ƒm thá»­ Reflection Controller

## Test Summary / TÃ³m táº¯t kiá»ƒm thá»­
- **Timestamp / Thá»i gian:** {test_results['timestamp']}
- **Total Tests / Tá»•ng kiá»ƒm thá»­:** {test_results['summary']['total_tests']}
- **Passed / ÄÃ£ qua:** {test_results['summary']['passed_tests']}
- **Failed / Tháº¥t báº¡i:** {test_results['summary']['failed_tests']}
- **Success Rate / Tá»· lá»‡ thÃ nh cÃ´ng:** {test_results['summary']['success_rate']:.1%}

## Test Details / Chi tiáº¿t kiá»ƒm thá»­
"""

    for test_name, test_result in test_results["tests"].items():
        status = "âœ… PASSED" if test_result.get("success", False) else "âŒ FAILED"
        report += f"\n### {test_name.replace('_', ' ').title()}\n"
        report += f"- **Status / Tráº¡ng thÃ¡i:** {status}\n"

        if "returncode" in test_result:
            report += f"- **Return Code / MÃ£ tráº£ vá»:** {test_result['returncode']}\n"

        if test_result.get("stderr"):
            report += f"- **Error / Lá»—i:** {test_result['stderr'][:200]}...\n"

    # Save report
    report_file = Path("reports/reflection_test_report.md")
    report_file.parent.mkdir(exist_ok=True)

    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"ğŸ“„ Test report saved to: {report_file}")
    print(f"ğŸ“„ BÃ¡o cÃ¡o kiá»ƒm thá»­ Ä‘Ã£ lÆ°u vÃ o: {report_file}")


if __name__ == "__main__":
    print("ğŸš€ Reflection Controller Test Runner")
    print("ğŸš€ TrÃ¬nh cháº¡y kiá»ƒm thá»­ Reflection Controller")
    print("=" * 50)

    # Run tests
    success = run_tests()

    # Run benchmarks
    benchmark_results = run_performance_benchmarks()

    # Generate report
    generate_test_report()

    # Exit with appropriate code
    if success:
        print("\nğŸ‰ All tests completed successfully!")
        print("ğŸ‰ Táº¥t cáº£ kiá»ƒm thá»­ Ä‘Ã£ hoÃ n thÃ nh thÃ nh cÃ´ng!")
        sys.exit(0)
    else:
        print("\nâŒ Some tests failed!")
        print("âŒ Má»™t sá»‘ kiá»ƒm thá»­ tháº¥t báº¡i!")
        sys.exit(1)
